<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[解决Windows版本Teamviewer将用户判定为商业行为的方法（修改ID）]]></title>
    <url>%2Fposts%2F33217%2F</url>
    <content type="text"><![CDATA[上星期Teamviewer把我判定为了商业用户，猜测可能是实验室电脑的局域网中有大量的计算机设备吧。没有办法，第一时间邮件联系了TV官方，说明我这台计算机是学校实验室里的，然而……至今仍没有回复。期间尝试了其他软件，像是向日葵、Anydesk，都实在是太卡了。 分享一个修改Teamviewer ID的方法，改ID之后可以正常连接。 首先下载TVTools AlterID 2.0，https://pan.baidu.com/s/1MPK80kzaQc-F6v8RiGr8ow，提取码：khgt 打开软件修改ID，选择Teamviewer的安装目录，然后依次点击Free、Start就可以了。 重启Teamviewer之后，就可以看到ID被成功修改了。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Teamviewer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[极大似然估计和EM算法初步]]></title>
    <url>%2Fposts%2F1422%2F</url>
    <content type="text"><![CDATA[极大似然估计是在知道结果的情况下，寻求使该结果出现可能性极大的条件，以此作为估计值。在维基百科中，极大似然估计的定义是这样的： 给定一个概率分布$D$，已知其概率密度函数（连续分布）或概率质量函数（离散分布）为$f_D$，以及一个分布参数$\theta$，我们可以从这个分布中抽出一个具有n个值的采样$X_1, X_2, …, X_n$，计算出其似然函数： L(\theta|x_1, ..., x_n) = f_{\theta}(x_1, ..., x_n)若$D$是离散分布，$f_{\theta}$即是在参数为$\theta$时观测到这一采样的概率。若其是连续分布，$f_{\theta}$则为$X_1, X_2, …, X_n$联合分布的概率密度函数在观测值处的取值。一旦我们获得$X_1, X_2, …, X_n$，我们就能求得一个关于$\theta$的估计。最大似然估计会寻找关于$\theta$的最可能的值。从数学上说，我们可以在$\theta$的所有可能取值中寻找一个值使得似然函数取到最大值。这个使可能性最大的$\hat \theta$值即称为$\theta$的极大似然估计。由定义，极大似然估计是样本的函数。 极大似然估计问题描述首先从一个例子入手，假设我们需要调查某个地区的人群身高分布，那么先假设这个地区人群身高服从正态分布$N(\mu, \sigma ^2)$。注意，极大似然估计的前提是要假设数据总体的分布，不知道数据分布是无法使用极大似然估计的。假设的正态分布的均值和方差未知，这个问题中极大似然估计的目的就是要估计这两个参数。 根据概率统计的思想，可以依据样本估算总体，假设我们随机抽到了1000个人，根据这1000个人的身高来估计均值$\mu$和方差$\sigma^2$。 将其翻译成数学语言：为了统计该地区的人群身高分布，我们独立地按照概率密度$p(x|\theta)$抽取了1000个样本组成样本集$X = x_1, …, x_N$，我们想通过样本集$X$来估计总体的未知参数$\theta$。这里概率密度$p(x|\theta)$服从高斯分布$N(\mu, \sigma ^ 2)$，其中的未知参数是$\theta = [\mu, \sigma]^T$。 那么怎样估算$\theta$呢？ 估算参数这里每个样本都是独立地从$p(x|\theta)$中抽取的，也就是说这1000个人之间是相互独立的。若抽到$i$的概率是$p(x_i|\theta)$，抽到$j$的概率是$p(x_j|\theta)$，那么同时抽到它们的概率就是$p(x_i|\theta)×p(x_j|\theta)$。同理，同时抽到这1000个人的概率就是他们各自概率的乘积，即为他们的联合概率，这个联合概率就等于这个问题的似然函数： L(\theta) = L(x_1, x_2, ..., x_n;\theta) = \Pi ^n_{i=1} p(x_i|\theta), \quad \theta \in \Theta对 L 取对数，将其变成连加的，称为对数似然函数，如下式： H(\theta) = ln L(\theta) = ln \Pi ^n_{i=1} p(x_i|\theta) = \sum^n_{i=1} ln p(x_i|\theta) 为什么要取对数？ 取对数之后累积变为累和，求导更加方便 概率累积会出现数值非常小的情况，比如1e-30，由于计算机的精度是有限的，无法识别这一类数据，取对数之后，更易于计算机的识别(1e-30以10为底取对数后便得到-30)。 对似然函数求所有参数的偏导数，然后让这些偏导数为0，假设有n个参数，就可以得到n个方程组成的方程组，方程组的解就是似然函数的极值点了，在似然函数极大的情况下得到的参数值$\theta$即为我们所求的值： \hat \theta = argmax \ L(\theta)极大似然估计是建立在这样的思想上：已知某个参数能使这个样本出现的概率极大，我们当然不会再去选择其他小概率的样本，所以干脆就把这个参数作为估计的真实值。 极大似然估计的步骤 写出似然函数； 对似然函数取对数，并整理； 求导数，令导数为 0，得到似然方程； 解似然方程，得到的参数。 EM算法初步和极大似然估计一样，EM算法的前提也是要假设数据总体的分布，不知道数据分布是无法使用EM算法的。 概率模型有时既含有观测变量，又含有隐变量。如果概率模型的变量都是观测变量，那么给定数据，可以直接用极大似然估计法，或贝叶斯估计法估计模型参数。但是，当模型含有隐变量时，就不能简单地使用这些估计方法。EM算法就是含有隐变量的概率模型参数的极大似然估计法，或极大后验概率估计法。 $Q$函数：完全数据的对数似然函数$\log P \left( Y , Z | \theta \right)$关于在给定观测数据$Y$和当前参数$\theta_{\left( i \right)}$下对未观测数据$Z$的条件概率分布$P \left( Z | Y, \theta_{\left( i \right)} \right)$的期望 \begin{align*} & Q \left( \theta, \theta_{\left( i \right)} \right) = E_{Z} \left[ \log P \left( Y, Z | \theta \right) | Y , \theta_{\left( i \right)} \right] \end{align*}含有隐变量$Z$的概率模型，目标是极大化观测变量$Y$关于参数$\theta$的对数似然函数，即 \begin{align*} & \max L \left( \theta \right) = \log P \left( Y | \theta \right) \\ & = \log \sum_{Z} P \left( Y,Z | \theta \right) \\ & = \log \left( \sum_{Z} P \left( Y|Z,\theta \right) P \left( Z| \theta \right) \right)\end{align*}EM算法的步骤输入：观测随机变量数据$Y$，隐随机变量数据$Z$，联合分布$P\left(Y,Z|\theta\right) $，条件分布$P\left(Y｜Z，\theta\right) $；输出：模型参数$\theta$ 初值$\theta^{\left(0\right)}$ $E$步： \begin{align*} & Q\left(\theta,\theta^\left(i\right)\right)=E_{Z}\left[\log P\left(Y,Z|\theta\right)|Y,\theta^{\left(i\right)}\right] \\ & = \sum_{Z} \log P\left(Y,Z|\theta \right) \cdot P\left(Z|Y, \theta^\left(i\right)\right)\end{align*} $M$步： \begin{align*} & \theta^{\left( i+1 \right)} = \arg \max Q\left(\theta, \theta^\left( i \right) \right)\end{align*} 重复2. 3.，直到收敛。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>极大似然</tag>
        <tag>EM算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[超越批处理的世界——流式计算（2）]]></title>
    <url>%2Fposts%2F41373%2F</url>
    <content type="text"><![CDATA[超越批处理的世界——流式计算（2） 作者：Tyler Akidau 译者：shenghaishxt 原文：https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-102 介绍欢迎回来！如果你错过了我之前的帖子——超越批处理的世界——流式计算 102，我强烈建议你先阅读它。上一篇帖子是这一篇的基础，而且在这一篇帖子中，我会假设你已经熟悉了之前的术语和概念。 另外请注意，这篇帖子中包含了大量的动画，因此尝试打印它的人可能会错过一些重要的部分。 那么让我开始狂欢吧。我们简单回顾以下，上个帖子中我们关注着三个主要领域：术语，准确定义我在使用“流式传输”这样的术语时的意思；批处理与流处理，比较两种系统的理论功能，并且假设只有两种事情才能使流处理超越批处理：即正确性和时间推理工具；数据处理模式，是研究批处理和刘处理在处理有界和无界数据时采用的基本方法。 在这篇文章中，我希望能够进一步关注上篇文章中的数据处理模式，但是下面会阐述更加详细的内容以及具体的示例。我们将遍历两个主要部分： 流式计算（1）回顾：简要回顾上一篇中引入的概念，然后增加一个示例来突出显示所做的要点。 流式计算（2）：这是流计算（1）的配套部分，详细介绍了在处理无限数据时重要的附加概念，并且将通过示例来解释这些概念。 在我们完成的时候，我们将涉及到强大的无序数据处理的核心原则与概念，这让你能够真正超越经典的批处理。 为了让你了解它们在运行中的意义，我将使用Dataflow中的代码片段（即Google Cloud Dataflow中的API），然后结合动画提供可视化表示。使用Dataflow而不是其他工具（如Spark或是Stiom）的原因是，这些系统都不能够提供我想要表达的东西。好消息是，很多项目正在朝这个方向发展，更好的消息是我们（谷歌）今天提交了一份提案到Apache软件基金会，建议创建Apache Dataflow孵化项目（联合Srtisans、Cloudera、Talend以及其他的一些公司），希望能够围绕强大的无序处理语义来构建更加开放的社区和由数据流模型提供的生态系统。这是个十分有趣的2016年，但是抱歉，我离题了。 这篇文章中缺少了我上次提到的比较部分，我对此感到抱歉。我低估了这篇文章所需要花费的时间。目前，我无法看到任何可以耽搁的部分。如果说有什么事情可以安慰我，那就是我在Strata + Hadoop World Singapore 2015上发表的题为“大规模数据处理的演变”（并且将在6月的Strata + Hadoop World London 2016上发布它的新版本）的演讲，这里的比较部分涵盖了我很多想要解决的内容；而且幻灯片十分精美，在这里使用的话能够增加你的阅读乐趣。 现在，让我们开始流计算吧！ 总结和路线图在流计算101中，我首先澄清了一些术语，从区分有限数据和无限数据开始。有限数据源的大小有限，通常被称作“批处理数据”。无限数据源的大小是无限的，通常被称作“流式”数据。我尽量避免使用批处理和流式传输来修饰数据源，因为这些名称可能是具有误导性的，并且常常受到限制。 然后，我继续解释了批处理和流处理之间的区别：批处理设计是优先考虑有限数据的，而流处理常常考虑到无限数据。我的目标是在描述执行引擎时仅使用批处理和流处理。 在介绍完这些术语之后，我介绍了两个与处理无限数据相关的重要的基本概念。我首先阐述了事件时间（事件发生的时间）和处理时间（事件被系统处理的时间）之间的关键区别。这为Streaming 101中提出的主要论点提供了基础：如果关心事件的实际发生时间，就必须分析与其事件时间相关的数据，而不是它们的处理时间。 然后我介绍了窗口的概念（即按照时间边界切分数据集），这是一种常见的方法，用于对于无限数据源的数据处理，在理论上，无限数据源永远不会消失。窗口策略的一些更简单的例子是固定窗口和滑动窗口。但是更复杂的窗口类型，如会话窗口（其中窗口由数据本身的特征定义，例如捕获每个用户的活动会话窗口，会话窗口之后紧接用户的不活动期）也是比较常见的用法。 除了上一部分中介绍的这两个概念之外，我们现在要仔细研究另外三个新的概念： Watermarks Watermarks是相对于事件时间的输入完整性的概念。它代表一个时间X，表示所有事件时间小于X的数据都已经到齐。因此，在处理无限数据源的时候，Water可以用作进度的度量。 触发器 触发器是一种由外部条件触发，用于表明何时计算窗口结果的机制。触发器在选择何时将计算结果发送给下游提供了灵活性。随着数据不停地到来，窗口能够产生多次输出。这又开启了能够先提供近似结果的大门，并且能够灵活地应对上游数据的变化（可能是上游的数据修正）或者相对于Watermarks延迟的数据（例如一个移动场景，某人的电话离线，在离线的过程中，电话记录了发生的各种动作和事件时间，然后在重新连接的时候上传这些事件进行处理）。 累积 累积模式定义了同一个窗口中观察到的不同结果之间的关系。这些结果之间可能是完全独立的，也可能存在着重叠。不同的累积模式具有不同的语义和计算成本，能够适用于各种各样的用例。 最后，我们将重新回顾一些旧问题，因为我认为它可以更容易地理解所有这些概念之间的关系，并且在回答无限数据处理中的四个问题时探索新的问题，我提出的所有问题在每个无限数据处理问题中都至关重要： What 计算的结果是什么？ 这个问题由管道中的转换来回答。包括计算总和、构建直方图、训练机器学习模型等等。这也是经典批处理回答的问题。 Where 在事件时间中的哪个位置计算结果？ 这个问题由使用事件时间窗口的数据管道来回答。这包括来自Streaming 101（固定，滑动和会话）窗口的常见示例，似乎没有窗口概念的用例（例如，Streaming 101中描述的时间不可知处理；传统的批处理通常也属于这种类别）和其他的更复杂的窗口类型，例如有时间限制的拍卖。需要注意的是，如果将入口时间指定为记录的事件时间，那么它也可以包含处理时间窗口。 When 在处理时间的哪个时刻触发计算结果？ 这个问题用Watermarks和触发器来回答。这个主题的变化性非常大，但是最常见的模式是使用Watermarks来描述给定的窗口的输入完成时间，触发器允许提前计算结果（对于在窗口完成之前发出部分具有推测性的结果）和延迟计算结果（Watermarks仅仅是预估完整性，在Watermarks声明的给定窗口全部到达之后还是有可能到达其他隶属于该窗口的输入数据的）。 How 怎么修正相关结果？ 这个问题由使用的累积类型来回答：丢弃（其中结果是相互独立和不同的），累加（之后的结果建立在先前的结果上），或累加和回收（当前的累加值和先前触发的值一起发送）。 我们将会在之后一一讨论这些问题，我们将引用一些例子来说明，从而清楚地说明哪些概念与What/Where/When/How中的哪个问题有关。 流式计算（1）回顾首先，让我们回顾以下流式计算101中的一些概念，这次我们将会提供具体的例子来将这些概念讲解得更加具体。 What：变换 经典批处理中的转换回答了一个问题：“计算的结果是什么？”尽管可能你们很多人已经熟悉了经典的批处理，但是我们还是以它为起点，因为经典的批处理是其他概念的基础。 在本节中，我们来看一个例子：计算一个由10个整数组成的数据集中的总和。如果你想理解得更加具体一点的话，可以把它看作是一个游戏，一个包含10个人的团队，将每个独立玩家所得到的分数相加就得到了总体的成绩。可以想象的是，对于计费和监控情况使用也同样适用。 对于每个例子，我将包含一小段剪短的Dataflow Java SDK伪代码片段，以使管道的定义更加具体。由于是伪代码，所以我有时会省略一些细节（例如具体使用的I/O源），以及简化名字（Java中当前的触发器名称非常冗长；为了能够清晰的表示，我将使用更为简单的名称）。除了一些不重要的东西之外（大部分我将在Postscript中列举），它基本上是实际情况中的Dataflow SDK代码。对于那些能够编译运行自己的例子的人，我可以提供代码练习的链接。 如果你熟悉Spark Streaming或者Flink这样的计算引擎，那你在熟悉Dataflow代码的问题上应该比较轻松。接下来请看，在Data中有两个基本的原语： PCollections，表示可能进行并行处理的数据集（可能十分庞大，因此名称开始处是P）。 PTransforms，它们被PCollections用来创造新的PCollections。PTransforms能够执行元素的转换，可以将多个元素聚合在一起，或者它们也可以是其他PTransforms的组合。 图1：变换的类型 如果你发现自己感到困惑，或者想要其他的一些参考的话，那么可以看看 Dataflow Java SDK文档。 就我们的例子而言，我们假定我们从一个PCollection]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>流式计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[超越批处理的世界——流式计算（1）]]></title>
    <url>%2Fposts%2F58781%2F</url>
    <content type="text"><![CDATA[超越批处理的世界——流式计算（1） 作者：Tyler Akidau 译者：shenghaishxt 原文：https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101 如今，流式数据处理在大数据中十分重要，这里有几个原因，如下： 商业竞争渴望更加及时的数据，而使用流计算能够有效地实现低延迟。 大量的、无限的数据集在现代商业领域变得越来越常见，如果我们使用专门设计用来处理这些无限数据的系统，那么这样的处理会变得更加轻松。 在数据到达前对其进行流式处理能够实现负载均衡，实现更好的一致性以及可预测的资源消耗。 尽管这样的商业驱动使得人们对于流式计算的兴趣大大增加，但是相对于已经在这个领域取得了许多令人激动的、积极的发展的批处理来说，流式计算仍然表现的不成熟。 对于一个在谷歌的大规模流式计算系统工作了超过五年的员工来说（曾经开发过MillWheel和Cloud Dataflow），我非常开心能够见证如今流式计算的热潮。我也对帮助人们理解关于流计算系统的方方面面以及如何最好地使用它感兴趣，尤其是在现有的批处理系统和流处理系统大多存在语义上的鸿沟的情况下。因此，O’Reilly的编辑邀请我写篇稿子谈谈我于2015年在Strata+Hadoop World伦敦大会上的演讲《对批处理说再见》的一些看法。由于我有相当多的内容需要叙述，因此我把它们分成两个部分： 流式计算（1）：在深入讨论关于时间域和高层次数据处理（包括批处理和流处理）的细节之前，第一篇文章将会涉及一些基本的背景信息并且说清楚一些基础的术语。 数据流模型：第二篇文章主要包括云数据流模型使用的关于统一批处理和流式计算模型的速览，我们通过各种实际例子来进行解读。之后，我会将现有地批处理系统和流处理系统进行一个简洁的语义比较。 背景在开始介绍前，我将会介绍一些重要的背景信息，这将会帮助我们理解剩余一些我们还未讨论的背景知识。我们将会分成三个不同的部分来讨论： 术语：为了能够精确地谈论复杂的术语，我们需要对术语进行精确地定义。对于一些如今已经被滥用了的术语，在谈论到时我也会精确地定义它们。 功能：我会对流式系统的一些经常性的缺点进行评论。我也会提出数据处理系统的建造者们需要的思维框架，从而解决现代数据消费者不断增长的需求。 时间域：我将会介绍两个与数据处理相关的关于时间的主要概念，解释它们是如何联系，并且指出关于这两个时间域带来的难题。 术语在进一步深入之前，我希望先解决一个问题：什么是流计算？如今我们使用“流计算”这个术语来表示着各种各样的事情（简单来说，在某种程度上我在很随意地使用它），这会导致我们对于流计算的误解，或者是我们不知道流计算到底能够做些什么。因此，我宁愿先定义好“流计算”。 问题的关键是，许多术语应该被描述为它们本该成为的样子（例如，无限数据处理，近似结果，等等），但却被描述成它们过去所被描述成的样子（例如，通过流计算执行的引擎）。缺乏在术语上的精确定义模糊了流计算真实的意义，在某些情况下，流计算系统本身还带有某种暗示，暗示着这种系统的能力被限制，常被冠以“流”的特征，例如近似或推测结果。鉴于设计良好的流计算系统与现有的批处理系统相比，同样拥有产生正确性、一致性和可重复性结果的能力，我更喜欢把“流计算”这个术语定义为一个非常明确的意义：一种设计时考虑了无限数据集的数据处理引擎。仅此而已。（为了实现完整性，值得强调的是这个定义包含了真正的流计算和微批处理的实现。） 至于关于流计算的其他一般的用途，这儿有一些我经常听到且每一个都有着更加具有精确性和描述性的术语，作为同一个社区，我建议我们应该采用这些术语。 无限数据：一种持续增长的、实际上是无限的一种数据集。它们经常被称为“流数据”。然而在应用于数据集的时候，流处理或批处理这样的术语是有问题的，如上所说，这就暗示着用某一种类型的执行引擎来处理那些数据。这两种类型的数据集之间的关键区别实际上是它们的有限性，所以最好用术语来描述它们从而研究这种区别。因此，我更喜欢把“流式”数据集称为无限数据集，并且把“批次”数据集称为有限数据集。 无限数据处理：一种不断向前发展着的数据处理模式，应用于前面所提及的无限数据。尽管我个人喜欢用“流”这个术语来描述这种类型的数据，但是在本文再次使用它意味着使用流式处理引擎，这是十分具有误导性的。自从批处理系统被构思出来以后，批处理引擎的循环处理就用于处理无限数据（相反，设计良好的流式系统比批处理系统更有能力处理有限数据）。所以为了使文章更加清晰，我简单将其称为无限数据处理。 低延迟、近似和/或推测结果：这种类型的结果经常与流式引擎联系在一起。在传统上，批处理系统并不是设计来实现低延迟、近似和/或推测结果的，但这并不是说它不可以。当然，批处理引擎更有能力得到近似结果。因此，在讨论了上列术语之后，再来描述这些结果是什么（低延迟、近似和/或推测），而不是通过历史表现（流式引擎）来描述。 在这之后，无论我在哪里用到“流计算”这个术语，你都能够理解为这是一种设计用来处理无限数据集的一种执行引擎，仅此而已。当我使用上述的任何术语时，我将会明确其是无限数据、无限数据处理还是低延迟、近似和/或推测结果。在云数据流中，这些是我们已经采用的术语，我鼓励大家使用相同的术语。 关于流式计算极其夸张的限制下一步，让我们讨论流式系统可以做什么以及不可以做什么，重点是可以做什么。我希望实现的其中一件最重要的事情是讨论一个设计良好的流式系统能够做什么事情。流式系统长期被认为是给市场提供低延迟、近似和/或推测结果，常常与批处理系统结合从而提供最终正确的结果。例如：Lambda架构。 对那些不熟悉Lambda架构的读者来说，其基本思想是在运行流处理系统的同时运行批处理系统，两者都执行本质上一样的计算。流处理系统提供低延迟、不精确的结果（要么是因为使用了近似算法，要么是因为流处理系统本身就不提供正确性），一段时间后批处理系统计算完成并且提供正确的输出。Lambda架构最初由推特的Nathan Marz提出，他是Storm的创始人，这种方法最后取得了很大的成功，事实上，这在当时的确是非常好的主意。不过，流处理系统在正确性方面有些令人失望，而批处理系统又是那样固有的不灵活，所以Lambda就给我们一套现成的方案解决这个问题。不幸的是，维护这个系统比较麻烦，你需要构建、规定和维护两套独立的管道，然后以某种方式融合这两个管道的最终结果。 当我在一个强一致的流引擎中工作了几年之后，我也发现整个Lambda架构是有些问题的。不出所料，当Jay Krep的《质问Lambda架构》未出版前，我就是它的狂热粉丝。下面是反对双模式处理系统的必要性的陈述理由之一。克雷普通过使用可重用系统，例如Kafka作为流计算的连接点来处理重复性问题。这意味着使用一个设计良好的系统来运行管道，处理Lambda的任务。我不认为这个概念本身需要一个名字，但是我原则上完全支持这个主意。 实话说，我希望更进一步。我认为设计良好的流式系统实际上提供一种严格的超集给批处理。模数也许是个效率增量，像今天这样存在着的批处理系统不再必要。Flink的员工基于这种想法建造了一个完全使用流计算的系统，甚至同时支持批处理模式。我爱它。 必然的结果是广泛成熟的流式系统结合了具有鲁棒性的框架能够应用于无限数据，同时，允许Lambda架构回到属于它的大数据历史进程中。我相信这个时刻会成为现实，我们只需要做到这两件事，就可以在批处理擅长的领域打败批处理： 正确性——这使流处理引擎和批处理引擎能够等同本质上，正确性最终归结于一致的存储。流式计算系统需要一种检查长久一致性的方法（Kreps曾在他的文章——《为什么本地状态是流式处理的基础》中谈到过这个问题），由于机器故障仍然存在，这个系统必须被设计得足够好来保持一致性。几年前，当Spark流计算第一次出现在公众大数据领域中，它就像一座灯塔照亮了黑暗的流世界。幸运的是，从那以后情况改善了很多。但是仍然有许多流式系统在没有强一致性的情况下继续尝试取得成功。我完全无法相信“至多一次处理”这样的处理方式依然存在。 再次重申，以下原因十分重要：“只处理一次”这个标准需要强一致性，这是正确性的要求，对于有机会超越批处理系统的流式系统来说，这也是必须的。除非你真正不在意你的结果的正确性，否则我还是建议你避开任何不能提供强一致性的流式系统。如果批处理系统有能力计算出正确的结果，那么它也不会要求你提前检查和确认，别把时间浪费在那些达不到这种能力的流式系统上。 如果你对于在流式系统中如何获取强一致性有疑问，我建议你查阅MillWheel和Spark Streaming里的相关论文。它们都花费了大量的篇幅讲解一致性。考虑到关于这个主题的信息也有大量的文献供参考，在下面的篇幅中我就不会再讨论它了。 时间推理工具——这使流处理引擎超越批处理引擎优秀的时间推理工具对于无限、存在事件时间偏差的无序数据是重要的。越来越多的现代数据集显示出这样的特征，而现在的批处理系统（也包括大部分的流系统）缺乏必要的工具来解决这些特性带来的问题。我将会使用余下的篇幅以及下一章的大部分来重点介绍这个问题。 在开始之前，我们将会理解关于时间域的一些基本概念，之后我们将会深入理解无限、存在事件时间偏差的无序数据的概念。我将会花费本章剩余的篇幅来介绍批处理系统和流处理系统处理数据的一般方法。 事件时间和处理时间对无限数据处理过程的进行强有力的说明需要对于时间域的清晰的理解。在任何数据处理系统中，我们主要关心以下两种时间域： 事件时间，即事件实际发生的时间。 处理时间，即系统中观察事件发生的时间。 不是所有的场景都需要关注事件时间（如果你不需要事件时间的话，那你的工作就轻松多了），但是大部分场景都是需要的。让我们举一些例子吧，比如说在一段时间内描述用户的行为，大多付费应用或者是许多类型的异常检测。 在理想的世界里，事件时间和处理时间永远是相等的，当事件发生的时候它就立即被处理了。然而事实并不是这么简单的，事件时间与处理时间的偏差不只是非零的，而常常是与输入源、执行引擎和硬件有关的一个可变化方程。以下因素能够影响偏差： 共享资源的限制，例如网络拥塞、网络分区或者是非专用环境下的共享CPU。 软件原因，例如分布式系统逻辑和争用等。 数据本身的特征，例如密钥分配、吞吐率变化或无序变化（例如，乘客在飞机落地后才把手机由飞行模式调为正常模式）。 所以，如果你在任何真实世界系统中绘制关于事件时间和处理时间的处理过程图，你通常会得到如同图1中红线一样的内容。 图1：时间域对应的描述图。X轴代表系统中事件时间的完整性，即事件发生在某一刻之前所发生的所有事件。Y轴代表处理时间的过程，即数据处理系统中处理数据时系统的时间。 图中斜率为1的黑线代表理想状态下处理时间和事件时间是相等的，而红线代表着真实情况。在这个例子的初始阶段，处理时间在系统中有一定的延迟，随后在中期趋向于理想状态，而在最后阶段又出现了一些延迟。理想线与红线的水平距离即为处理时间和事件时间之间的偏差。本质上，这种偏差就是由处理管道所引入的延迟。 事件时间和处理时间之间的描述图并不是静态的，这意味着当在管道中观察它们时，如果你关心事件时间（如事件实际上发生的时间）的话，那么你就不能只分析管道中观察到的数据，即处理时间。不幸的是，这现有大多数无限数据处理系统分析数据的方法（即大多数无限数据处理系统都按照处理时间来设计）。为了解决无限数据集无穷的特性，这些系统常常提供一种窗口来将输入数据分块。我们将会在下面深度讨论如何进行分块，但本质上来说，它们都是将一大块数据集按照时间分成有限的块。 如果你在意正确性和数据中的事件时间，那么你不能用处理时间来定义那些数据边界（即处理时间窗口），但是仍然有许多现有的系统这样做。在处理时间和事件时间不存在一致性关联的情况下，有些按照事件时间分块的数据可能会被分到错误的处理时间窗口下（由分布式系统内在的延迟，或是许多在线/离线的数据源类型的延迟引起），在处理时间窗口下，正确性无法保证。我会在下面一篇文章中举大量例子，从而更加详细地讲解这个问题。 不幸的是，当使用事件窗口分块时，得出的图片似乎也不太乐观。在无限数据的环境下，无序和变化的偏差为事件时间窗口引入了完整性问题：在处理时间和事件时间之间缺少可预测的映射，在给定处理时间X的情况下，你如何决定是否已经观察到了所有的数据？对于大多数真实数据源来说，你没有办法。如今使用中的大多数数据处理系统都依赖于某种完整性的概念，但是在应用于无限数据集的时这样的缺点是十分严重的。 我的建议是不再尝试将无限数据分成有限次的最终具有完整性的数据，而是应该设计一种能够允许我们处理不确定的复杂数据的工具。当新数据到来时，老数据将会被撤销或更新，而且我们建造的任何系统都应该有能力解决这些情况，不断将完整性概念进行方便的优化，而不是语义上的需要。 在我们深入讨论如何使用Cloud Dataflow中的Dataflow Model建造这样的系统之前，让我们先了解一个更有用的背景概念：常用的数据处理模式。 数据处理模式目前为止，我们已经拥有了足够的背景，足以让我们开始关注于如今有限数据处理和无限数据处理的常见的核心模型。我们将会在这两种引擎（流计算和批处理）的情况下，着重讨论这两种处理模式，我把微批处理模式与流处理归为一类。因为在这个层面上它们之间的差异并不是十分重要。 有限数据处理有限数据是十分直接的，并且对于每个人来说都很熟悉。在下图中，我们首先从左边的非结构化的数据着手。我们通过一些数据处理引擎来处理它（典型的是批处理，不过设计优秀的流处理也可以完成得同样好，例如MapReduce），然后得到右边新的有着完好结构化的数据以及其内在的价值。 图2：使用经典批处理引擎处理有限数据。左侧的有限非结构化数据在经过数据处理引擎后，得到右侧对应的结构化数据。 当然，当你使用这个模型来进行计算的时候，当中会有无数种变化，但是总体来说这个模型是十分简单的。更有趣的是处理无限数据集的任务。现在让我们看看处理无限数据集的各种典型方式。我们从使用传统的批处理引擎的方法开始，最后以专门为无限数据集设计的系统所使用的方法结束，如流引擎或微批处理引擎。 无限数据——批处理尽管批处理引擎并不是为无限数据量身定做的，但是自从批处理系统被构思出来时，批处理系统就一直用于处理无限数据集。我们可以想象，这样的方法将无限数据切分成一系列的有限数据集，以便其方便为批处理引擎处理。 固定窗口最常见的处理无限数据集的方法多次重复将输入数据分割成一个个固定的窗口，然后将每一个窗口作为一个独立的、有限的数据源进行处理。特别是对于像日志这样的输入数据源，在这里事件能被记录到文件系统中的层级中，日志的名字就对应了它的窗口。其实在数据建立之前，系统实际上就已经基于事件时间把数据记录到对应的时间窗口中。 实际上，大多数系统仍然需要解决完整性问题：倘若由于网络的故障导致你的事件被延迟了怎么办？倘若你的事件在处理之前都要被传送到一个通用的地点怎么办？倘若你的事件是来自移动设备怎么办？这些情况都意味着我们需要用一些特别的方法处理它们（例如延迟处理事件直到你确认所有的事件都已经到达，或者当数据迟到时就在给定的窗口内对所有数据进行再次处理）。 图3：使用经典批处理引擎通过固定窗口处理无限数据。一个无限数据集被分为有限的、窗口固定的有限数据集，然后通过经典的批处理引擎来对其进行连续处理。 会话单元如果你要用批处理引擎将无线数据划分为更加复杂的窗口（如会话单元），以上方法会失效。会话单元通常被定义为活动（例如特定的用户）的周期，以一段不活跃的时间来作为结束的标志。当使用经典的批处理引擎来计算会话单元时，你常常会看到会话单元被分到不同的批次中，如下图红色的标注所示。这些裂缝的数量可以通过增加批次的大小来减少，但这样做的话会增大延迟。另一个选择是增加额外的逻辑来拼接上一批的会话单元，但是这样会大大增加复杂度。 图4：使用经典批处理引擎通过固定窗口处理无限数据。一个无界的数据集被收集到有限的固定大小的有界数据窗口中，然后通过连续运行一个经典的批处理将这些有界数据划分为动态会话窗口。 无论使用哪种方式，使用批处理引擎计算会话窗口并不是十分理想的。更好的方法是以流的方式建立会话，我们将会在之后进行讲解。 无限数据——流式计算与基于批处理的无限数据处理方法的临时特性相反，流系统专门为无限数据所建造。我先前说过，在大多数真实世界里分布式输入源中，你不仅仅需要解决无限数据的问题，还要解决： 基于事件时间的高度无序性，这意味着如果你想要按照事件时间来分析数据的话，你需要处理时序问题。 事件时间变化的时间差，这意味着在一个常量Y时间内，你无法假设看到对应给定的事件时间X内发生的所有数据。 以下有多种能够解决这样特性的数据的方法，我通常把它们归为以下几类： 时间不可知 近似算法 按处理时间分片 按事件时间分片 我们现在将花费一些时间来讲解每一种方法。 时间不可知时间不可知处理本质上跟时间没有关系，其所有的关联逻辑是数据本身。这些情况下只关心更多数据的到达，因此并不需要使用流引擎对其进行特殊的支持，只需要保证基本的数据传送就足够了。所以，本质上所有的流系统都支持时间不可知（对于那些对于正确性有要求的场景来说，还需要排除不支持强一致的系统）。通过简单地将无限数据源切分成一系列有限数据集，然后独立地处理这些有限数据集，批处理系统也同样适用于处理时间不可知的无限数据源场景。我们将会讲解一组这个领域中的具体的例子，但由于处理时间不可知过程比较简单，我们不会在这上面花太多时间。 过滤时间不可知处理过程的一个非常基础的形式是过滤。想象一个画面，你在处理网络流量日志，并且你想要过滤掉所有不来自某个特殊域的所有流量。那么你只要在每个记录到达时，判定其是否来自那个特殊域，来决定是留下还是丢弃。在任何时间内，它都只决定于数据本身，与数据源是否是无限的、是否是无序的以及事件的时间偏差没有任何关系。 图5：无限数据的过滤。从左侧流向右侧的不同类型的数据在经过过滤后，成为均匀的、只包含一种类型的数据。 内连接另一种时间不可知的例子是内连接（又名哈希连接）。当连接两个无限数据源时，如果你只关心这两个无限数据源中共有的元素，那么它们的逻辑便是与时间无关的。当你从其中一个数据源中得到一个值后，只需要将它缓存在一个持久的缓存里，然后等到另外一个数据源也传来这个值，然后输出它们。（事实上，你可以会希望有一种垃圾回收装置来处理那些没有出现过的的与时间有关的连接元素。但是对于那些几乎不出现不完全连接的例子，这些就是小问题了。） 图6：对两个无限数据源执行内连接。在两个数据源中都观察到相同的匹配元素时，就执行内连接。 如果语义成为了外连接，那么之前说过的完整性问题又会出现：当你看到连接的一边，那你怎么能确定另外一边能否出现？我必须告诉你，你绝对不知道，所以你不得不引入某种超时装置，这就又涉及到了时间。这种时间本质是时间窗口分片，我们一会儿将会仔细研究它。 近似算法 图7：计算无限数据的近似值。数据经过复杂的算法，会产生看起来或多或少像另一侧的预期结果的输出数据。 第二类算法是近似算法，例如近似Top-N算法、流式K-means算法等等。它们都是输入无限数据，然后提供给你输出数据。这些近似算法的优点是在通过设计之后，它们的开销比较低，适合用于处理无限数据。但它们也有缺点，缺点是它们的数量有限，且实现复杂。近似的特性同时限制了它们的实用性。 值得一提的是，这些算法本质上是有一些时间域的特性（例如，某种衰退机制）。与此同时，这些方法一般都在数据到达之后进行处理，因此它们通常是用处理时间。对于那些能够提供证明错误范围的算法来说，这是十分重要的。如果算法能够通过数据到达的顺序来预测错误的界限，那么就算是事件-时间漂移有变化，对于无限数据来说都是可以忽略不计的了。这是需要注意的一点。 近似算法本来是一个很有趣的话题，但是本质上近似算法是一种时间不可知（如果不考虑它们自身的时间特征的话）。它们使用起来相当简单，所以我们不再详细介绍了。 时间窗口分片其他两个无限数据处理的方法也是事件窗口分片。在深入介绍它们的差异之前，我会花一些时间来讲清楚时间窗口分片的具体含义。对于一个输入数据源来说（无论是有限还是无限），分片就是按照时间区间把数据分成有限片再进行处理。下图展示了三种不同的分片模式。 图8：不同的分片模式。每个例子都包括了三个不同的键，并且突出显示了窗口对齐（对于所有的数据都适用）以及窗口不对齐（只适用于数据子集）。 固定窗口：固定窗口按照固定长度的时间进行分片。通常情况下（如图8所示），固定窗口的分段适用于所有数据集，这叫做对齐的窗口。在某些情况下，我们会希望对于不同的数据子集进行不同的相位偏移，从而让分片的完整度更加均匀。这就是非对齐窗口的一个示例，因为它们在数据之间变化。 滑动窗口：滑动窗口可以看做是固定窗口更一般的一个形式。滑动窗口由两个量来定义：固定长度和固定周期。如果滑动时间比窗口小，那么窗口重叠。如果滑动时间等于窗口，那就是固定窗口。如果滑动时间比窗口大，那么就会出现一种奇怪的采样窗口，即按照时间来看数据集的一部分子集数据。类似于固定窗口，滑动窗口通常是对齐的。但是在某些情况下可能会不对齐，这是为了性能的优化。请注意，图8中的滑动窗口是为了给出滑动的感觉来绘制的；实际上，所有的五个窗口都适用于整个数据集。 会话单元：它是动态窗口的一个实例。会话是在不活跃时间段的一连串事件，这个不活跃时间段通常比设定的超时时间长。会话单元通常用于将一系列与时间相关的事件（例如一次观看的视频序列）分组在一起来随时分析用户的行为。会话单元很有趣，因为它们的长度无法事先定义，这完全取决于涉及的实际数据。会话单元也是非对齐窗口的一个标准示例，因为在实际的情况下，不同子集数据的会话单元长度几乎不可能一致地对齐。 我们讨论的两个领域——处理时间和事件时间是我们关心的两个领域。窗口化在这两个领域都是有意义的，因此我们将详细地讨论每个领域，看看它们有什么不同。由于按照处理时间进行窗口分片是最普遍的，我们就从这里开始。 按照处理时间做时间窗口分片 图9：按照处理时间做时间窗口分片。根据它们到达管道的顺序将数据收集到窗口中。 当按照处理时间做时间窗口分片时，系统本质上是将输入的数据进行缓存，在经过一定的处理时间窗口之后再对缓存好的数据进行处理。例如，在一个5分钟的固定窗口中，系统会按照自己的系统时间缓存5分钟以内的数据，然后将这5分钟内的数据视为一个窗口，交由下一步的流程进行处理。 用处理时间窗口分片有如下几个很好的属性： 简单。实现起来十分简洁，你不用担心随着时间推移数据会失去顺序，只需要在到达时将数据缓存，并在窗口关闭时将它们发送到下一步即可。 判断窗口的完整性很简单。因为系统可以清楚地知道某窗口中的数据是否已经全部被看到，所以数据的完整性很容易保证。这意味着，当通过处理时间做时间窗口分片时，系统不需要以任何方式处理那些“迟到的”数据。 如果你关心的是事件被观察到后的信息，那么按照处理时间做时间窗口分片就正是你所需要的方法。很多监控场景都属于这一类。比如你希望可以获得某个网站的每秒请求量，再通过监控这个数量来判断网站是否有服务中断，这时用处理时间做时间窗口分片就是最好的选择。 除了这些优点之外，这种方法也存在一个巨大的缺点，即如果需要处理的数据具有与其相关的事件时间，而时间窗口需要反映数据的事件时间，那么这些数据就必须以事件时间的顺序到达了。不幸的是，现实世界中，按照事件时间排序并到达的数据几乎是没有的。 我们来举一个简单的例子，想象一个手机中的应用程序收集使用统计信息以供后期分析。当手机在离网一段时间后（比如离开网络、处于飞行模式等），这期间记录的数据就需要等到手机接入网络后才能够上传。这就意味着数据可能会以几分钟、几小时、几天、几周甚至更长的事件时间或者处理时间偏差到达。这时用处理时间做时间窗口分片就无法对这样的数据进行处理。 再举一个例子，许多分布式的数据源在系统正常的情况下能够提供有序的事件时间的数据（或是接近有序）。不幸的是，在系统健康得不到保证的时候就很难保证有序性了。比如某个处理多个大陆收集的数据的全球业务，洲际间的网络带宽一般都会受限（这是很常见的），这时突然间一部分的输入数据会比通常情况下更加晚到。如果继续使用处理时间对数据做时间窗口分片，就无法有效反映出数据实际发生时的情景。相反，这时窗口内的数据是一些任意组合的新旧数据。 在这两种情况下，我们想要按照事件到达的顺序按照事件时间进行时间窗口分片，这样才能保证数据到达的有序性。我们真正想要的是事件的时间窗口。 按照事件时间做时间窗口分片当你需要将事件按照发生时的时间分进有限的块内，那么就需要用到事件时间窗口。这是时间窗口分片的黄金标准。令人遗憾的是，目前大多数数据处理系统都缺乏对齐的本地支持（尽管支持强一致的系统（如Hadoop或Spark Streaming）经过修改之后能够支持这种方法）。 下图显示了一个将无限数据按照事件时间分片的实例。 图10：按照事件时间用固定窗口分片。根据数据发生的时间将数据收集到窗口中。白色箭头将事件时间属于同一个分片的数据放到同一个窗口中去。 图中的白色箭头线对应着两个特别的数据。这两个数据都到达了处理时间窗口，但是与它们所属的事件时间窗口不匹配。因此，如果是按照处理时间来分片处理，但是我们关心的是事件发生时的信息，那么计算结果是不正确的。正如人们所期望的那样，用事件时间分片来保证事件时间计算的正确性是很好的。 这个方法来处理无限数据的另外一个好处就是你可以使用动态大小的窗口（如会话单元），不会出现在前面使用批处理引擎的时候，会话被分到两个窗口内的情况。 图11：按照事件时间窗口用会话单元做窗口分片。将数据按照它们发生的时间和活动性收集到不同的会话窗口内。白色箭头将那些属于同一个分片的数据放到同一个会话窗口中，按照正确的事件时间排序。 当然，强大的语义并不是免费的。按事件时间做时间窗口分片也不例外。由于窗口必须经常比窗口本身的实际长度长，所以事件时间窗口有两个明显的缺点： 缓存：由于延长了窗口的使用寿命，需要更多的数据缓存。值得庆幸的是，持久性存储已经是大多数数据处理系统中最便宜的（其他的是CPU、网络带宽和RAM）。因此，这个问题没有想象中的那么严重。而且，许多有用的聚合不要求将整个输入集缓存起来（如总和、平均值），而是只要把中间的计算结果缓存下来然后递增地累积就可以了。 完整性：考虑到我们往往无法判定是否已经收集到了一个窗口中的所有数据，那么我们如何知道什么时候才能将窗口中的数据交给下游去处理呢？事实上，我们根本就不知道。对于很多类型的输入，系统可以通过类似MillWheel的水印（我们将在第二部分详细讨论它）给出合理准确的完整性估计。但是对于正确性要求极高的场景中（如计费），唯一真正的选择是提供一个方法来让引擎决定什么时候交出数据，以及如何让系统不断地修正结果。处理窗口内数据（或者缺少窗口）的完整性是一个十分令人感兴趣的话题。但是最好能够在一个具体的例子中来讨论说明，我们下次再介绍。 结论好吧！这篇包含太多信息了。如果你已经读到这里的话：你应该受到表扬！在这一点上，我想这些大概是我想介绍内容的一半。我们可以退一步，来回顾一下我们目前为止所学到的内容，并且在进入第二部分之前解决这些内容。尽管第一部分有些无聊，但令人兴奋的是，第二部分是乐趣真正开始的地方。 回顾总结一下，目前我已经介绍了以下几点： 澄清术语，特别是将“流计算”的定义缩小为仅适用于执行引擎，同时使用了更多描述性术语，如将“无限数据”和近似/推测算法都放在流计算的概念下。 分析了精心设计的批处理系统和流计算系统，总结出流计算系统是批处理系统的功能超集，而像Lambda架构这样的概念最终会被流计算取代。 提出了流计算系统所需的重要的两个概念，能够帮助流计算追赶并最终超批处理：完整性和时间工具。 确定了事件时间和处理时间之间的重要差异，描述了这些差异在分析数据时出现的困难。根据完整性的概念，提出系统应该适应时间上的变化，提供完整、精确的结果。 分析了针对无限数据和有限数据的常用数据处理方法，主要通过批处理和流计算。并且将无限数据的处理分为四类：时间不可知、近似、通过处理时间进行窗口分片、通过事件时间进行窗口分片。 下一步的内容本文提供了我们将在第二部分进行探讨的具体示例的基础。第二部分大致包含以下内容： 从数据处理的概念上看，我们将从四个角度入手：什么、何处、何时以及怎么做。 详细介绍如何在多个场景中处理简单、具体的示例数据集，突出显示数据流模型支持的多个用例以及涉及的具体API。这些例子将有助于推动本文介绍的事件时间和处理时间的概念，同时还将探索新的概念，如watermarks。 比较现有的数据处理系统的重要特征，让我们更好地选择它们，并且鼓励大家对它们进行改善，帮助实现我们的最终目标：让流计算成为大数据处理的最好的方式。 现在是个好时机。回见！]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>流式计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用PicGo和Github搭建个人图床]]></title>
    <url>%2Fposts%2F32287%2F</url>
    <content type="text"><![CDATA[一直在寻找好用而且稳定的图床，最近微博图床也挂了，那些个人的私有免费图床不太敢用，感觉指不定哪一天作者就退网了，风险太大。今天发现PicGo这个好用的工具，搭配Github仓库可以实现一个属于你自己的个人图床，再也不用担心图片丢失的问题啦~ 新建Github公有仓库首先登陆Github建立一个专门来存放图片的公有仓库，注意必须是公有仓库，否则图片url是无法共享出去的，如下图所示 下载安装PicGoPicGo的下载地址：https://github.com/Molunerfinn/PicGo/releases 安装之后打开的界面如下图所示 点开图床设置里的GitHub图床，设置好仓库名，分支名和Token，下面是我的设置，怎么获取Token网上都可以查得到，就不再讲了 PicGo使用PicGo的使用是非常方便的，可以从剪贴板里上传，也可以从路径上传或者是直接拖拽，就可以直接获取到链接了，而且链接格式也支持好多种，常用的一般是Markdown和URL。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>图床</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K-均值聚类算法]]></title>
    <url>%2Fposts%2F59577%2F</url>
    <content type="text"><![CDATA[K-均值聚类算法算法流程K-均值算法首先随机确定k个初始点作为质心，然后将数据集中的每个点寻找其最近的质心，并将其分配给该质心所对应的簇。这一步完成后，每个簇的质心更新为该簇所有点的平均值。 这个过程可以用伪代码表示如下： 1234567创建k个点作为起始质心（经常是随机选择）当任意一个点的簇分配结果发生改变时 对数据集中的每个数据点 对每个质心 计算质心与数据点之间的距离 将数据点分配到其最近的簇 对每一个簇，计算簇中所有点的均值并将均值作为质心 库导入及数据可视化1234import numpy as npimport matplotlib.pyplot as pltfrom sklearn.cluster import KMeansfrom sklearn import datasets 使用sklean自带的莺尾花数据集，取其中的两个维度对其进行可视化： 1234iris = datasets.load_iris()X, y = iris.data, iris.targetdata = X[:,[1,3]]plt.scatter(data[:,0],data[:,1]) 模型训练及可视化 n_clusters: 即k值，代表簇的个数 max_iter：最大的迭代次数，这里设置为1，代表算法在第一次循环过后的结果， init：代表初始质心的选择方式，这里使用‘random’代表随机选择 n_init：代表用不同的初始质心运行算法的次数，这里设置为1定义好分类器后，可以得到算法第一次迭代的结果，并对其可视化 随机确定k个初始点作为质心，然后将数据集中的每个点寻找其最近的质心，并将其分配给该质心所对应的簇。然后将每个簇的质心更新为该簇所有点的平均值，得到centers1。 1234567estimator1 = KMeans(n_clusters=2, max_iter=1, init='random', n_init=1)y_pred1 = estimator1.fit_predict(data)centers1 = estimator1.cluster_centers_print(centers1)plt.scatter(data[:, 0], data[:, 1], c=y_pred1)plt.scatter(centers1[:, 0], centers1[:, 1], marker='x', c="black", s=300)plt.show() [[2.62666667 0.78 ] [3.165 1.30416667]] 在第二次迭代时，将初始质心设置为上次迭代后更新的质心centers1，重复第一次迭代时的过程，得到centers2。 1234567estimator2 = KMeans(n_clusters=2, max_iter=1, init=centers1, n_init=1)y_pred2 = estimator2.fit_predict(data)centers2 = estimator2.cluster_centers_print(centers2)plt.scatter(data[:, 0], data[:, 1], c=y_pred2)plt.scatter(centers2[:, 0], centers2[:, 1], marker='x', c="black", s=300)plt.show() [[2.99821429 0.52142857] [3.09255319 1.60319149]] 同样，在第三次迭代时，也将初始质心设置为上次迭代后更新的质心centers2，之后的迭代过程与之类似，以此类推。 1234567estimator3 = KMeans(n_clusters=2, max_iter=1, init=centers2, n_init=1)y_pred3 = estimator3.fit_predict(data)centers3 = estimator3.cluster_centers_print(centers3)plt.scatter(data[:, 0], data[:, 1], c=y_pred3)plt.scatter(centers3[:, 0], centers3[:, 1], marker='x', c="black", s=300)plt.show() [[3.25666667 0.37666667] [2.92444444 1.74777778]] 可以看到，在三次迭代之后，K-均值算法就已经成功地将此数据集成功地聚类了。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker容器的SSH服务配置]]></title>
    <url>%2Fposts%2F1597%2F</url>
    <content type="text"><![CDATA[新建Docker容器举个例子，将镜像名字改为你自己的就可以： 1sudo nvidia-docker run -p 5592:5592 -p 5593:5593 -p 10022:22 --name ocr -it deeplearning:v1 bash 配置Docker的SSH服务检查容器内是否安装openssh-server和openssh-client12apt-get install openssh-serverapt-get install openssh-client 修改SSH配置文件1234567vim /etc/ssh/sshd_config# 修改以下选项# PermitRootLogin prohibit-password # 默认打开 禁止root用户使用密码登陆，需要将其注释RSAAuthentication yes #启用 RSA 认证PubkeyAuthentication yes #启用公钥私钥配对认证方式PermitRootLogin yes #允许root用户使用ssh登录 启动SSHD服务1/etc/init.d/ssh restart 在宿主机内测试是否能够通过SSH连接上容器执行以下命令，然后输入密码，若能进入容器内即配置成功 1ssh root@127.0.0.1 -p 10022 提交修改后的容器到镜像可将修改后的容器保存为镜像，以免后续重复操作，在宿主机内执行： 1docker commit ocr_0421 deeplearning:v1]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Learning Deep Features for Discriminative Localization 笔记]]></title>
    <url>%2Fposts%2F52825%2F</url>
    <content type="text"><![CDATA[引言最近有研究已经表明，尽管没有对物体的位置进行监督，但CNN各个层的卷积单元实际上是物体检测器。虽然卷积层定位物体的能力很出色，但是当最后一层使用全连接层进行分类时，这种能力会丧失。最近一些流行的神经网络，比如Network in Nextwork (NIN) 和GoogleNet，已经提出通过避免使用全连接层来保持高性能。 因此，NIN使用全局平均池化来做正则化，避免训练过程中的过拟合。全局平均池化还有另一个优点，只需要稍微调整就可以使网络保持很好的定位能力。如下图所示，在物体分类上训练的CNN能够成功地定位与人类交互的物体而非人物本身。 尽管我们的方法显得很简单，但在ILSVRC弱监督物体定位比赛中，我们最好的网络在测试集中与全监督的AlexNet十分接近。 近期研究弱监督物体定位与我们的方法最相近的是一个基于全局最大池化的方法，这个方法的定位局限于物体边缘的点，而不是确定物体的全部范围。尽管max和average很相似，但是平均池化更鼓励网络识别完整的物体区域。原理是相比于最大池化，平均池化网络识别整个物体的辨别性区域的损失更小。 全局平均池化不是我们提出来的新技术，我们的创新点在于用它来精确定位区别性区域。 可视化卷积神经网络最近有研究发现CNNs在训练识别场景时会学习物体侦测，并证明了统一网络既能进行场景识别，又能进行物体定位。这些研究都只分析了卷积层，忽略了全连接层，因此是不全面的。 还有研究发现通过转化不同层的深度特征，可以分析CNN的可视化解码。虽然这些方法可以转化全连接层，但是只展示了深层特征中什么信息被保留，没有凸显出这些信息的重要性。不同于这些方法，我们的方法能凸显出图片的辨别性区域。总的来说，我们的方法是对CNN深入灵魂的呈现。 类激活映射下图是用CNN的全局平均池化来生成class activation maps (CAM)的过程。某一确切分类的CAM代表了CNN用于做分类时所看到的区别性区域的位置。 下面是ILSVRC中四个分类的CAM图。这些图将图片中用于分类的区别性区域高亮标记。 我们使用了与NIN和GoogleNet相似的网络结构——网络的大部分都是卷积层，只在输出层（用于分类的softmax）前使用了全局平均池化层，并将它们作为得出分类的全连接层的特征。通过这种简单的连接结构，我们可以把图片中的重要区域用输出层权重映射回卷积层特征的方式标记出来，我们称这种技术为类激活映射(CAM)。 看回第一张图，全局平均池化层输出最后一个卷积层的每个单元的特征图的平均值，这些值的加权总和用于生成最后的输出。也就是说，我们计算最后一个卷积层特征图的加权总和来生成CAM。 弱监督物体定位这一部分，我们在ILSVRC2014数据集上评估了CAM的定位能力。我们先描述了实验的设置和用到的CNN网络，然后验证了我们的技术不会在学习定位时对网络产生不利影响，并详细给出了弱监督对象定位的结果。 我们使用AlexNet、VGGnet和GoogleNet这些流行的CNN网络评估CAM的影响。我们通常把这些网络的全连接层全部移除，替换为GAP，然后接一个softmax全连接层。 我们发现当GAP前的最后一个卷积层有较高空间分辨率时，网络的定位能力可以得到改善，我们称为图分辨率。因此，我们移除了一些网络的卷积层。具体地，对AlexNet，我们移除conv5之后的卷积层（pool5到prob），得到图分辨率（mapping resolution）为13x13。对VGGnet，移除了conv5-3后的所有卷积层（pool5到prob），得到14x14的图分辨率。对GoogLeNet，移除了inception4e后的卷积层（pool4到prob），得到14x14的图分辨率。对上述的每个网络，我们都添加一个3x3，步长为1，padding为1, 1024个单元的卷积层，然后接一个GAP层和一个softmax层。最后对每个网络在ILSVRC的1.3M张要分成1000类的训练图片进行精调（fine-tuned），分别产生我们最终的AlexNet-GAP，VGGnet-GAP和GoogLeNet-GAP。 总结本文针对使用GAP的CNN网络提出了名为CAM的技术，它能够让经过训练的CNN学会物体定位。而且，CAM定位技术能够推广到其他视觉识别任务中，这是最大的利好。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>弱监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最近邻插值和双线性插值的算法实现]]></title>
    <url>%2Fposts%2F48467%2F</url>
    <content type="text"><![CDATA[有时，在进行图像处理时，将源图像的一个像素对应到目标图像上时，对应目标图像上的像素坐标是小数，那么这个时候就需要进行插值了，本文给出了两种图像旋转时常用的插值算法的实现。 灰度最近邻插值首先来看看最近邻插值，这最简单的插值算法，不需要计算，只需把与待求坐标的相邻四个像素中最近的坐标的像素值赋值给待求像素。 双线性插值双线性内插法是利用待求象素四个邻象素的灰度在两个方向上作线性内插。 实现代码下面贴出我基于OpenCV的一个简单代码实现。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768class Interpolation&#123;public: Mat nearest(cv::Mat img, float angle) &#123; int len = (int)(sqrtf(pow(img.rows, 2) + pow(img.cols, 2)) + 0.5); Mat retMat = Mat::zeros(len, len, CV_8UC1); float anglePI = angle * CV_PI / 180; int xSm, ySm; for(int i = 0; i &lt; retMat.rows; i++) for(int j = 0; j &lt; retMat.cols; j++) &#123; xSm = (int)((i-retMat.rows/2)*cos(anglePI) - (j-retMat.cols/2)*sin(anglePI) + 0.5); ySm = (int)((i-retMat.rows/2)*sin(anglePI) + (j-retMat.cols/2)*cos(anglePI) + 0.5); xSm += img.rows / 2; ySm += img.cols / 2; if(xSm &gt;= img.rows || ySm &gt;= img.cols || xSm &lt;= 0 || ySm &lt;= 0)&#123; retMat.at&lt;uchar &gt;(i, j) = 0; &#125; else&#123; retMat.at&lt;uchar &gt;(i, j) = img.at&lt;uchar &gt;(xSm, ySm); &#125; &#125; return retMat; &#125; Mat bilinear(cv::Mat img, float angle) &#123; int len = (int)(sqrtf(pow(img.rows, 2) + pow(img.cols, 2)) + 0.5); Mat retMat = Mat::zeros(len, len, CV_8UC1); float anglePI = angle * CV_PI / 180; float alpha, beta; for(int i = 0; i &lt; retMat.rows; i++) for(int j = 0; j &lt; retMat.cols; j++) &#123; float resu = (i-retMat.rows/2)*cos(anglePI) - (j-retMat.cols/2)*sin(anglePI) + img.rows / 2; float resv = (i-retMat.rows/2)*sin(anglePI) + (j-retMat.cols/2)*cos(anglePI) + img.cols / 2; int u = (int) resu; int v = (int) resv; if (resu &gt;= u) alpha = resu - u; else alpha = u - resu; if (resv &gt;= v) beta = resv - v; else beta = v - resv; // cout &lt;&lt; alpha &lt;&lt; ' ' &lt;&lt; beta &lt;&lt; endl; if(u &gt;= img.rows || v &gt;= img.cols || u &lt;= 0 || v &lt;= 0)&#123; retMat.at&lt;uchar &gt;(i, j) = 0; &#125; else&#123; int res = (1 - alpha) * (1 - beta) * img.at&lt;uchar &gt;(u, v) + alpha * (1-beta) * img.at&lt;uchar &gt;(u+1, v) + (1 - alpha) * beta * img.at&lt;uchar &gt;(u, v+1) + alpha * beta * img.at&lt;uchar &gt;(u+1, v+1); retMat.at&lt;uchar &gt;(i, j) = res; &#125; &#125; return retMat; &#125;&#125;;]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Clion上配置OpenCV C++库]]></title>
    <url>%2Fposts%2F37464%2F</url>
    <content type="text"><![CDATA[昨天粗略查了一下OpenCV C++库在Win下的配置，得知目前有两种使用OpenCV库的方法，一种是使用官方已经编译好的文件，但这是在IDE是VS2015或VS2017的情况下。我使用的是Jetbrains家族的Clion，只能自己进行编译，实在感到有点麻烦，因为如果自己进行编译的话，由于各种各样的原因，可能会有许多报错，而且网上的教程坑又特别多，所以这里记录一个极简版的配置过程，省去编译的过程。 我的环境：Win10，MingGW-W64 6.0，Clion 2019.1。 下载OpenCV源码和编译后的文件下载编译后的文件一开始我是自己编译的，使用的是CMake-64 3.6.0和MingGW-W64 6.0，编译了好久，报各种错误，最后还是放弃了。本着能不折腾就尽量不折腾的原则，还是下载开源社区中别人编译好的源码吧。这里给出一个作者的项目，OpenCV库版本是3.4.1，当然Github上应该还有其他类似的项目，可以下载其他版本，道理都是相通的。这个是网址：https://github.com/zxxf18/OpenCV341-MinGW-Build-x64 。如果想折腾的话可以自己试着编译一下，作者也给出了几个常见的错误，可以对照着排除。打开这个页面，点击右上角绿色的小图标，Clone or download，然后点击Download ZIP就可以下载了。 下载OpenCV源码下载OpenCV源码，选择3.4.1版本的Sources，给出OpenCV的官网：https://opencv.org/releases.html 。 配置环境首先建立一个oepncv文件夹，在里面建立两个文件夹，一个是mingw-build，用于放置编译后的文件，另一个是sources，放置刚才下载的源码。我这里的目录是 C:\opencv\mingw-build 和 C:\opencv\sources 。可以参考，不一定要在C盘，只要环境变量配置对，系统能够找到OpenCV，随便任何的一个位置都是可以的。 添加环境变量首先在系统变量中添加一个变量，变量名为OpenCV_DIR，变量值为 C:\opencv\mingw-build 然后在系统变量的Path中添加 C:\opencv\mingw-build\bin 让Clion调用OpenCV最后一步是让IDE能够正常调用OpenCV库，给出我的CMakeLists.txt： 12345678910111213cmake_minimum_required(VERSION 3.13)project(DIP)set(CMAKE_CXX_STANDARD 14)add_executable(DIP main.cpp)set(OpenCV_DIR C:\\opencv\\mingw-build)find_package(OpenCV REQUIRED)target_link_libraries(DIP $&#123;OpenCV_LIBS&#125;) 然后看看cpp文件中的 #include 会不会报错吧！若正常显示，那么就代表正式配置成功了。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[修改Linux默认python版本]]></title>
    <url>%2Fposts%2F28999%2F</url>
    <content type="text"><![CDATA[查看Python版本首先列出系统中所有Python版本： 1ls /usr/bin/python* 查看默认的Python版本： 1python --version 基于软链接的系统级修改先删除默认的Python软链接： 1sudo rm /usr/bin/python 然后创建一个新的软链接指向需要的Python版本： 1sudo ln -s /usr/bin/python3.5 /usr/bin/python]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CART算法]]></title>
    <url>%2Fposts%2F31705%2F</url>
    <content type="text"><![CDATA[分类与回归树（classification and regression tree, CART）模型是应用广泛的决策树学习方法，同样由特征选择、树的生成和剪枝组成，既可以用于分类也可以用于回归。 CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。 CART算法主要由以下两步组成： 决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大。 决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。 CART生成决策树的生成就是递归地构建二叉决策树的过程，对回归树用平方误差最小化准则，对分类树用基尼系数最小化准则，进行特征选择，生成二叉树。 回归树的生成假设$X$与$Y$分别为输入和输出变量，并且$Y$是连续变量，给定训练数据集 \begin{align*} \\ & D = \left\{ \left(x_{1},y_{1}\right), \left(x_{2},y_{2}\right),\cdots,\left(x_{N},y_{N}\right) \right\} \end{align*}可选择第$j$个变量$x_{j}$及其取值$s$作为切分变量和切分点，并定义两个区域 \begin{align*} \\ & R_{1} \left( j,s \right) = \left\{ x | x_{j} \leq s \right\}, \quad R_{2} \left( j,s \right) = \left\{ x | x_{j} > s \right\} \end{align*}然后寻找最优切分变量$x_{j}$及最优切分点$s$，具体地，求解 \begin{align*} \\ & j,s = \arg \min_{j,s} \left[ \min_{c_{1}} \sum_{x_{i} \in R_{1} \left(j,s\right)} \left( y_{i} - c_{1} \right)^{2} + \min_{c_{2}} \sum_{x_{i} \in R_{2} \left(j,s\right)} \left( y_{i} - c_{2} \right)^{2}\right] \end{align*}其中，$c_{m}$是区域$R_{m}$上的回归决策树输出，是区域$R_{m}$上所有输入实例$x_{i}$对应的输出$y_{i}$的均值 \begin{align*} \\ & c_{m} = ave \left( y_{i} | x_{i} \in R_{m} \right), \quad m=1,2 \end{align*}对每个区域$R_{1}$和$R_{2}$重复上述过程，将输入空间划分为$M$个区域$R_{1},R_{2},\cdots,R_{M}$，在每个区域上的输出为$c_{m},m=1,2,\cdots,M$，最小二乘回归树可表示为 \begin{align*} \\ & f \left( x \right) = \sum_{m=1}^{M} c_{m} I \left( x \in R_{m} \right) \end{align*}最小二乘回归树生成算法输入：训练数据集$D$输出：回归树$f \left( x \right)$ 选择最优切分变量$x_{j}$与切分点$s$ \begin{align*} \\ & j,s = \arg \min_{j,s} \left[ \min_{c_{1}} \sum_{x_{i} \in R_{1} \left(j,s\right)} \left( y_{i} - c_{1} \right)^{2} + \min_{c_{2}} \sum_{x_{i} \in R_{2} \left(j,s\right)} \left( y_{i} - c_{2} \right)^{2}\right] \end{align*} 用最优切分变量$x_{j}$与切分点$s$划分区域并决定相应的输出值 \begin{align*} \\ & R_{1} \left( j,s \right) = \left\{ x | x_{j} \leq s \right\}, \quad R_{2} \left( j,s \right) = \left\{ x | x_{j} > s \right\} \\ & c_{m} = \dfrac{1}{N} \sum_{x_{i} \in R_{m} \left( j,s \right)} y_{i}, \quad m=1,2\end{align*} 继续对两个子区域调用步骤1.和2.，直到满足停止条件 将输入空间划分为$M$个区域$R_{1},R_{2},\cdots,R_{M}​$，生成决策树 \begin{align*} \\ & f \left( x \right) = \sum_{m=1}^{M} \hat c_{m} I \left( x \in R_{m} \right) \end{align*} 分类树的生成分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点。 分类问题中，假设有$K$个类，样本点属于第$k$类的概率为$p_{k}$，则概率分布的基尼指数定义为： \begin{align*} \\ & Gini \left( p \right) = \sum_{k=1}^{2} p_{k} \left( 1 - p_{k} \right) = 2p\left(1-p\right) \end{align*}对于二类分类问题，若样本点属于第1类的概率为$p$，则概率分布的基尼指数为： \begin{align*} \\ & Gini \left( p \right) = \sum_{k=1}^{2} p_{k} \left( 1 - p_{k} \right) = 2p\left(1-p\right) \end{align*}对于给定样本集和$D$，其基尼指数为： \begin{align*} \\ & Gini \left( D \right) = 1 - \sum_{k=1}^{K} \left( \dfrac{\left| C_{k} \right|}{\left| D \right|} \right)^{2}\end{align*}其中，$C_{k}$是$D$中属于第$k$类的样本自己，$K$是类别个数。 如果样本集合$D$根据特征$A$是否取某一可能值$a$被分割成$D_{1}$和$D_{2}$两个部分，即 \begin{align*} \\ & D_{1} = \left\{ \left(x,y\right) | A\left(x\right)=a \right\}, \quad D_{2} = D - D_{1} \end{align*}则在特征$A$的条件下，集合$D$的基尼指数为： \begin{align*} \\ & Gini \left( D, A \right) = \dfrac{\left| D_{1} \right|}{\left| D \right|} Gini \left( D_{1} \right) + \dfrac{\left| D_{2} \right|}{\left| D \right|} Gini \left( D_{2} \right)\end{align*}基尼指数$Gini \left( D \right)$表示集合$D$的不确定性，基尼指数$Gini \left( D,A \right)$表示经$A=a$分割后集合$D$的不确定性。基尼指数值越大，样本集合的不确定性也越大，这一点与熵类似。 CART生成算法输入：训练数据集$D$，特征$A$，阈值$\varepsilon​$输出：CART决策树 设结点的训练数据集为$D$，对每一个特征$A$，对其可能取的每个值$a$，根据样本点对$A=a$的测试为“是”或“否”将$D$分割成$D_{1}$和$D_{2}$两部分，并计算$Gini\left(D,A\right)$。 在所有可能的特征$A$以及其所有可能的切分点$a$中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依此从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。 对两个子结点递归地调用1.和2.，直至满足停止条件。 生成CART决策树$T$。 算法停止计算的条件是结点中的样本个数小于预定阈值，或样本集的基尼指数小于预定阈值（样本基本属于同一类），或者没有更多特征。 CART剪枝CART剪枝算法从“完全生长”的决策树低端剪去一些子树，使决策树边小，从而能够对未知数据有更准确的预测。 剪枝，形成一个子树序列 $T_0, T_1, … , T_n$对整体树$T_{0}$任意内部结点$t$，以$t$为单结点树的损失函数是： \begin{align*} \\ & C_{\alpha} \left( t \right) = C \left( t \right) + \alpha \end{align*}以$t$为根结点的子树$T_{t}$的损失函数是： \begin{align*} \\ & C_{\alpha} \left( T_{t} \right) = C \left( T_{t} \right) + \alpha \left| T_{t} \right| \end{align*}当$\alpha = 0$及$\alpha$充分小时，有不等式： \begin{align*} \\ & C_{\alpha} \left( T_{t} \right) < C_{\alpha} \left( t \right) \end{align*}当$\alpha$增大时，在某一$\alpha$有： \begin{align*} \\ & \quad\quad C_{\alpha} \left( T_{t} \right) ＝ C_{\alpha} \left( t \right) \\ & C \left( T_{t} \right) + \alpha \left| T_{t} \right| ＝ C \left( t \right) + \alpha \\ & \quad\quad \alpha = \dfrac{C\left( t \right) - C \left(T_{t}\right)} { \left| T_{t} \right| -1 }\end{align*}即$T_{t}$与$t$有相同的损失函数值，而$t$的结点少，因此对$T_{t}$进行剪枝。 在剪枝得到的子树序列 $T_0, T_1, … , T_n$ 中通过交叉验证选取最优子树$T_n​$具体地，利用独立的验证数据集，测试子树序列 $T_0, T_1, … , T_n$ 中各棵子树的平方误差或基尼指数。平方误差或基尼指数最小的决策树被认为是最优的决策树。在子树序列中，每棵子树 $T_0, T_1, … , T_n$ 都对应一个参数$\alpha_1, \alpha_2, … , \alpha_n$。所以，当最优子树$T_k$确定时，对应的$\alpha_k$也确定了，即得到最优决策树$T_{\alpha}$。 CART剪枝算法输入：CART决策树$T_{0}$输出：最优决策树$T_{\alpha}​$ 设$k=0, T=T_{0}$ 设$\alpha=+\infty$ 自下而上地对各内部结点$t$计算$ C\left(T_{t}\right),\left| T_{t} \right|$，以及 \begin{align*} \\ & g\left(t\right) = \dfrac{C\left( t \right) - C \left(T_{t}\right)} { \left| T_{t} \right| -1 } \\ & \alpha = \min \left( \alpha, g\left( t \right) \right) \end{align*}其中，$T_{t}​$表示以$t​$为根结点的子树，$ C\left(T_{t}\right)​$是对训练数据的预测误差，$\left| T_{t} \right|​$是$T_{t}​$的叶结点个数。 自下而上地访问内部结点$t​$，如果有$g\left(t\right)=\alpha​$，则进行剪枝，并对叶结点$t​$以多数表决法决定其类别，得到树$T​$ 设$k=k+1, \alpha_{k}=\alpha, T_{k}=T$ 如果$T​$不是由根结点单独构成的树，则回到步骤4. 采用交叉验证法在子树序列$T_{0},T_{1},\cdots,T_{n}​$中选取最优子树$T_{\alpha}​$]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[感知机]]></title>
    <url>%2Fposts%2F48428%2F</url>
    <content type="text"><![CDATA[感知机（perceptron）是二类分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别。感知机是神经网络与支持向量机的基础。 感知机模型假设输入空间$\mathcal{X} \subseteq R^{n}$，输出空间$\mathcal{Y} = \left\{+1, -1 \right\}$。输入$x \in \mathcal{X}$表示实例的特征向量，对应于输入空间的点；输出$y \in \mathcal{Y}$表示实例的类别。由输入空间到输出空间的函数： \begin{align*} \\& f \left( x \right) = sign \left( w \cdot x + b \right) \end{align*}称为感知机。其中，$w$和$b$为感知机模型参数，$w \in R^{n}$叫做权值或权值向量，$b \in R$叫偏置，$w \cdot x$表示$w$和$x$的内积。$sign$是符号函数，即 \begin{align*} sign \left( x \right) = \left\{ \begin{aligned} \ & +1, x \geq 0 \\ & -1, x 0 \end{align*}误分类点$x_{i}$到分离超平面的距离为 \begin{align*} \\& -\dfrac{1}{\| w \|} y_{i}\left( w \cdot x_{i} + b \right) \end{align*}假设超平面$S$的误分类点集合为$M$，则所有误分类点到超平面$S$的总距离为 \begin{align*} \\& -\dfrac{1}{\| w \|} \sum_{x_{i} \in M} y_{i} \left( w \cdot x_{i} + b \right) \end{align*}给定训练数据集 \begin{align*} \\& T = \left\{ \left( x_{1}, y_{1} \right), \left( x_{2}, y_{2} \right), \cdots, \left( x_{N}, y_{N} \right) \right\} \end{align*}其中，$x_{i} \in \mathcal{X} = R^{n}, y_{i} \in \mathcal{Y} = \left\{ +1, -1 \right\}, i = 1, 2, \cdots, N$。感知机$sign \left( w \cdot x + b \right)$的损失函数定义为 \begin{align*} \\& L \left( w, b \right) = -\sum_{x_{i} \in M} y_{i} \left( w \cdot x_{i} + b \right) \end{align*}其中，$M$为误分类点的集合。显然，损失函数$L(w, b)$是非负的。如果没有误分类点，损失函数值是0。而且，误分类点越少，误分类点离超平面越近，损失函数值就越小。感知机学习的策略是在假设空间中选取使上述损失函数式最小的模型参数$w, b$，即感知机模型。 感知机学习算法感知机算法（原始形式）输入：训练数据集$T = \left\{ \left( x_{1}, y_{1} \right), \left( x_{2}, y_{2} \right), \cdots, \left( x_{N}, y_{N} \right) \right\}$，其中$x_{i} \in \mathcal{X} = R^{n}, y_{i} \in \mathcal{Y} = \left\{ +1, -1 \right\}, i = 1, 2, \cdots, N $；学习率$\eta \left( 0 &lt; \eta \leq 1 \right)$。 输出：$w,b$；感知机模型$f \left( x \right) = sign \left( w \cdot x + b \right)$ 选取初值$w_{0},b_{0}$ 在训练集中选取数据$\left( x_{i}, y_{i} \right)$ 如果$y_{i} \left( w \cdot x_{i} + b \right) \leq 0​$ \begin{align*} \\& w \leftarrow w + \eta y_{i} x_{i} \\ & b \leftarrow b + \eta y_{i} \end{align*} 转至2，直至训练集中没有误分类点 感知机算法（对偶形式）设$w,b$修改n次，则$w,b$关于$\left( x_{i}, y_{i} \right)$的增量分别是$\alpha_{i} y_{i} x_{i}$和$\alpha_{i} y_{i}$，其中$\alpha_{i} = n_{i} \eta$。$w,b$可表示为 \begin{align*} \\& w = \sum_{i=1}^{N} \alpha_{i} y_{i} x_{i} \\ & b = \sum_{i=1}^{N} \alpha_{i} y_{i} \end{align*}其中，$\alpha_{i} \geq 0, i=1,2, \cdots, N$ 输入：训练数据集$T = \left\{ \left( x_{1}, y_{1} \right), \left( x_{2}, y_{2} \right), \cdots, \left( x_{N}, y_{N} \right) \right\}$，其中$x_{i} \in \mathcal{X} = R^{n}, y_{i} \in \mathcal{Y} = \left\{ +1, -1 \right\}, i = 1, 2, \cdots, N $；学习率$\eta \left( 0 &lt; \eta \leq 1 \right)$。 输出：$\alpha,b$；感知机模型$f \left( x \right) = sign \left( \sum_{j=1}^{N} \alpha_{j} y_{j} x_{j} \cdot x + b \right)$ ，其中$\alpha = \left( \alpha_{1}, \alpha_{2}, \cdots, \alpha_{N} \right)^{T}$ $\alpha \leftarrow 0, b \leftarrow 0$ 在训练集中选取数据$\left( x_{i}, y_{i} \right)$ 如果$y_{i} \left( \sum_{j=1}^{N} \alpha_{j} y_{j} x_{j} \cdot x_{i} + b \right) \leq 0$ \begin{align*} \\& \alpha_{i} \leftarrow \alpha_{i} + \eta \\ & b \leftarrow b + \eta y_{i} \end{align*} 转至2，直至训练集中没有误分类点 注：对偶形式中训练实例仅以内积形式出现，可预先计算$Gram$矩阵存储实例间内积 \begin{align*} \\& G = \left[ x_{i} \cdot x_{j} \right]_{N \times N} \end{align*}]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>感知机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K-Nearest Neighbors Algorithm]]></title>
    <url>%2Fposts%2F63793%2F</url>
    <content type="text"><![CDATA[k 近邻法（K-Nearest Neighbors Algorithm，k-NN）是一种基本分类与回归方法，通过多数表决等方式进行预测，因此不具有显式的学习过程。 k 近邻算法输入：训练数据集$T = \left\{ \left( x_{1}, y_{1} \right), \left( x_{2}, y_{2} \right), \cdots, \left( x_{N}, y_{N} \right) \right\}$，其中$x_{i} \in \mathcal{X} \subseteq R^{n}$是实例的特征向量，$ y_{i} \in \mathcal{Y} = \left\{ c_{1}, c_{2}, \cdots, c_{K} \right\}$是实例的类别，$ i = 1, 2, \cdots, N$；实例特征向量$x$ ； 输出：实例$x$所属的类$y$ 根据给定的距离度量，在训练集$T$中找出与$x$最近邻的$k$个点，涵盖这$k$点的$x$的邻域记作$N_{k} \left( x \right)$； 在$N_{k} \left( x \right)$中根据分类决策规则决定$x$的类别$y$： \begin{align*} \\ & y = \arg \max_{c_{j}} \sum_{x_{i} \in N_{k} \left( x \right)} I \left( y_{i} = c_{j} \right), \quad i=1,2, \cdots, N; \quad j=1,2,\cdots,K \end{align*} k 近邻模型k 近邻法使用的模型实际上对应于特征空间的划分。模型由三个基本要素——距离度量、k值的选择和分类决策规则决定。 特征空间中，对每个训练实例点$x_i​$，距离该点比其他点更近的所有点组成一个区域，叫做单元。下图是二维特征空间划分的一个例子。 距离度量设特征空间$\mathcal{X}$是$n$维实数向量空间$R^{n}$，$x_{i},x_{j} \in \mathcal{X},x_{i} = \left( x_{i}^{\left( 1 \right)},x_{i}^{\left( 2 \right) },\cdots,x_{i}^{\left( n \right) } \right)^{T},x_{j} = \left( x_{j}^{\left( 1 \right)},x_{j}^{\left( 2 \right) },\cdots,x_{j}^{\left( n \right) } \right)^{T}$，$x_{i},x_{j}$的$L_{p}$距离定义为 \begin{align*} \\ & L_{p} \left( x_{i},x_{j} \right) = \left( \sum_{l=1}^{N} \left| x_{i}^{\left(l\right)} - x_{j}^{\left( l \right)} \right|^{p} \right)^{\dfrac{1}{p}}\end{align*}其中，$p \geq 1$。当$p=2$时，称为欧氏距离，即 \begin{align*} \\ & L_{2} \left( x_{i},x_{j} \right) = \left( \sum_{l=1}^{N} \left| x_{i}^{\left(l\right)} - x_{j}^{\left( l \right)} \right|^{2} \right)^{\dfrac{1}{2}}\end{align*}当$p=1$时，称为曼哈顿距离，即 \begin{align*} \\ & L_{1} \left( x_{i},x_{j} \right) = \sum_{l=1}^{N} \left| x_{i}^{\left(l\right)} - x_{j}^{\left( l \right)} \right| \end{align*}当$p=\infty$时，是各个坐标距离的最大值，即 \begin{align*} \\ & L_{\infty} \left( x_{i},x_{j} \right) = \max_{l} \left| x_{i}^{\left(l\right)} - x_{j}^{\left( l \right)} \right| \end{align*}下图给出了二维空间中$p$取不同值时，与原点的$L_p$距离为1的点的图形： k 值的选择如果选择较小的k值： 学习的近似误差减小，但学习的估计误差增大 噪声敏感 整体模型变复杂，容易发生过拟合 如果选择较大的k值： 学习的估计误差减小，但学习的近似误差增大 整体模型变简单 分类决策规则多数表决规则：如果分类的损失函数为0-1损失函数，分类函数为 \begin{align*} \\ & f: R^{n} \to \left\{ c_{1}, c_{2}, \cdots, c_{K} \right\} \end{align*}则误分类的概率是 \begin{align*} \\ & P \left( Y \neq f \left( X \right) \right) = 1 - P \left( Y = f\left( X \right) \right) \end{align*}对给定的实例$x \in \mathcal{X}$，其最近邻的$k$个训练实例点构成的集合$N_{k} \left( x \right)$。如果涵盖$N_{k} \left( x \right)$的区域的类别是$c_{j}$，则误分类率是 \begin{align*} \\ & \dfrac{1}{k} \sum_{x_{i} \in N_{k} \left( x \right)} I \left( y_{i} \neq c_{j}\right) = 1 -\dfrac{1}{k} \sum_{x_{i} \in N_{k} \left( x \right)} I \left( y_{i} = c_{j}\right) \end{align*}要使误分类率最小即经验风险最小，就要使$\sum_{x_{i} \in N_{k} \left( x \right)} I \left( y_{i} = c_{j}\right)$最大，所以多数表决规则等价于经验风险最小化。 k 近邻法的实现：kd 树构造 kd 树平衡kd树构造算法：输入：$k$维空间数据集$T = \left\{ x_{1}, x_{2}, \cdots, x_{N} \right\}$，其中$x_{i} = \left( x_{i}^{\left(1\right)}, x_{i}^{\left(1\right)},\cdots,x_{i}^{\left(k\right)} \right)^{T}, i = 1, 2, \cdots, N​$；输出：kd树 开始：构造根结点，根结点对应于包涵$T$的$k$维空间的超矩形区域。选择$x^{\left( 1 \right)}$为坐标轴，以$T$中所欲实例的$x^{\left( 1 \right)}$坐标的中位数为切分点，将根结点对应的超矩形区域切分成两个子区域。切分由通过切分点并与坐标轴$x^{\left( 1 \right)}$垂直的超平面实现。由根结点生成深度为1的左、右子结点：坐子结点对应坐标$x^{\left( 1 \right)}$小于切分点的子区域，右子结点对应于坐标$x^{\left( 1 \right)}​$大与切分点的子区域。将落在切分超平面上的实例点保存在跟结点。 重复：对深度为j的结点，选择$x^{\left( l \right)}$为切分坐标轴，$l = j \left(\bmod k \right) + 1 $，以该结点的区域中所由实例的$x^{\left( l \right)}$坐标的中位数为切分点，将该结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴$x^{\left( l \right)}$垂直的超平面实现。由根结点生成深度为$j+1$的左、右子结点：坐子结点对应坐标$x^{\left( l \right)}$小于切分点的子区域，右子结点对应于坐标$x^{\left( l \right)}$大与切分点的子区域。将落在切分超平面上的实例点保存在跟结点。 直到两个子区域没有实例存在时停止。 下面给出一个例子： 搜索 kd 树kd树的最近邻搜索算法：输入：kd树；目标点$x$；输出：$x$的最近邻。 在kd树中找出包含目标点$x$的叶结点：从跟结点出发，递归地向下访问kd树。若目标点$x$当前维的坐标小于切分点的坐标，则移动到左子结点，否则移动到右子结点。直到子结点为叶结点为止。 以此叶结点为“当前最近点”。 递归地向上回退，在每个结点进行以下操作：3.1 如果该结点保存的实例点比当前最近点距离目标点更近，则以该实例点为“当前最近点”。3.2 当前最近点一定存在于该结点一个子结点对应的区域。检查该子结点的父结点的另一子结点对应的区域是否有更近的点。具体地，检查另一子结点对应的区域是否与以目标点为球心、以目标点与“当前最近点”间的距离为半径的超球体相交。如果相交，可能在另一个子结点对应的区域内存在距目标点更近的点，移动到另一个子结点。接着，递归地进行最近邻搜索；如果不相交，向上回退。 当回退到根结点时，搜索结束。最后的“当前最近点”即为$x$的当前最近邻点。 下面给出一个例子：]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>k近邻法</tag>
        <tag>kd树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯法]]></title>
    <url>%2Fposts%2F62831%2F</url>
    <content type="text"><![CDATA[朴素贝叶斯的学习与分类训练数据集 \begin{align*} \\& T = \left\{ \left( x_{1}, y_{1} \right), \left( x_{2}, y_{2} \right), \cdots, \left( x_{N}, y_{N} \right) \right\} \end{align*}由$P \left( X, Y \right)​$独立同分布产生。其中，$x_{i} \in \mathcal{X} \subseteq R^{n}, y_{i} \in \mathcal{Y} = \left\{ c_{1}, c_{2}, \cdots, c_{K} \right\}, i = 1, 2, \cdots, N​$，$x_{i}​$为第$i​$个特征向量（实例），$y_{i}​$为$x_{i}​$的类标记，$X​$是定义在输入空间$\mathcal{X}​$上的随机向量，$Y​$是定义在输出空间$\mathcal{Y}​$上的随机变量。$P \left( X, Y \right)​$是$X​$和$Y​$的联合概率分布。 朴素贝叶斯法对条件概率分布作了条件独立性的假设，条件独立性假设是 \begin{align*} \\& P \left( X = x | Y = c_{k} \right) = P \left( X^{\left( 1 \right)} = x^{\left( 1 \right)} , \cdots, X^{\left( n \right)} = x^{\left( n \right)} | Y = c_{k}\right) \\ & \quad\quad\quad\quad\quad\quad = \prod_{j=1}^{n} P \left( X^{\left( j \right)} = x^{\left( j \right)} | Y = c_{k} \right) \end{align*}即，用于分类的特征在类确定的条件下都是条件独立的。 朴素贝叶斯法分类时，对给定的输入x，通过学习到的模型计算后验概率分布$P(Y=c_k)|X=x$，将后验概率最大的类作为x的类输出，后验概率计算根据贝叶斯定理进行： \begin{align*} \\& P \left(X = x | Y = c_{k} \right) P \left( Y = c_{k} \right) = P \left( Y = c_{k}| X = x \right) P \left( X = x \right) \\ & P \left( Y = c_{k}| X = x \right) = \dfrac{P \left(X = x | Y = c_{k} \right) P \left( Y = c_{k} \right)}{P \left( X = x \right)} \\ & \quad\quad\quad\quad\quad\quad = \dfrac{P \left(X = x | Y = c_{k} \right) P \left( Y = c_{k} \right)}{\sum_{Y} P \left( X = x, Y = c_{k} \right)} \\ & \quad\quad\quad\quad\quad\quad = \dfrac{P \left(X = x | Y = c_{k} \right) P \left( Y = c_{k} \right)}{\sum_{Y} P \left(X = x | Y = c_{k} \right) P \left( Y = c_{k} \right)} \\ & \quad\quad\quad\quad\quad\quad = \dfrac{ P \left( Y = c_{k} \right)\prod_{j=1}^{n} P \left( X^{\left( j \right)} = x^{\left( j \right)} | Y = c_{k} \right)}{\sum_{Y} P \left( Y = c_{k} \right)\prod_{j=1}^{n} P \left( X^{\left( j \right)} = x^{\left( j \right)} | Y = c_{k} \right)}\end{align*}朴素贝叶斯分类器可表示为 \begin{align*} \\& y = f \left( x \right) = \arg \max_{c_{k}} \dfrac{ P \left( Y = c_{k} \right)\prod_{j=1}^{n} P \left( X^{\left( j \right)} = x^{\left( j \right)} | Y = c_{k} \right)}{\sum_{Y} P \left( Y = c_{k} \right)\prod_{j=1}^{n} P \left( X^{\left( j \right)} = x^{\left( j \right)} | Y = c_{k} \right)} \\ & \quad\quad\quad = \arg \max_{c_{k}} P \left( Y = c_{k} \right)\prod_{j=1}^{n} P \left( X^{\left( j \right)} = x^{\left( j \right)} | Y = c_{k} \right)\end{align*}朴素贝叶斯算法的参数估计极大似然估计 先验概率$P \left( Y = c_{k} \right)$的极大似然估计是 \begin{align*} \\& P \left( Y = c_{k} \right) = \dfrac{\sum_{i=1}^{N} I \left( y_{i} = c_{k} \right)}{N} \quad k = 1, 2, \cdots, K\end{align*} 设第$j$个特征$x^{\left( j \right)}$可能取值的集合为$\left\{ a_{j1}, a_{j2}, \cdots, a_{j S_{j}} \right\}$，条件概率$P \left( X^{\left( j \right)} = a_{jl} | Y = c_{k} \right)$的极大似然估计是 \begin{align*} \\& P \left( X^{\left( j \right)} = a_{jl} | Y = c_{k} \right) ＝ \dfrac{\sum_{i=1}^{N} I \left(x_{i}^{\left( j \right)}=a_{jl}, y_{i} = c_{k} \right)}{\sum_{i=1}^{N} I \left( y_{i} = c_{k} \right)} \\ & j = 1, 2, \cdots, n;\quad l = 1, 2, \cdots, S_{j};\quad k = 1, 2, \cdots, K\end{align*}其中，$x_{i}^{\left( j \right)}$是第$i$个样本的第$j$个特征；$a_{jl}$是第$j$个特征可能取的第$l$个值；$I$是指示函数。 朴素贝叶斯算法输入：线性可分训练数据集$T = \left\{ \left( x_{1}, y_{1} \right), \left( x_{2}, y_{2} \right), \cdots, \left( x_{N}, y_{N} \right) \right\}​$，其中$x_{i}＝ \left( x_{i}^{\left(1\right)},x_{i}^{\left(2\right)},\cdots, x_{i}^{\left(n\right)} \right)^{T}​$，$x_{i}^{\left( j \right)}​$是第$i​$个样本的第$j​$个特征，$x_{i}^{\left( j \right)} \in \left\{ a_{j1}, a_{j2}, \cdots, a_{j S_{j}} \right\}​$，$a_{jl}​$是第$j​$个特征可能取的第$l​$个值，$j = 1, 2, \cdots, n; l = 1, 2, \cdots, S_{j},y_{i} \in \left\{ c_{1}, c_{2}, \cdots, c_{K} \right\}​$；实例$x​$； 输出：实例$x$的分类 计算先验概率及条件概率 \begin{align*} \\ & P \left( Y = c_{k} \right) = \dfrac{\sum_{i=1}^{N} I \left( y_{i} = c_{k} \right)}{N} \quad k = 1, 2, \cdots, K \\ & P \left( X^{\left( j \right)} = a_{jl} | Y = c_{k} \right) ＝ \dfrac{\sum_{i=1}^{N} I \left(x_{i}^{\left( j \right)}=a_{jl}, y_{i} = c_{k} \right)}{\sum_{i=1}^{N} I \left( y_{i} = c_{k} \right)} \\ & j = 1, 2, \cdots, n;\quad l = 1, 2, \cdots, S_{j};\quad k = 1, 2, \cdots, K\end{align*} 对于给定的实例$x=\left( x^{\left( 1 \right)}, x^{\left( 2 \right)}, \cdots, x^{\left( n \right)}\right)^{T}$，计算 \begin{align*} \\ & P \left( Y = c_{k} \right)\prod_{j=1}^{n} P \left( X^{\left( j \right)} = x^{\left( j \right)} | Y = c_{k} \right) \quad k=1,2,\cdots,K\end{align*} 确定实例$x$的类 \begin{align*} \\& y = f \left( x \right) = \arg \max_{c_{k}} P \left( Y = c_{k} \right)\prod_{j=1}^{n} P \left( X^{\left( j \right)} = x^{\left( j \right)} | Y = c_{k} \right) \end{align*} 贝叶斯估计用极大似然估计可能会出现所要估计的概率值为0的情况，这时会影响到后验概率的计算结果，解决这一问题的方法是采用贝叶斯估计。 先验概率的贝叶斯估计 \begin{align*} \\& P \left( Y = c_{k} \right) = \dfrac{\sum_{i=1}^{N} I \left( y_{i} = c_{k} \right) + \lambda}{N + K \lambda}\end{align*} 条件概率的贝叶斯估计 \begin{align*} \\& P_{\lambda} \left( X^{\left( j \right)} = a_{jl} | Y = c_{k} \right) ＝ \dfrac{\sum_{i=1}^{N} I \left(x_{i}^{\left( j \right)}=a_{jl}, y_{i} = c_{k} \right) + \lambda}{\sum_{i=1}^{N} I \left( y_{i} = c_{k} \right) + S_{j} \lambda} \end{align*}式中$\lambda \geq 0​$。当$\lambda ＝ 0​$时，是极大似然估计；当$\lambda ＝ 1​$时，称为拉普拉斯平滑。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>贝叶斯定理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最大似然估计、最大后验概率估计和贝叶斯公式]]></title>
    <url>%2Fposts%2F49959%2F</url>
    <content type="text"><![CDATA[本文转载自 https://blog.csdn.net/u011508640/article/details/72815981，感谢原创作者，文章版权归原作者所有。 首先一句话总结，极大似然估计就是利用已知的样本结果信息，反推最大概率导致这些样本结果出现的模型参数值。 最大似然估计（Maximum likelihood estimation, 简称MLE）和最大后验概率估计（Maximum a posteriori estimation, 简称MAP）是很常用的两种参数估计方法，如果不理解这两种方法的思路，很容易弄混它们。下文将详细说明MLE和MAP的思路与区别。 但别急，我们先从概率和统计的区别讲起。 概率和统计是一个东西吗？概率（probabilty）和统计（statistics）看似两个相近的概念，其实研究的问题刚好相反。 概率研究的问题是，已知一个模型和参数，怎么去预测这个模型产生的结果的特性（例如均值，方差，协方差等等）。 举个例子，我想研究怎么养猪（模型是猪），我选好了想养的品种、喂养方式、猪棚的设计等等（选择参数），我想知道我养出来的猪大概能有多肥，肉质怎么样（预测结果）。 统计研究的问题则相反。统计是，有一堆数据，要利用这堆数据去预测模型和参数。仍以猪为例。现在我买到了一堆肉，通过观察和判断，我确定这是猪肉（这就确定了模型。在实际研究中，也是通过观察数据推测模型是／像高斯分布的、指数分布的、拉普拉斯分布的等等），然后，可以进一步研究，判定这猪的品种、这是圈养猪还是跑山猪还是网易猪，等等（推测模型参数）。 一句话总结：概率是已知模型和参数，推数据。统计是已知数据，推模型和参数。 显然，本文解释的MLE和MAP都是统计领域的问题。它们都是用来推测参数的方法。为什么会存在着两种不同方法呢？ 这需要理解贝叶斯思想，我们来看看贝叶斯公式。 贝叶斯公式到底在说什么？学习机器学习和模式识别的人一定都听过贝叶斯公式(Bayes’ Theorem)： P(A|B)=\frac{P(B|A)P(A)}{P(B)} \quad【式1】贝叶斯公式看起来很简单，无非是倒了倒条件概率和联合概率的公式。 把B展开，可以写成： P(A|B)=\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|-A)P(-A)} \quad【式2】（-A表示”非A”）这个式子就很有意思了。 想想这个情况。一辆汽车（或者电瓶车）的警报响了，你通常是什么反应？有小偷？撞车了？ 不， 你通常什么反应都没有。因为汽车警报响一响实在是太正常了！每天都要发生好多次。本来，汽车警报设置的功能是，出现了异常情况，需要人关注。然而，由于虚警实在是太多，人们渐渐不相信警报的功能了。 贝叶斯公式就是在描述，你有多大把握能相信一件证据？（how much you can trust the evidence）。 我们假设响警报的目的就是想说汽车被砸了。把A计作“汽车被砸了”，B计作“警报响了”，带进贝叶斯公式里看。我们想求等式左边发生$A|B$的概率，这是在说警报响了，汽车也确实被砸了。汽车被砸引起（trigger）警报响，即$B|A$。但是，也有可能是汽车被小孩子皮球踢了一下、被行人碰了一下等其他原因（统统计作$-A$），其他原因引起汽车警报响了，$B|-A$。那么，现在突然听见警报响了，这时汽车已经被砸了的概率是多少呢（这即是说，警报响这个证据有了，多大把握能相信它确实是在报警说汽车被砸了）？想一想，应当这样来计算。用警报响起、汽车也被砸了这事件的数量，除以响警报事件的数量（这即【式1】）。进一步展开，即警报响起、汽车也被砸了的事件的数量，除以警报响起、汽车被砸了的事件数量加上警报响起、汽车没被砸的事件数量（这即【式2】）。 可能有点绕，请稍稍想一想。 再思考【式2】。想让$P(A|B) = 1$，即警报响了，汽车一定被砸了，该怎么做呢？让$P(B|-A)P(-A) = 0$即可。很容易想清楚，假若让其等于0，即杜绝了汽车被球踢、被行人碰到等等其他所有情况，那自然，警报响了，只剩下一种可能——汽车被砸了。这即是提高了响警报这个证据的说服力。 从这个角度总结贝叶斯公式：做判断的时候，要考虑所有的因素。 老板骂你，不一定是你把什么工作搞砸了，可能只是他今天出门前和太太吵了一架。 再思考【式2】。观察【式2】右边的分子，$P(B|A)$为汽车被砸后响警报的概率。姑且仍为这是1吧。但是，若$P(A)$很小，即汽车被砸的概率本身就很小，则$P(B|A)P(A)$仍然很小，即【式2】右边分子仍然很小，$P(A|B)$还是大不起来。 这里，$P(A)$即是常说的先验概率，如果A的先验概率很小，就算$P(B|A)$较大，可能A的后验概率$P(A|B)$还是不会大（假设$P(B|-A)P(-A)$不变的情况下）。 从这个角度思考贝叶斯公式：一个本来就难以发生的事情，就算出现某个证据和他强烈相关，也要谨慎。证据很可能来自别的虽然不是很相关，但发生概率较高的事情。 发现刚才写的代码编译报错，可是我今天状态特别好，这语言我也很熟悉，犯错的概率很低。因此觉得是编译器出错了。 ————别，还是先再检查下自己的代码吧。 好了好了，说了这么多，下面言归正传，说一说MLE。 ——————不行，还得先说似然函数（likelihood function）。 似然函数似然（likelihood）这个词其实和概率（probability）是差不多的意思，Colins字典这么解释：The likelihood of something happening is how likely it is to happen. 你把likelihood换成probability，这解释也读得通。但是在统计里面，似然函数和概率函数却是两个不同的概念（其实也很相近就是了）。 对于这个函数：$P(x|θ)$，输入有两个：$x$表示某一个具体的数据；$θ$表示模型的参数。 如果$θ$是已知确定的，$x$是变量，这个函数叫做概率函数(probability function)，它描述对于不同的样本点$x$，其出现概率是多少。 如果$x$是已知确定的，$θ$是变量，这个函数叫做似然函数(likelihood function), 它描述对于不同的模型参数，出现$x$这个样本点的概率是多少。 这有点像“一菜两吃”的意思。其实这样的形式我们以前也不是没遇到过。例如，$f(x,y)=x^y$， 即$x$的$y$次方。如果$x$是已知确定的(例如$x=2$)，这就是$f(y)=2^y$， 这是指数函数。 如果$y$是已知确定的(例如$y=2$)，这就是$f(x)=x^2$，这是二次函数。同一个数学形式，从不同的变量角度观察，可以有不同的名字。 这么说应该清楚了吧？ 如果还没讲清楚，别急，下文会有具体例子。 最大似然估计（MLE）假设有一个造币厂生产某种硬币，现在我们拿到了一枚这种硬币，想试试这硬币是不是均匀的。即想知道抛这枚硬币，正反面出现的概率（记为$θ$）各是多少？ 这是一个统计问题，回想一下，解决统计问题需要什么？ 数据！ 于是我们拿这枚硬币抛了10次，得到的数据（$x_0$）是：反正正正正反正正正反。我们想求的正面概率$θ$是模型参数，而抛硬币模型我们可以假设是 二项分布。 那么，出现实验结果$x_0$（即反正正正正反正正正反）的似然函数是多少呢？ f(x_0,θ)=(1−θ)×θ×θ×θ×θ×(1−θ)×θ×θ×θ×(1−θ)=θ^7(1−θ)^3=f(θ)注意，这是个只关于$θ$的函数。而最大似然估计，顾名思义，就是要最大化这个函数。我们可以画出$f(θ)$的图像： 可以看出，在$θ=0.7$时，似然函数取得最大值。 这样，我们已经完成了对$θ$的最大似然估计。即，抛10次硬币，发现7次硬币正面向上，最大似然估计认为正面向上的概率是0.7。（ummm..这非常直观合理，对吧？） 且慢，一些人可能会说，硬币一般都是均匀的啊！ 就算你做实验发现结果是“反正正正正反正正正反”，我也不信$θ=0.7$。 这里就包含了贝叶斯学派的思想了——要考虑先验概率。 为此，引入了最大后验概率估计。 最大后验概率估计（MAP）最大似然估计是求参数$θ$，使似然函数$P(x_0|θ)$最大。最大后验概率估计则是想求$θ$使$P(x_0|θ)P(θ)$最大。求得的$θ$不单单让似然函数大，$θ$自己出现的先验概率也得大。 （这有点像正则化里加惩罚项的思想，不过正则化里是利用加法，而MAP里是利用乘法） MAP其实是在最大化$P(\theta|x_0) = \frac{P(x_0|\theta)P(\theta)}{P(x_0)}$，不过因为$x_0$是确定的（即投出的“反正正正正反正正正反”），$P(x_0)$是一个已知值，所以去掉了分母$P(x_0)$（假设“投10次硬币”是一次实验，实验做了1000次，“反正正正正反正正正反”出现了n次，则$P(x_0)=n/1000$。总之，这是一个可以由数据集得到的值）。最大化$P(θ|x_0)$的意义也很明确，$x_0$已经出现了，要求θ取什么值使$P(θ|x_0)$最大。顺带一提，$P(θ|x_0)$即后验概率，这就是“最大后验概率估计”名字的由来。 对于投硬币的例子来看，我们认为（”先验地知道“）$θ$取0.5的概率很大，取其他值的概率小一些。我们用一个高斯分布来具体描述我们掌握的这个先验知识，例如假设$P(θ)$为均值0.5，方差0.1的高斯函数，如下图： 则$P(x_0|θ)P(θ)$的函数图像为： 注意，此时函数取最大值时，$θ$取值已向左偏移，不再是0.7。实际上，在$θ=0.558$时函数取得了最大值。即，用最大后验概率估计，得到$θ=0.558$。 最后，那要怎样才能说服一个贝叶斯派相信$θ=0.7$呢？你得多做点实验。 如果做了1000次实验，其中700次都是正面向上，这时似然函数为: 如果仍然假设$P(θ)$为均值0.5，方差0.1的高斯函数，$P(x_0|θ)P(θ)$的函数图像为： 在$θ=0.696$处，$P(x_0|θ)P(θ)$取得最大值。 这样，就算一个考虑了先验概率的贝叶斯派，也不得不承认得把$θ$估计在0.7附近了。 不过，要是遇上了顽固的贝叶斯派，认为$P(θ=0.5)=1$，那就没得玩了。无论怎么做实验，使用MAP估计出来都是$θ=0.5$。这也说明，一个合理的先验概率假设是很重要的。（通常，先验概率能从数据中直接分析得到） 最大似然估计和最大后验概率估计的区别相信读完上文，MLE和MAP的区别应该是很清楚的了。MAP就是多个作为因子的先验概率$P(θ)$。或者，也可以反过来，认为MLE是把先验概率$P(θ)$认为等于1，即认为θ是均匀分布。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>最大似然估计</tag>
        <tag>贝叶斯公式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[提升树]]></title>
    <url>%2Fposts%2F34928%2F</url>
    <content type="text"><![CDATA[提升树模型提升方法实际采用加法模型（即基函数的线性组合）与前向分布算法。以决策树为基函数的提升方法称为提升树。提升树可以表示为决策树的加法模型： f_M(x) = \sum_{m=1}^MT(x;\Theta_m)其中，$T(x;\Theta_m)$表示决策树，$\Theta_m$为决策树的参数，$M$为树的个数。 提升树算法提升树算法采用前向分布算法，首先确定初始提升树$f_0(x) = 0$，第m步的模型是： f_m(x) = f_{m-1}(x) +T(x;\Theta_m)其中，$f_{m-1}(x)$为当前模型，通过经验风险极小化确定下一棵决策树的参数$\Theta_m$： \hat \Theta_m = arg \quad min_{\Theta_m} \sum_{i=1}^N L(y_i, f_{m-1}(x_i)+T(x_i;\Theta_m))当采用平方损失函数时， L(y,f(x)) = (y-f(x))^2其损失为 L(y_i, f_{m-1}(x_i)+T(x_i;\Theta_m) = [r-T(x;\Theta_m)]^2这里$r$是残差，$r = y-f_{m-1}(x)$。 对于提升树算法，通常使用平方误差损失函数解决回归问题，而使用指数损失函数解决分类问题，以及使用一般损失函数解决决策问题。对于二类分类问题，提升树算法只需将 AdaBoost 算法中的基本分类器限制为二类分类器即可。可以说这时的提升树算法是 AdaBoost 算法的特殊情况。本节主要叙述回归问题的提升树。 回归问题的提升树算法输入：训练数据集$T = \left\{ \left( x_{1}, y_{1} \right), \left( x_{2}, y_{2} \right), \cdots, \left( x_{N}, y_{N} \right) \right\},x_{i} \in \mathcal{X} \subseteq R^{n}, y_{i} \in \mathcal{Y} \subseteq R, i = 1, 2, \cdots, N$ 输出：回归提升树$f_{M}\left(x\right)$ 初始化$f_{0}\left(x\right)=0$ 对$m=1,2,\cdots,M​$2.1 计算残差 \begin{align*} \\ & r_{mi}=y_{i}-f_{m-1}\left(x_{i}\right),\quad i=1,2,\cdots,N \end{align*}2.2 拟合残差$r_{mi}$学习一个回归树，得到$T\left(x;\varTheta_{m}\right)$2.3 更新$f_{m}=f_{m-1}\left(x\right)+T\left(x;\varTheta_{m}\right)$ 得到回归提升树 \begin{align*} \\ & f_{M} \left( x \right) = \sum_{m=1}^{M} T \left(x;\varTheta_{m}\right) \end{align*} 梯度提升提升树利用加法模型与前向分布算法实现学习的优化过程，当损失函数是平方损失和指数损失函数时，每一步的优化是很简单的。但对于一般损失函数而言，往往每一步优化并不那么容易，于是梯度提升 (gradient boosting) 算法被提出。 梯度提升算法输入：训练数据集$T = \left\{ \left( x_{1}, y_{1} \right), \left( x_{2}, y_{2} \right), \cdots, \left( x_{N}, y_{N} \right) \right\},x_{i} \in \mathcal{X} \subseteq R^{n}, y_{i} \in \mathcal{Y} \subseteq R, i = 1, 2, \cdots, N$，损失函数$L\left(y,f\left(x\right)\right)$ 输出：回归树$\hat f\left(x\right)​$ 初始化 \begin{align*} \\ & f_{0}\left(x\right) = \arg \min_{c} \sum_{i=1}^{N} L \left(y_{i},c\right) \end{align*} 对$m=1,2,\cdots,M$2.1 对$i=1,2,\cdots,N$，计算 \begin{align*} \\ & r_{mi}=- \left[ \dfrac {\partial L \left(y_{i},f\left(x_{i}\right) \right)}{\partial f \left(x_{i} \right)}\right]_{f\left(x\right)=f_{m-1}\left(x\right)} \end{align*}2.2 对$r_{mi}$拟合回归树，得到第$m$棵树的叶结点区域$R_{mj}，j=1,2,\cdots,J$2.3 对$j=1,2,\cdots,J$，计算 \begin{align*} \\ & c_{mj}=\arg \min_{c} \sum_{x_{i} \in R_{mj}} L \left( y_{i},f_{m-1} \left(x_{i}\right)+c \right) \end{align*}2.4 更新$f_{m}\left(x\right)= f_{m-1}\left(x\right) + \sum_{j=1}^{J} c_{mj} I \left(x \in R_{mj} \right)$ 得到回归树 \begin{align*} \\ & \hat f \left( x \right) = f_{M} \left( x \right) = \sum_{m=1}^{M} \sum_{j=1}^{J} c_{mj} I \left( x \in R_{mj} \right) \end{align*}]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>提升树</tag>
        <tag>GBDT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决Chrome账户无法同步]]></title>
    <url>%2Fposts%2F59649%2F</url>
    <content type="text"><![CDATA[最近遇到一个特别坑的情况，在Chrome上登陆账户总是登不上去，提示账户无法同步，导致书签、历史记录、插件什么的都无法正常使用。网上查了一圈，的确是有很多人发生这种情况，但大家的原因好像都不相同。 折腾了一圈我的账户依然无法同步，最后偶然发现先登陆Gmail，然后再登陆账户，竟然同步成功了，不知道这算不算谷歌的一个bug。记录下来吧，供大家参考。 顺便记一个小技巧，由于Chrome是默认使用缓存进行刷新的，从而导致提交文章到网站之后无法实时显示。为了解决这个问题，可以使用Ctrl+Shift+R强制Chrome不使用缓存进行刷新。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Google</tag>
        <tag>Chrome</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logistic 回归模型]]></title>
    <url>%2Fposts%2F48429%2F</url>
    <content type="text"><![CDATA[Logistic 回归是统计学习中的经典方法，属于对数线性模型。 二项 Logistic 回归模型将线性回归函数和Logistic函数复合起来，称为逻辑回归函数，二项Logistic回归模型是一种分类模型，二项Logistic回归模型是如下的条件概率分布： \begin{align*} \\& P \left( Y = 1 | x \right) = \dfrac{1}{1+\exp{-\left(w \cdot x + b \right)}} \\ & \quad\quad\quad\quad = \dfrac{\exp{\left(w \cdot x + b \right)}}{\left( 1+\exp{-\left(w \cdot x + b \right)}\right) \cdot \exp{\left(w \cdot x + b \right)}} \\ & \quad\quad\quad\quad = \dfrac{\exp{\left(w \cdot x + b \right)}}{1+\exp{\left( w \cdot x + b \right)}}\\& P \left( Y = 0 | x \right) = 1- P \left( Y = 1 | x \right) \\ & \quad\quad\quad\quad=1- \dfrac{\exp{\left(w \cdot x + b \right)}}{1+\exp{\left( w \cdot x + b \right)}} \\ & \quad\quad\quad\quad=\dfrac{1}{1+\exp{\left( w \cdot x + b \right)}}\end{align*}其中，$x \in R^{n}$是输入，$Y \in \left\{ 0, 1 \right\}$是输出，$w \in R^{n}$和$b \in R$是参数，$w$称为权值向量，$b$称为偏置，$w \cdot x$为$w$和$b$的内积。 Logistic回归比较两个条件概率值的大小，将实例$x$分到概率值较大的那一类。 可将权值权值向量和输入向量加以扩充，即$w = \left( w^{\left(1\right)},w^{\left(2\right)},\cdots,w^{\left(n\right)},b \right)^{T}$，$x = \left( x^{\left(1\right)},x^{\left(2\right)},\cdots,x^{\left(n\right)},1 \right)^{T}$，则逻辑斯谛回归模型如下： \begin{align*} \\& P \left( Y = 1 | x \right) = \dfrac{\exp{\left(w \cdot x \right)}}{1+\exp{\left( w \cdot x \right)}}\\& P \left( Y = 0 | x \right) =\dfrac{1}{1+\exp{\left( w \cdot x \right)}}\end{align*}模型参数估计Logistic回归模型学习时，对于给定训练数据集$T = \left\{ \left( x_{1}, y_{1} \right), \left( x_{2}, y_{2} \right), \cdots, \left( x_{N}, y_{N} \right) \right\} $，其中，$x_{i} \in R^{n+1}, y_{i} \in \left\{ 0, 1 \right\}, i = 1, 2, \cdots, N$，可以应用极大似然估计法估计模型参数，从而得到Logistic回归模型。 设： \begin{align*} \\& P \left( Y =1 | x \right) = \pi \left( x \right) ,\quad P \left( Y =0 | x \right) = 1 - \pi \left( x \right) \end{align*}似然函数为： \begin{align*} \\& l \left( w \right) = \prod_{i=1}^{N} P \left( y_{i} | x_{i} \right) \\ & = P \left( Y = 1 | x_{i} , w \right) \cdot P \left( Y = 0 | x_{i}, w \right) \\ & = \prod_{i=1}^{N} \left[ \pi \left( x_{i} \right) \right]^{y_{i}}\left[ 1 - \pi \left( x_{i} \right) \right]^{1 - y_{i}}\end{align*}对数似然函数为： \begin{align*} \\& L \left( w \right) = \log l \left( w \right) \\ & = \sum_{i=1}^{N} \left[ y_{i} \log \pi \left( x_{i} \right) + \left( 1 - y_{i} \right) \log \left( 1 - \pi \left( x_{i} \right) \right) \right] \\ & = \sum_{i=1}^{N} \left[ y_{i} \log \dfrac{\pi \left( x_{i} \right)}{1- \pi \left( x_{i} \right)} + \log \left( 1 - \pi \left( x_{i} \right) \right) \right] \\ & = \sum_{i=1}^{N} \left[ y_{i} \left( w \cdot x_{i} \right) - \log \left( 1 + \exp \left( w \cdot x \right) \right) \right]\end{align*}对$L(w)$求极大值，得到$w$的估计值。这样，问题就变成了以对数似然函数为目标函数的最优化问题。Logistic回归学习中通常采用的方法是梯度下降法和拟牛顿法。 假设$w$的极大似然估计值是$\hat{w}$，则学得的Logistic回归模型为： \begin{align*} \\& P \left( Y = 1 | x \right) = \dfrac{\exp{\left(\hat{w} \cdot x \right)}}{1+\exp{\left( \hat{w} \cdot x \right)}}\\& P \left( Y = 0 | x \right) =\dfrac{1}{1+\exp{\left( \hat{w} \cdot x \right)}}\end{align*}多项 Logistic 回归模型可将上面介绍的二项分类Logistic回归模型推广为多项Logistic回归模型，用于多类分类。 假设离散型随机变量$Y$的取值集合$\left\{ 1, 2, \cdots, K \right\}$，则多项逻辑斯谛回归模型为： \begin{align*} \\& P \left( Y = k | x \right) = \dfrac{\exp{\left(w_{k} \cdot x \right)}}{1+ \sum_{k=1}^{K-1}\exp{\left( w_{k} \cdot x \right)}}, \quad k=1,2,\cdots,K-1 \\ & P \left( Y = K | x \right) = 1 - \sum_{k=1}^{K-1} P \left( Y = k | x \right) \\ & = 1 - \sum_{k=1}^{K-1} \dfrac{\exp{\left(w_{k} \cdot x \right)}}{1+ \sum_{k=1}^{K-1}\exp{\left( w_{k} \cdot x \right)}} \\ & = \dfrac{1}{1+ \sum_{k=1}^{K-1}\exp{\left( w_{k} \cdot x \right)}}\end{align*}二项Logistic回归的参数估计法也可以推广到多项Logistic回归。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Logistic回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker命令使用记录]]></title>
    <url>%2Fposts%2F41898%2F</url>
    <content type="text"><![CDATA[容器启动Docker容器： 注：# tensorcomprehensions是用户名，tc-cuda9.0-cudnn7.1-ubuntu16.04-devel是仓库名，latest是标签 1sudo nvidia-docker run -ti tensorcomprehensions/tc-cuda9.0-cudnn7.1-ubuntu16.04-devel:latest bash 查看当前系统中容器的列表： 12docker ps -a # 查看包括所有容器docker ps # 查看正在运行的容器 删除容器： 1docker rm CONTAINER ID 重新启动已经停止的容器： 12sudo docker start CONTAINER ID # 也可以使用 docker restartsudo docker attach CONTAINER ID # 附着到容器的会话上 重启Docker： 1systemctl restart docker 新建容器： 1docker run --name OCR -it deeplearning:v1 bash 镜像列出镜像： 1sudo docker images]]></content>
      <categories>
        <category>Docker命令使用纪录</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu 16.04更换apt源为国内源]]></title>
    <url>%2Fposts%2F9484%2F</url>
    <content type="text"><![CDATA[直接使用国外源的话太慢了，试了几个国内的镜像源，感觉还是阿里源最快。现在北京时间2019年3月12日有效。 首先备份 sources.list： 1cp /etc/apt/sources.list /etc/apt/sources.list.bak 删除 /etc/apt/sources.list 下的内容，修改为： 12345678910deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse 然后更新源试试： 12sudo apt-get updatesudo apt-get upgrade]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树]]></title>
    <url>%2Fposts%2F51507%2F</url>
    <content type="text"><![CDATA[决策树模型通常包括3个步骤： 特征选择 决策树的生成 决策树的修剪 特征选择特征选择在于选取对训练数据具有分类能力的特征，这样可以提高决策树学习的效率。通常特征选择的准则是信息增益或信息增益比。 信息增益熵表示随机变量不确定性的度量。设$X$是一个取有限个值的离散随机变量，其概率分布为 \begin{align} P \left( X = x_{i} \right) = p_{i}, \quad i =1, 2, \cdots, n \end{align}则随机变量$X​$的熵定义为 \begin{align} H \left( X \right) = H \left( p \right) = - \sum_{i=1}^{n} p_{i} \log p_{i} \end{align}其中，若$p_{i}=0$，则定义$0 \log 0 = 0$。 若$p_i = \frac1n​$，则 \begin{align*} \\ & H \left( p \right) = - \sum_{i=1}^{n} p_{i} \log p_{i} \\ & = - \sum_{i=1}^{n} \dfrac{1}{n} \log \dfrac{1}{n} \\ & = \log n\end{align*}从定义可验证 \begin{align*} \\ & 0 \leq H \left( p \right) \leq \log n\end{align*}在随机变量$X$给定的条件下随机变量$Y$的条件熵为： \begin{align*} \\ & H \left( Y | X \right) = \sum_{i=1}^{n} p_{i} H \left( Y | X = x_{i} \right) \end{align*}即，$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望。其中，$p_{i}=P \left( X = x_{i} \right), i= 1,2,\cdots,n$，条件熵$H \left( Y | X \right)$表示在已知随机变量$X$的条件下随机变量$Y​$的不确定性。 特征$A$对训练集$D$的信息增益为： \begin{align*} \\ & g \left( D, A \right) = H \left( D \right) - H \left( D | A \right) \end{align*}一般地，熵$H(Y)$与条件熵$H(Y|X)$之差称为互信息。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。 设训练数据集为$D$，$\left| D \right|$表示其样本容量，即样本个数。设有$K$个类$C_{k}, k=1,2,\cdots,K$，$\left| C_{k} \right|$为属于类$C_{k}$的样本的个数，$\sum_{k=1}^{K} \left| C_{k} \right| = \left| D \right|$。设特征$A$有$n$个不同的特征取值$\left\{ a_{1},a_{2},\cdots,a_{n}\right\}$，根据特征$A$的取值将$D$划分为$n$个子集$D_{1},D_{2},\cdots,D_{n}$，$\left| D_{i} \right|$为$D_{i}$的样本数，$\sum_{i=1}^{n}\left| D_{i} \right| = \left| D \right|$。记子集$D_{i}$中属于类$C_{k}$的样本的集合为$D_{ik}$，即$D_{ik} = D_{i} \cap C_{k}$，$\left| D_{ik} \right|$为$D_{ik}$的样本个数。 信息增益算法输入：训练数据集$D$和特征$A$ 输出：特征$A$对训练数据集$D$的信息增益$g(D, A)$ 计算训练数据集$D$的经验熵$H(D)$ \begin{align*} \\ & H \left( D \right) = -\sum_{k=1}^{K} \dfrac{\left|C_{k}\right|}{\left| D \right|}\log_{2}\dfrac{\left|C_{k}\right|}{\left| D \right|} \end{align*} 计算特征$A$对 数据集$D$的经验条件熵$H(D|A)$ \begin{align*} \\ & H \left( D | A \right) = \sum_{i=1}^{n} \dfrac{\left| D_{i} \right|}{\left| D \right|} H \left( D_{i} \right) \\ & = \sum_{i=1}^{n} \dfrac{\left| D_{i} \right|}{\left| D \right|} \sum_{k=1}^{K} \dfrac{\left| D_{ik} \right|}{\left| D_{i} \right|} \log_{2} \dfrac{\left| D_{ik} \right|}{\left| D_{i} \right|}\end{align*} 计算信息增益 \begin{align*} \\ & g \left( D, A \right) = H \left( D \right) - H \left( D | A \right) \end{align*} 信息增益比以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比可以对这一问题进行校正，这是特征选择的另一准则。 特征$A$对训练集$D$的信息增益比定义为其信息增益$g\left( D, A \right)$与训练数据集$D$关于特征$A$的经验熵$H_{A}\left(D\right)​$之比： \begin{align*} \\ & g_{R} \left( D, A \right) = \dfrac{g \left( D, A \right)}{H_{A} \left(D\right)}\end{align*}其中， \begin{align*} \\ & H_{A} \left( D \right) = -\sum_{i=1}^{n} \dfrac{\left|D_{i}\right|}{\left|D\right|}\log_{2}\dfrac{\left|D_{i}\right|}{\left|D\right|}\end{align*}决策树的生成ID3 算法ID3算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。 输入：训练数据集$D$，特征$A$，阈值$\varepsilon$输出：决策树$T$ 若$D$中所有实例属于同一类$C_{k}$，则$T$为单结点树，并将类$C_{k}$作为该结点的类标记，返回$T$； 若$A = \emptyset$，则$T$为单结点树，并将$D$中实例数最大的类$C_{k}$作为该结点的类标记，返回$T​$； 否则，计算$A$中各特征$D$的信息增益，选择信息增益最大的特征$A_{g}$ \begin{align} \ & A{g} = \arg \max{A} g \left( D, A \right) \end{align} 如果$A_{g}$的信息增益小于阈值$\varepsilon$，则置$T$为单结点树，并将$D$中实例数量最大的类$C_{k}$作为该结点的类标记，返回$T$; 否则，对$A_{g}​$的每一个可能值$a_{i}​$，依$A_{g}=a_{i}​$将$D​$分割为若干非空子集$D_{i}​$，将$D_{i}​$中实例数对大的类作为标记，构建子结点，由结点及其子结点构成树$T​$，返回$T​$； 对第$i$个子结点，以$D_{i}$为训练集，以$A-\left\{A_{g}\right\}$为特征集，递归地调用步1.～步5.，得到子树$T_{i}$，返回$T_{i}$。 C4.5 的生成算法C4.5算法与ID3算法相似，只是将信息增益改为信息增益比来选择特征。 决策树的剪枝在决策树学习中将已生成的树进行简化的过程称为剪枝。决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。 设树$T$的叶结点个数为$\left| T \right|$，$t$是树$T$的叶结点，该叶结点有$N_{t}$个样本点，其中$k$类的样本点有$N_{tk}$个，$k=1,2,\cdots,K$，$H_{t}\left(T\right)$为叶结点$t$上的经验熵，$\alpha \geq 0$为参数，则决策树的损失函数可以定义为： \begin{align*} \\ & C_{\alpha} \left( T \right) = \sum_{t=1}^{\left| T \right|} N_{t} H_{t} \left( T \right) + \alpha \left| T \right| \end{align*}其中经验熵为： \begin{align*} \\ & H_{t} \left( T \right) = - \sum_{k} \dfrac{N_{tk}}{N_{t}} \log \dfrac{N_{tk}}{N_{t}} \end{align*}在损失函数中，记： \begin{align*} \\ & C \left( T \right) = \sum_{t=1}^{\left| T \right|} N_{t} H_{t} \left( T \right) = - \sum_{t=1}^{\left| T \right|} \sum_{k=1}^{K} N_{tk} \log \dfrac{N_{tk}}{N_{t}} \end{align*}这时有： \begin{align*} \\ & C_{\alpha} \left( T \right) = C \left( T \right) + \alpha \left| T \right| \end{align*}其中，$C \left( T \right)$表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，$\left| T \right|$表示模型复杂度，参数$\alpha \geq 0​$控制两者之间的影响。 决策树的剪枝算法输入：决策树$T$，参数$\alpha$输出：修剪后的子树$T_{\alpha}​$ 计算每个结点的经验熵 。 递归地从树的叶结点向上回缩。 设一组叶结点回缩到其父结点之前与之后的整体树分别为$T_{B}$与$T_{A}$，其对应的损失函数值分别是$C_{\alpha} \left( T_{B} \right)$与$C_{\alpha} \left( T_{A} \right)$，如果 \begin{align} \ & C_{\alpha} \left( T_{A} \right) \leq C_{\alpha} \left( T_{B} \right) \end{align}则进行剪枝，即将父结点变为新的叶结点。 返回2.，直到不能继续为止，得到损失函数最小的子树$T_{\alpha}$。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装Docker和Nvidia-Docker]]></title>
    <url>%2Fposts%2F35246%2F</url>
    <content type="text"><![CDATA[安装DockerDocker官方为了简化流程，提供了一套便捷的安装脚本，在Ubuntu上可以使用脚本自动安装Docker CE： 12curl -fsSL get.docker.com -o get-docker.shsudo sh get-docker.sh --mirror Aliyun 启动Docker CE： 12sudo systemctl enable dockersudo systemctl start docker 建立Docker用户组： 12sudo groupadd dockersudo usermod -aG docker $USER 测试Docker是否安装正确： 1docker run hello-world 安装Nvidia-Docker123456789101112131415# If you have nvidia-docker 1.0 installed: we need to remove it and all existing GPU containersdocker volume ls -q -f driver=nvidia-docker | xargs -r -I&#123;&#125; -n1 docker ps -q -a -f volume=&#123;&#125; | xargs -r docker rm -fsudo yum remove nvidia-docker# Add the package repositoriesdistribution=$(. /etc/os-release;echo $ID$VERSION_ID)curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | \ sudo tee /etc/yum.repos.d/nvidia-docker.repo# Install nvidia-docker2 and reload the Docker daemon configurationsudo yum install -y nvidia-docker2sudo pkill -SIGHUP dockerd# Test nvidia-smi with the latest official CUDA imagedocker run --runtime=nvidia --rm nvidia/cuda:9.0-base nvidia-smi]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[提升算法 AdaBoost 算法]]></title>
    <url>%2Fposts%2F48763%2F</url>
    <content type="text"><![CDATA[提升算法的基本思路提升算法是从弱学习算法出发，反复学习，得到一系列弱分类器（又称为基本分类器），然后组合这些弱分类器，构成一个强分类器。 有两个问题： 在每一轮如何改变训练数据的权值或概率分布 如何将弱分类器组合成一个强分类器 对于第一个问题，AdaBoost的做法是提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。对于第二个问题，AdaBoost采取加权多数表决的方法。即加大分类误差率小的弱分类器的权值，减小分类误差率大的弱分类器的权值。 AdaBoost 算法输入：训练数据集$T = \left\{ \left( x_{1}, y_{1} \right), \left( x_{2}, y_{2} \right), \cdots, \left( x_{N}, y_{N} \right) \right\}$，其中$x_{i} \in \mathcal{X} \subseteq R^{n}, y_{i} \in \mathcal{Y} = \left\{ +1, -1 \right\}, i = 1, 2, \cdots, N$；弱学习算法。 输出：最终分类器$G(x)$。 初始化训练数据的权值分布 \begin{align*} \\ & D_{1}=\left(w_{11},w_{12},\cdots,w_{1N}\right), \quad w_{1i} = \dfrac{1}{N}, \quad i=1,2,\cdots,N\end{align*} 对$m=1,2,\cdots,M​$ （a）使用具有权值分布$D_m$的训练数据集学习，得到基本分类器 \begin{align*} \\ & G_{m}\left(x\right): \mathcal{X} \to \left\{ -1, +1\right\} \end{align*}（b）（b）计算$G_m(x)$在训练数据集上的分类误差率 \begin{align*} \\& e_{m} = P\left(G_{m}\left(x_{i}\right) \neq y_{i}\right) \\ & = \sum_{i=1}^{N} w_{mi} I \left(G_{m}\left(x_{i}\right) \neq y_{i} \right) \end{align*}（c）计算$G_m(x)​$的系数 \begin{align*} \\ & \alpha_{m} = \dfrac{1}{2} \log \dfrac{1-e_{m}}{e_{m}} \end{align*}（d）更新训练数据集的权值分布 \begin{align*} \\ & D_{m+1}=\left(w_{m+1,1},\cdots,w_{m+1,i},\cdots,w_{m+1,N}\right) \\ & w_{m+1,i} = \dfrac{w_{mi}}{Z_{m}} \exp \left(- \alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right), \\ & \quad \quad = \left\{ \begin{aligned} \ & \dfrac{w_{mi}}{Z_{m}} \exp \left(- \alpha_{m} \right), G_{m}\left(x_{i}\right) = y_{i} \\ & \dfrac{w_{mi}}{Z_{m}} \exp \left( \alpha_{m} \right), G_{m}\left(x_{i}\right) \neq y_{i} \end{aligned} \right. \quad i=1,2,\cdots,N \end{align*}​ 其中，$Z_m​$是规范化因子 \begin{align*} \\ & Z_{m}＝ \sum_{i=1}^{N} w_{mi} \exp \left(- \alpha_{m} y_{i}, G_{m}\left(x_{i}\right)\right)\end{align*} 构建基本分类器的线性组合 \begin{align*} \\ & f \left( x \right) = \sum_{m=1}^{M} \alpha_{m} G_{m} \left( x \right) \end{align*}得到最终分类器 \begin{align*} \\ & G\left(x\right) = sign\left(f\left(x\right)\right)=sign\left(\sum_{m=1}^{M} \alpha_{m} G_{m} \left( x \right)\right) \end{align*} AdaBoost 算法的解释AdaBoost 算法可以认为是模型为加法模型、损失函数为指数函数、学习算法为前向分布算法时的二类分类学习方法。 前向分布算法前向分布算法：输入：训练数据集$T = \left\{ \left( x_{1}, y_{1} \right), \left( x_{2}, y_{2} \right), \cdots, \left( x_{N}, y_{N} \right) \right\}$，损失函数$L\left(y,f\left(x\right)\right)$；基函数集$\left\{b\left(x;\gamma\right)\right\}$输出：加法模型$f\left(x\right)​$ 初始化$f_{0}\left(x\right)=0​$ 对$m=1,2,\cdots,M$ （a） 极小化损失函数 \begin{align} \ & \left(\beta_{m},\gamma_{m}\right) = \arg \min{\beta,\gamma} \sum_{i=1}^{N} L \left( y_{i},f_{m-1} \left(x_{i}\right) + \beta b\left(x_{i};\gamma \right)\right) \end{align} 得到参数$\beta_{m},\gamma_{m}$ （b）更新 \begin{align*} \\& f_{m} \left(x\right) = f_{m-1} \left(x\right) + \beta_{m} b\left(x;\gamma_{m}\right) \end{align*} （c）得到加法模型 \begin{align*} \\ & f \left( x \right) = f_{M} \left( x \right) = \sum_{m=1}^{M} \beta_{m} b \left( x; \gamma_{m} \right) \end{align*}前向分布算法与 AdaBoostAdaBoost 算法是前向分布加法算法的特例。这时，模型是由基本分类器组成的加法模型，损失函数是指数函数。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>提升算法</tag>
        <tag>集成学习</tag>
        <tag>AdaBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Linux的Jupyter Notebook中安装R内核]]></title>
    <url>%2Fposts%2F61698%2F</url>
    <content type="text"><![CDATA[普通电脑已经无法满足日益增长的计算需求了，于是在服务器上安装了R，还没有在Jupyter上写过Python之外的语言，这次来试试在Jupyter上写R。 网上很多方法是直接安装devtools包的，利用其中的的 install_github() 函数用于从Github上安装R内核。但如果直接install.packages的话，反正我是死活装不上这个包，提示各种错误。这里分享我的安装方法，给大家提供一个参考吧。 安装devtools前的准备直接在终端执行如下命令： 1234567sudo apt-get install gfortransudo apt-get install build-essential sudo apt-get install libxt-dev sudo apt-get install libcurl4-openssl-devsudo apt-get install libxml++2.6-devsudo apt-get install libssl-devsudo R # 进入R 安装devtools添加dependencies = T同时安装必要的依赖： 12install.packages(&quot;devtools&quot;, dependencies = T)library(devtools) # 如果没有任何提示就代表成功了 安装R kernel并注册从Github安装R kernel： 1devtools::install_github(&apos;IRkernel/IRkernel&apos;) 在R中注册激活R kernel： 1IRkernel::installspec() 然后打开Jupyter，看到可以新建R notebook了：]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>R语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多机使用SSH key连接至Github Page]]></title>
    <url>%2Fposts%2F35104%2F</url>
    <content type="text"><![CDATA[设置Github在本地机器中安装好Git和Hexo之后，在终端或是Git Bash中执行如下代码： 12git config --global user.name &quot;YOUR NAME&quot; # Github注册账户名git config --global user.email &quot;YOUR EMAIL ADDRESS&quot; # Github注册邮箱 验证Github输入以下命令，生成SSH key： 1ssh-keygen -t rsa -b 4096 -C &quot;YOUR EMAIL ADDRESS&quot; # Github注册邮箱 将 SSH key 添加到 ssh-agent： 123eval &quot;$(ssh-agent -s)&quot; # 开启 ssh-agentssh-add ~/.ssh/id_rsa # 添加SSH key到 ssh-agentclip &lt; ~/.ssh/id_rsa.pub # 将SSH key复制出来 登录Github，依次点击自己的头像，Settings，SSH and GPG keys， Add SSH key， 在 Title 这里输入 Key 的label，我的是Linux-PC，将SSH key添加到Github账户。 然后测试SSH连接，在终端或Git Bash中执行： 1ssh -T git@github.com 如果返回的是You’ve successfully authenticated, but GitHub does not provide shell access，那么就表示已经成功了。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[V2Ray的Linux客户端配置]]></title>
    <url>%2Fposts%2F26401%2F</url>
    <content type="text"><![CDATA[V2Ray 是 Project V 下的一个工具。Project V 是一个包含一系列构建特定网络环境工具的项目，而 V2Ray 属于最核心的一个。我们可以将其理解为类似于Shadowsocks的代理软件，但V2Ray的功能更强大，而且更不容易被墙检测到。V2Ray在win和mac下都有GUI客户端可使用，但在Linux下没有，只能通过修改配置来使用，由于客户端配置比较复杂，本文提供一种偷懒版配置方法，即在win或mac下将配置好的config文件复制一份到Linux下使用。 安装V2RayV2Ray官方提供了一个自动化安装的脚本，直接使用这个脚本安装即可，在终端中运行： 1234wget https://install.direct/go.sh # 下载脚本sudo bash go.sh # 执行脚本sudo systemctl start v2ray # 启动V2Ray，测试是否安装成功sudo systemctl stop v2ray # 停止V2Ray 配置V2Ray在win或mac中找到已配置好的config.json，复制到/etc/v2ray/config.json后执行： 1sudo systemctl restart v2ray # 重启V2Ray 现在有两种方法启动V2Ray的代理，先介绍第一种，在系统层面上启动代理，这种算是全局代理了。进入本地计算机设置中的网络——系统代理，我这里监听的端口是2333，协议是socks，如下图： 另一种方法是在Chrome或Firefox中安装SwitchyOmega来管理，使用SwitchyOmega网上已经很多了，这里就不再讲了。等全部设置完毕后，可以试试能不能上油管，享受自由的互联网吧！ 如果想访问国内网站不走代理的话，建议在Chrome下配合SwitchyOmega来使用，用自动切换模式自由度更高，而且能够设置规则列表，实时更新被墙的网址。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>V2Ray</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[序列最小最优算法(SMO算法)]]></title>
    <url>%2Fposts%2F3981%2F</url>
    <content type="text"><![CDATA[序列最小最优化算法(sequential minimal optimization, SMO)算法：1998年由Platt提出。支持向量机的学习问题可以形式化为求解凸二次规划问题，这样的凸二次规划问题具有全局最优解，并且有许多最优化算法可以用于这一问题的求解。但是当训练样本容量很大时，这些算法往往变得非常低效，以致无法使用。所以，如何高效地实现支持向量机学习就成为一个重要的问题。 SMO算法是一种启发式算法，基本思路如下： 如果所有变量的解都满足此最优化问题的KKT条件，那么得到解。 否则，选择两个变量，固定其它变量，针对这两个变量构建一个二次规划问题，称为子问题，可通过解析方法求解，提高了计算速度。 子问题的两个变量：一个是违反KKT条件最严重的那个，另一个由约束条件自动确定。 SMO算法包括两个部分： 求解两个变量二次规划的解析方法 选择变量的启发式方法 序列最小最优化（sequential minimal optimization，SMO）算法要解如下凸二次规划的对偶问题： \begin{align*} \\ & \min_{\alpha} \dfrac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} K \left( x_{i}, x_{j} \right) - \sum_{i=1}^{N} \alpha_{i} \\ & s.t. \sum_{i=1}^{N} \alpha_{i} y_{i} = 0 \\ & 0 \leq \alpha_{i} \leq C , \quad i=1,2, \cdots, N \end{align*}子问题有两个变量，一个是违反KKT条件最严重的那个，另一个由约束条件自动确定。子问题的两个变量只有一个是自由变量，如果$\alpha_2$确定，那么$\alpha_1$也随之确定，所以子问题中同时更新两个变量。 SMO算法输入：训练数据集$T = \left\{ \left( x_{1}, y_{1} \right), \left( x_{2}, y_{2} \right), \cdots, \left( x_{N}, y_{N} \right) \right\}$，其中$x_{i} \in \mathcal{X} = R^{n}, y_{i} \in \mathcal{Y} = \left\{ +1, -1 \right\}, i = 1, 2, \cdots, N$，精度$\varepsilon$； 输出：近似解$\hat \alpha​$ 取初始值$\alpha^{0} = 0​$，令$k = 0​$； 选取优化变量$\alpha_{1}^{\left( k \right)},\alpha_{2}^{\left( k \right)}​$，求解 \begin{align*} \\ & \min_{\alpha_{1}, \alpha_{2}} W \left( \alpha_{1}, \alpha_{2} \right) = \dfrac{1}{2} K_{11} \alpha_{1}^{2} + \dfrac{1}{2} K_{22} \alpha_{2}^{2} + y_{1} y_{2} K_{12} \alpha_{1} \alpha_{2} \\ & \quad\quad\quad\quad\quad\quad - \left( \alpha_{1} + \alpha_{2} \right) + y_{1} \alpha_{1} \sum_{i=3}^{N} y_{i} \alpha_{i} K_{i1} + y_{2} \alpha_{2} \sum_{i=3}^{N} y_{i} \alpha_i K_{i2} \\ & s.t. \quad \alpha_{1} + \alpha_{2} = -\sum_{i=3}^{N} \alpha_{i} y_{i} = \varsigma \\ & 0 \leq \alpha_{i} \leq C , \quad i=1,2 \end{align*} 求得最优解$\alpha_{1}^{\left( k＋1 \right)},\alpha_{2}^{\left( k+1 \right)}$，更新$\alpha$为$\alpha^{\left( k+1 \right)}$； 若在精度$\varepsilon​$范围内满足停机条件 \begin{align*} \\ & \sum_{i=1}^{N} \alpha_{i} y_{i} = 0 \\ & 0 \leq \alpha_{i} \leq C, i = 1, 2, \cdots, N \\ & \end{align*} \begin{align*} y_{i} \cdot g \left( x_{i} \right) = \left\{ \begin{aligned} \ & \geq 1, \left\{ x_{i} | \alpha_{i} = 0 \right\} \\ & = 1, \left\{ x_{i} | 0 < \alpha_{i} < C \right\} \\ & \leq 1, \left\{ x_{i} | \alpha_{i} = C \right\} \end{aligned} \right.\end{align*}其中， g(x_i) = \sum_{j=1}^N \alpha_jy_jK(x_j,x_i) + b则转(4)；否则令$k = k + 1$，转(2)； 4.取$\hat \alpha = \alpha^{\left( k + 1 \right)}​$。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[非线性支持向量机与核函数]]></title>
    <url>%2Fposts%2F13688%2F</url>
    <content type="text"><![CDATA[核技巧非线性分类问题如果能用$R^n$中的一个超曲面将正负例正确分开，则称这个问题为非线性可分问题。对应下图的例子，通过变换将左图中椭圆变换称右图中的直线，将非线性分类问题变换为线性分类问题。 核技巧的基本想法是通过一个非线性变换将输入空间（欧式空间$R^n$或离散集合）对应于一个特征空间（希尔伯特空间$\mathcal{H}$），使得在输入空间$R^n$中的超曲面模型对应于特征空间$\mathcal{H}​$中的超平面模型（支持向量机）。 核函数的定义设$\mathcal{X}$是输入空间（欧式空间$R^n$或离散集合），$\mathcal{H}$是特征空间（希尔伯特空间），如果存在一个从$\mathcal{X}$到$\mathcal{H}$的映射： \begin{align*} \\& \phi \left( x \right) : \mathcal{X} \to \mathcal{H} \end{align*}使得对所有$x,z \in \mathcal{X}$，函数$K \left(x, z \right)$满足条件： \begin{align*} \\ & K \left(x, z \right) = \phi \left( x \right) \cdot \phi \left( z \right) \end{align*}则称$K \left(x, z \right)$为核函数，$\phi \left( x \right)$为映射函数，式中$\phi \left( x \right) \cdot \phi \left( z \right)$为$\phi \left( x \right)$和$\phi \left( z \right)$的内积。 核函数在支持向量机中的应用在线性支持向量机的对偶问题中，无论是目标函数还是决策函数都只涉及输入实例与实例之间的内积，它们都可以用核函数$K(x_i,x_j) = \phi(x_i) \cdot \phi(x_j)$来代替。这等价于经过映射函数$\phi$将原来的输入空间变换到一个新的特征空间，将输入空间中的内积 $x_i \cdot x_j$变换为特征空间中的内积$\phi(x_i) \cdot \phi(x_j)$，在新的特征空间里从训练样本中学习线性支持向量机。 在实际应用中，往往依赖领域知识直接选择核函数，核函数选择的有效性需要通过实验验证。 常用核函数多项式核函数 \begin{align*} \\& K \left( x, z \right) = \left( x \cdot z + 1 \right)^{p} \end{align*}对应的支持向量机是一个p次多项式分类器 高斯核函数 \begin{align*} \\& K \left( x, z \right) = \exp \left( - \dfrac{\| x - z \|^{2}}{2 \sigma^{2}} \right) \end{align*}非线性支持向量机将线性支持向量机扩展到非线性支持向量机，只需将线性支持向量机对偶形式中的内积换成核函数即可。 非线性支持向量机学习算法输入：训练数据集线性可分训练数据集 $T={(x_1, y_1), (x_2, y_2),…,(x_N, y_N)}$，其中 $x_i \in R^n$，$y_i \in \{-1, 1\}$，$i=1,2,…,N​$。 输出：分类决策函数 选取适当的核函数$K(x, z)$和适当的参数$C$，构造并求解最优化问题 \begin{align*} \\ & \min_{\alpha} \dfrac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} K \left( x_{i}, x_{j} \right) - \sum_{i=1}^{N} \alpha_{i} \\ & s.t. \sum_{i=1}^{N} \alpha_{i} y_{i} = 0 \\ & 0 \leq \alpha_{i} \leq C , \quad i=1,2, \cdots, N \end{align*} 求得最优解$\alpha^{\star} = \left( \alpha_{1}^{\star}, \alpha_{1}^{\star}, \cdots, \alpha_{N}^{\star} \right)$。 选择$\alpha^{\star}$的一个分量$0 &lt; \alpha_{j}^{\star} &lt; C​$，计算 \begin{align*} \\ & b^{*} = y_{j} - \sum_{i=1}^{N} \alpha_{i}^{*} y_{i} K \left( x_{i}, x_{j} \right) \end{align*} 构造决策函数 \begin{align*} \\& f \left( x \right) = sign \left( \sum_{i=1}^{N} \alpha_{i}^{*} y_{i} K \left( x_{i}, x_{j} \right) + b^{*} \right) \end{align*}当$K(x, z)$是正定核函数时，第一步中的式子是凸二次规划问题，解是存在的。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>SVM</tag>
        <tag>核函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo中渲染MathJax数学公式的问题]]></title>
    <url>%2Fposts%2F2405%2F</url>
    <content type="text"><![CDATA[在学习机器学习算法的过程中有大量的公式需要渲染，原本使用的是Hexo默认的”hexo-renderer-marked”引擎，现在感觉已经力不从心了，好多复杂的公式都无法渲染，网上关于这个问题的解决方法其实有很多，但这里还是po出我觉得最靠谱的一种方法吧。 替换默认渲染引擎hexo-renderer-kramed 是 hexo-renderer-marked 的Fork修改版，只是在渲染部分进行了修改，首先替换默认渲染引擎： 12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save 解决语义冲突的问题hexo s 启动本地服务器后看到行间公式已经渲染成功了，但一些行内公式还是无法渲染。原因是hexo-renderer-kramed 也有语义冲突的问题。于是来到hexo根目录下，打开node_modules\kramed\lib\rules\inline.js 作相应的修改： 1234//escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/, # 第11行escape: /^\\([`*\[\]()#$+\-.!_&gt;])/ //em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, # 第20行em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/ 重启本地服务器后可以看到问题基本解决了，不过行内公式中针对两个 * 的语义冲突依然存在，查了一圈，发现这个问题目前也没有什么解决方法，不过问题也不太大，直接用 \star 代替就好了。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jupyter服务器端多用户配置]]></title>
    <url>%2Fposts%2F34463%2F</url>
    <content type="text"><![CDATA[实验室的服务器目前只有一个jupyter端口，而现在有多用户使用jupyter在服务器操作的需求，大家不方便用一个账户，于是就创建了多个账户。 原理其实很简单，使用不同的配置文件运行jupyter即可，下面是启动jupyter的代码： 1jupyter notebook --config &lt;config_file_path&gt; 然后使用ipython配置密码，这里配置的是哈希之后的密码，进入ipython输入： 12from notebook.auth import passwdpasswd() 复制生成的哈希密码，然后编辑配置文件： 1vim ~/.jupyter/jupyter_notebook_config.py 添加： 1c.NotebookApp.password = &apos;sha1:...&apos; # 刚才复制的哈希密码]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[拉格朗日对偶性]]></title>
    <url>%2Fposts%2F22545%2F</url>
    <content type="text"><![CDATA[在约束最优化问题中，常常利用拉格朗日对偶性将原始问题转换为对偶问题，通过解对偶问题而得到原始问题的解。 原始问题假设$f(x),c_i(x), h_j(x)​$是定义在$R^n​$上的连续可微函数，考虑约束最优化问题 min_{x \in R^n}f(x) s.t. \qquad c_i(x) \leq 0, \qquad i = 1,2,...,k h_j(x) = 0, \quad j=1,2,...,l首先，引入拉格朗日函数： L(x, \alpha, \beta) = f(x)+\sum_{i=1}^k \alpha_i c_i(x)+ \sum_{j=1}^l \beta_jh_j(x)这里，$x = (x^{(1)},x^{(1)},…, x^{(n)} )^T \in R^n$，$\alpha_i, \beta_i$是拉格朗日乘子，$\alpha_i \geq 0$，考虑x的函数： \theta_P(x) = max_{\alpha, \beta: \alpha_i \geq0}L(x, \alpha, \beta)考虑极小化问题： min_x\theta_P(x)= min_xmax_{\alpha, \beta: \alpha_i \geq0}L(x, \alpha, \beta)这与原始最优化问题等价，即它们有着相同的解，将其称为广义拉格朗日函数的极小极大问题。 对偶问题定义： \theta_D(\alpha, \beta) = min_xL(x, \alpha, \beta)再考虑极大化$\theta_D(\alpha, \beta)​$，即原始最优化问题的对偶问题： max_{\alpha, \beta: \alpha_i \geq 0}\theta_D(\alpha, \beta)= max_{\alpha, \beta: \alpha_i \geq 0}min_xL(x, \alpha, \beta) s.t. \qquad \alpha_i \geq 0, \qquad i = 1,2,...,k原始问题和对偶问题的关系若原始问题和对偶问题都有最优值，则： max_{\alpha, \beta: \alpha_i \geq 0}min_xL(x, \alpha, \beta) \leq min_xmax_{\alpha, \beta: \alpha_i \geq0}L(x, \alpha, \beta)在某些条件下，上式的等号成立，这时可以用解对偶问题替代解原始问题。 对原始问题和对偶问题，假设函数$f(x)​$和$c_i(x)​$是凸函数，$h_j(x)​$是仿射函数，并且不等式约束$c_i(x)​$是严格可行的，则$x^{\star}​$和$\alpha^{\star}，\beta^{\star}​$分别是原始问题和对偶问题的解的充分必要条件是$x^{\star}​$，$\alpha^{\star}，\beta^{\star}​$满足下面的Karush-Kuhn-Tucker (KKT)条件： \nabla_xL(x^{\star},\alpha^{\star}，\beta^{\star}) = 0 \alpha_i^{\star}c_i(x^{\star}) = 0, \qquad i=1,2,...,k c_i(x^{\star}) \leq 0, \qquad i=1,2,...,k \alpha_i^{\star} \geq 0 , \qquad i=1,2,...,k h_j(x^{\star}) = 0, \qquad i=1,2,...,k可以看出，若$\alpha_i^{\star} \geq 0$，则$c_i(x^{\star})=0$，$\alpha_i^{\star} \geq 0$称为KKT的对偶互补条件。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>对偶</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Python模拟鼠标(键盘)操作]]></title>
    <url>%2Fposts%2F42676%2F</url>
    <content type="text"><![CDATA[昨天在帮师兄做脑核磁共振图像的颅骨去除，使用的是一个桌面软件，需要输入所需要的文件，然后软件自动进行分割，手动做的话每例大概需要三四分钟，共两百多例，这样效率太低了，且实在是很不优雅。于是决定用python写一个脚本来自动模拟鼠标操作，使用的是pyautogui库，特此记录以便参考。 首先进入base环境安装pyautogui库： 12activate baseconda install -c jim-hart pyautogui 因为需要输入的是4个nii文件，所以首先删除不需要的文件，保证文件夹内的文件数量$\leq4$： 1234567src_dir = r"F:\0"need_file = ['t1c.nii', 't2.nii', 't1.nii', 'flair.nii']for dir in os.listdir(src_dir): dst_dir = os.path.join(src_dir, dir) for file in os.listdir(dst_dir): if (file not in need_file): os.remove(os.path.join(dst_dir, file)) 然后判断需要的4个文件是否都存在，去除不满足条件的病例。 12345678src_dir = r&quot;F:\0&quot;need_file = [&apos;t1c.nii&apos;, &apos;t2.nii&apos;, &apos;t1.nii&apos;, &apos;flair.nii&apos;]for dir in os.listdir(src_dir): dst_dir = os.path.join(src_dir, dir) filenum = len([lists for lists in os.listdir(dst_dir) if os.path.isfile(os.path.join(dst_dir, lists))]) if (filenum != 4): print(dst_dir) 使用脚本控制鼠标太快了，可能会导致应用出问题，为了减缓鼠标点击速度，pyautogui提供了中断措施，为所有的pyatutogui函数增加延迟，默认的延迟是0.1s，这里做如下调整： 1pyautogui.PAUSE = 0.6 接着确定各个按钮的坐标，坐标的位置可以用以下代码确定： 12x, y = pyautogui.position()print(x, y) 这里用到的鼠标操作主要是移动、单击、双击和多次点击： 1234pag.moveTo(x,y)pyautogui.click()pyautogui.doubleClick()pyautogui.click(clicks=10, interval=0.25) 本来是有用到滚轮操作的，但是不知道什么原因，在代码中失效，也就放弃了这个操作，用多次点击代替。pyautogui这个库还是很强大的，也支持键盘操作，只不过这里不需要键盘操作也就没有用到。 比较简单地介绍了使用python模拟鼠标（键盘）操作，以下是具体的源码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134import pyautogui as pagimport osimport timescreenWidth, screenHeight = pag.size()assert screenWidth == 1920assert screenHeight == 1080x, y = pag.position()print(x, y)pag.moveTo(700, 700)pag.PAUSE = 0.6pag.moveTo(folder_rightx, folder_righty)# pag.click(clicks=1, interval=0.25)loadx, loady = 18, 32patientx, patienty = 71, 56outputx, outputy = 1247, 666Fdiskx, Fdisky = 724, 425folderx, foldery = 695, 443folder_rightx, folder_righty = 981, 359t1_extrax, t1_extray = 712, 441t1x, t1y = 956, 380t1cx, t1cy = 956, 380+17t2x, t2y = 956, 380+35flairx, flariy = 956, 380-17load2x, load2y = 700, 700performx, performy = 121, 404next1x, next1y = 900, 724next2x, next2y = 1207, 742# set T1 as baselineblx, bly = 1230, 444for i in range(0, 1): print(&quot;current No: %d&quot; % i) try: # open add patient pag.moveTo(loadx, loady) pag.click() pag.moveTo(patientx, patienty) pag.click() # open output folder pag.moveTo(outputx, outputy) pag.click() pag.moveTo(Fdiskx, Fdisky) pag.doubleClick() pag.moveTo(folderx, foldery) pag.doubleClick() if i == 0: pag.moveTo(folder_rightx, folder_righty) else: pag.moveTo(next2x, next2y) pag.click(clicks=i, interval=0.25) pag.moveTo(folder_rightx, folder_righty) pag.doubleClick() # set T1 path pag.moveTo(blx, bly) pag.click() if i == 0: pag.moveTo(t1_extrax, t1_extray) else: pag.moveTo(next1x, next1y) pag.click(clicks=i, interval=0.25) pag.moveTo(t1_extrax, t1_extray) pag.doubleClick() pag.moveTo(t1x, t1y) pag.doubleClick() # set T1c path pag.moveTo(blx, bly+38*1) pag.click() pag.moveTo(t1cx, t1cy) pag.doubleClick() # set t2 path pag.moveTo(blx, bly+38*2) pag.click() pag.moveTo(t2x, t2y) pag.doubleClick() # set Flair path pag.moveTo(blx, bly+38*3) pag.click() pag.moveTo(flairx, flariy) pag.doubleClick() # set Template (same as T1c) pag.moveTo(blx, bly+38*4) pag.click() pag.moveTo(t1cx, t1cy) pag.doubleClick() # load pag.moveTo(load2x, load2y) pag.click() time.sleep(3) # perform pag.moveTo(performx, performy) pag.click() # performing #time.sleep(60) except: print(&quot;No.%d is Error&quot; % i) # 删除不需要的文件，保证文件夹内文件数量小于等于4src_dir = r&quot;F:\0&quot;need_file = [&apos;t1c.nii&apos;, &apos;t2.nii&apos;, &apos;t1.nii&apos;, &apos;flair.nii&apos;]for dir in os.listdir(src_dir): dst_dir = os.path.join(src_dir, dir) for file in os.listdir(dst_dir): if (file not in need_file): os.remove(os.path.join(dst_dir, file)) # 判断需要的四个文件是否都存在src_dir = r&quot;F:\0&quot;need_file = [&apos;t1c.nii&apos;, &apos;t2.nii&apos;, &apos;t1.nii&apos;, &apos;flair.nii&apos;]for dir in os.listdir(src_dir): dst_dir = os.path.join(src_dir, dir) filenum = len([lists for lists in os.listdir(dst_dir) if os.path.isfile(os.path.join(dst_dir, lists))]) if (filenum != 4): print(dst_dir)]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性可分支持向量机与软间隔最大化]]></title>
    <url>%2Fposts%2F35913%2F</url>
    <content type="text"><![CDATA[线性支持向量机线性支持向量机包含线性可分支持向量机和线性不可分支持向量机。 通常情况下，训练数据中有一些特异点，线性不可分意味着这些特异点$(x_i, y_i)​$不能满足函数间隔大于等于1的约束条件。为了解决这个问题，对每个样本点引进一个松弛变量 $\xi_i \geq 0​$，使得函数间隔加上松弛变量大于等于1，约束条件变为： y_{i} \left( w \cdot x_{i} + b \right) \geq 1 - \xi_{i}同时，对每个松弛变量 $\xi_i​$ 都支付一个代价 $\xi_i​$，目标函数就由原来的 $\frac12 ||w||^2​$变成 \dfrac{1}{2} \| w \|^{2} + C \sum_{i=1}^{N} \xi_{i}其中$C&gt;0​$为惩罚参数。 目标函数有两层含义: 使 $\frac12 ||w||^2$尽量小即间隔尽量大 使误分类点的个数尽量小 这样就将线性不可分的支持向量机的学习问题变成如下凸二次规划问题： \begin{align*} \\ & \min_{w,b,\xi} \quad \dfrac{1}{2} \| w \|^{2} + C \sum_{i=1}^{N} \xi_{i} \\ & s.t. \quad y_{i} \left( w \cdot x_{i} + b \right) \geq 1 - \xi_{i} \\ & \xi_{i} \geq 0, \quad i=1,2, \cdots, N \end{align*}学习得到分离超平面为： \begin{align*} \\& w^{*} \cdot x + b^{*} = 0 \end{align*}以及相应的分类决策函数： \begin{align*} \\& f \left( x \right) = sign \left( w^{*} \cdot x + b^{*} \right) \end{align*}称为线性支持向量机。 学习的对偶算法构建拉格朗日函数引入拉格朗日乘子$\alpha_{i} \geq 0, \mu_{i} \geq 0, i = 1, 2, \cdots, N$构建拉格朗日函数： \begin{align*} \\ & L \left( w, b, \xi, \alpha, \mu \right) = \dfrac{1}{2} \| w \|^{2} + C \sum_{i=1}^{N} \xi_{i} + \sum_{i=1}^{N} \alpha_{i} \left[- y_{i} \left( w \cdot x_{i} + b \right) + 1 - \xi_{i} \right] + \sum_{i=1}^{N} \mu_{i} \left( -\xi_{i} \right) \\ & = \dfrac{1}{2} \| w \|^{2} + C \sum_{i=1}^{N} \xi_{i} - \sum_{i=1}^{N} \alpha_{i} \left[ y_{i} \left( w \cdot x_{i} + b \right) -1 + \xi_{i} \right] - \sum_{i=1}^{N} \mu_{i} \xi_{i} \end{align*}其中，$\alpha = \left( \alpha_{1}, \alpha_{2}, \cdots, \alpha_{N} \right)^{T}$以及$\mu = \left( \mu_{1}, \mu_{2}, \cdots, \mu_{N} \right)^{T}$为拉格朗日乘子向量。 求极小对偶问题是拉格朗日函数的极大极小问题，首先求$\min_{w,b}L \left( w, b, \xi, \alpha, \mu \right)$： \begin{align*} \\ & \nabla_{w} L \left( w, b, \xi, \alpha, \mu \right) = w - \sum_{i=1}^{N} \alpha_{i} y_{i} x_{i} = 0 \\ & \nabla_{b} L \left( w, b, \xi, \alpha, \mu \right) = -\sum_{i=1}^{N} \alpha_{i} y_{i} = 0 \\ & \nabla_{\xi_{i}} L \left( w, b, \xi, \alpha, \mu \right) = C - \alpha_{i} - \mu_{i} = 0 \end{align*}得 \begin{align*} \\ & w ＝ \sum_{i=1}^{N} \alpha_{i} y_{i} x_{i} \\ & \sum_{i=1}^{N} \alpha_{i} y_{i} = 0 \\ & C - \alpha_{i} - \mu_{i} = 0\end{align*}代入拉格朗日函数，得 \begin{align*} \\ & L \left( w, b, \xi, \alpha, \mu \right) = \dfrac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} \left( x_{i} \cdot x_{j} \right) + C \sum_{i=1}^{N} \xi_{i} - \sum_{i=1}^{N} \alpha_{i} y_{i} \left[ \left( \sum_{j=1}^{N} \alpha_{j} y_{j} x_{j} \right) \cdot x_{i} + b \right] \\ & \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad + \sum_{i=1}^{N} \alpha_{i} - \sum_{i=1}^{N} \alpha_{i} \xi_{i} - \sum_{i}^{N} \mu_{i} \xi_{i} \\ & = - \dfrac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} \left( x_{i} \cdot x_{j} \right) - \sum_{i=1}^{N} \alpha_{i} y_{i} b + \sum_{i=1}^{N} \alpha_{i} + \sum_{i=1}^{N} \xi_{i} \left( C - \alpha_{i} - \mu_{i} \right) \\ & = - \dfrac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} \left( x_{i} \cdot x_{j} \right) + \sum_{i=1}^{N} \alpha_{i} \end{align*}求极大求$\max_{\alpha} \min_{w,b, \xi}L \left( w, b, \xi, \alpha, \mu \right)$： \begin{align*} \\ & \max_{\alpha} - \dfrac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} \left( x_{i} \cdot x_{j} \right) + \sum_{i=1}^{N} \alpha_{i} \\ & s.t. \sum_{i=1}^{N} \alpha_{i} y_{i} = 0 \\ & C - \alpha_{i} - \mu_{i} = 0 \\ & \alpha_{i} \geq 0 \\ & \mu_{i} \geq 0, \quad i=1,2, \cdots, N \end{align*}等价于 \begin{align*} \\ & \min_{\alpha} \dfrac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} \left( x_{i} \cdot x_{j} \right) - \sum_{i=1}^{N} \alpha_{i} \\ & s.t. \sum_{i=1}^{N} \alpha_{i} y_{i} = 0 \\ & 0 \leq \alpha_{i} \leq C , \quad i=1,2, \cdots, N \end{align*}于是，可以通过求解此对偶问题而得到原始问题的解，进而确定分离超平面和决策函数。 线性可分支持向量机学习算法输入：训练数据集线性可分训练数据集 $T={(x_1, y_1), (x_2, y_2),…,(x_N, y_N)}$，其中 $x_i \in R^n$，$y_i \in \{-1, 1\}$，$i=1,2,…,N$。 输出：分离超平面和分类决策函数 选择惩罚参数$C \geq 0$，构早并求解凸二次规划问题 \begin{align*} \\ & \min_{\alpha} \dfrac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} \left( x_{i} \cdot x_{j} \right) - \sum_{i=1}^{N} \alpha_{i} \\ & s.t. \sum_{i=1}^{N} \alpha_{i} y_{i} = 0 \\ & 0 \leq \alpha_{i} \leq C , \quad i=1,2, \cdots, N \end{align*}求得最优解 $\alpha^{\star} = \left( \alpha_{1}^{\star}, \alpha_{1}^{\star}, \cdots, \alpha_{N}^{\star} \right)$ 计算 \begin{align*} \\ & w^{*} = \sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i} \end{align*}并选择$\alpha^{\star}$的一个分量$0 &lt; \alpha_{j}^{\star} &lt; C​$，计算 \begin{align*} \\ & b^{*} = y_{j} - \sum_{i=1}^{N} \alpha_{i}^{*} y_{i} \left( x_{i} \cdot x_{j} \right) \end{align*} 求得分离超平面 \begin{align*} \\ & w^{*} \cdot x + b^{*} = 0 \end{align*}以及分类决策函数 \begin{align*} \\& f \left( x \right) = sign \left( w^{*} \cdot x + b^{*} \right) \end{align*} 支持向量实例$x_i$的几何间隔为： \begin{align*} \\& \gamma_{i} = \dfrac{y_{i} \left( w \cdot x_{i} + b \right)}{ \| w \|} = \dfrac{| 1 - \xi_{i} |}{\| w \|} \end{align*}则实例$x_i$到间隔边界的距离为： \begin{align*} \\& \left| \gamma_{i} - \dfrac{1}{\| w \|} \right| = \left| \dfrac{| 1 - \xi_{i} |}{\| w \|} - \dfrac{1}{\| w \|} \right| \\ & = \dfrac{\xi_{i}}{\| w \|}\end{align*}对于每一个 $\xi_i \geq 0​$，有： \begin{align*} \xi_{i} \geq 0 \Leftrightarrow \left\{ \begin{aligned} \ & \xi_{i}=0,\quad x_{i}在间隔边界上; \\ & 0 < \xi_{i} < 1, \quad x_{i}在间隔边界与分离超平面之间; \\ & \xi_{i}=1, \quad x_{i}在分离超平面上; \\ & \xi_{i}>1, \quad x_{i}在分离超平面误分类一侧; \end{aligned} \right.\end{align*} 合页损失函数线性支持向量机学习还有另外一种解释，就是最小化以下目标函数： \sum_{i=1}^N \left[ 1 - y \left(w \cdot x + b \right) \right]_{+} + \lambda||w||^2线性支持向量机（软间隔）的合页损失函数为： \begin{align*} \\& L \left( y \left( w \cdot x + b \right) \right) = \left[ 1 - y \left(w \cdot x + b \right) \right]_{+} \end{align*}其中，“+”为取正函数： \begin{align*} \left[ z \right]_{+} = \left\{ \begin{aligned} \ & z,\quad z > 0 \\ & 0, \quad z \leq 0 \end{aligned} \right.\end{align*}]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git远程仓库的使用]]></title>
    <url>%2Fposts%2F757%2F</url>
    <content type="text"><![CDATA[在Github中创建远程库，然后从远程库克隆到本地。 12345git clone git@github.com:shenghaishxt/ProjectName.git # 克隆cd ProjectName/git add . # 添加文件git commit -m &apos;ProjectName&apos;git push]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[统计学习方法概论]]></title>
    <url>%2Fposts%2F13076%2F</url>
    <content type="text"><![CDATA[本系列是《统计学习方法》这本书的读书笔记，本博客会陆续更新笔记内容。 统计学习对象：数据 目的：对数据进行预测和分析 方法：监督学习、非监督学习、半监督学习、强化学习（本书主要研究监督学习） 监督学习每个具体的输入是一个实例，通常由特征向量表示。 输入实例x的特征向量： x_i = (x_i^{(1)},x_i^{(2)},..., x_i^{(n)})^T$x^{(i)}$与$x_i$不同，后者表示多个输入变量中的第$i​$个： x_i = (x_i^{(1)},x_i^{(2)}, ...,x_i^{(n)} )^T训练集通常表示为： T = \{(x_1, y_1),(x_2, y_2) ,...,(x_N, y_N)\} 回归问题：输入变量与输出变量均为连续变量的预测问题 分类问题：输出变量为有限个离散变量的预测问题 标注问题：输入变量与输出变量均为变量序列的预测问题 输入与输出的随机变量X和Y具有联合概率分布的假设是监督学习关于数据的基本假设。 模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间。 监督学习分为学习和预测两个过程，可用下图来描述： 监督学习可以概括如下：从给定有限的训练数据出发，假设数据是独立同分布的，而且假设模型属于某个假设空间，应用某一评价准则，从假设空间中选取一个最优的模型，使它对已给训练数据及未知测试数据在给定评价标准意义下有最准确的预测。 统计学习三要素统计学习方法 = 模型 + 策略 + 算法。 模型假设空间中的模型一般有无穷多个，假设空间用$\digamma$表示。 假设空间可以定义为决策函数的集合：$\digamma = \{f| Y = f(X)\}​$ ，$\digamma​$通常是由一个参数向量决定的函数族：$\digamma = \{f| Y = f_{\theta}(X), \theta \in R^n\}​$ 假设空间也可以定义为条件概率的结合：$\digamma = \{P| P(Y|X)\}$，$\digamma$通常是由一个参数向量决定的条件概率分布族：$\digamma = \{P| P_{\theta}(Y|X), \theta \in R^n\}​$ 策略损失函数和风险函数有了模型的假设空间，统计学习接着考虑的就是按什么样的准则学习。 损失函数：度量模型一次预测的好坏，如0-1损失函数、平方损失函数、绝对损失函数、对数损失函数。 风险函数：度量平均意义下模型预测的好坏。 损失函数的期望（风险函数）： R_{exp}(f) = E_p[L(Y, f(X))] = \int L(y, f(x))P(x, y)dxdy用P(X, Y)可以直接求出P(Y | X)，但我们不知道。 模型$f(X)$关于训练数据集的平均损失称为经验风险或经验损失： R_{emp}(f) = \frac1N \sum_{i=1}^N L(y_i, f(x_i))根据大数理论，当样本N趋于无穷时，$R_{emp}(f)$趋于$R_{exp}(f)$，一个很自然的想法是用$R_{emp}(f)$来估计$R_{exp}(f)$。但现实中训练样本有限，要对经验风险进行矫正，这就关系到监督学习的两个基本策略：经验风险最小化和结构风险最小化。 经验风险最小化和结构风险最小化按照经验风险最小化求最优模型就是求解最优化问题： min_{f \in \digamma} \frac1N \sum_{i=1}^N L(y_i, f(x_i)) 当样本容量很小时，经验风险最小化的效果未必很好，会产生过拟合（over-fitting），结构风险最小化由此提出。 结构风险的定义是： R_{srm}(f)= \frac1N \sum_{i=1}^N L(y_i, f(x_i)) + \lambda J(f)按照结构风险最小化求最优模型就是求解最优化问题： min_{f \in \digamma} \frac1N \sum_{i=1}^N L(y_i, f(x_i)) + \lambda J(f)这时，监督学习的问题就编程了经验风险或结构风险函数的最优化问题。 算法将统计学习问题归结为最优化问题之后，统计学习的算法成为求解最优化问题的算法。 模型评估与模型选择训练误差与测试误差训练误差是模型$Y = \hat f (X)​$关于训练集的平均损失： R_{emp}(\hat f) = \frac1N \sum_{i=1}^N L(y_i, \hat f(x_i))测试误差是模型$Y = \hat f (X)$关于测试集的平均损失： e_{test} = \frac1 {N'} \sum_{i=1}^{N'} L(y_i, \hat f(x_i))测试误差反应了学习方法对未知的测试数据集的预测能力。 过拟合与模型选择下图是训练误差和测试误差与模型复杂度的关系。当模型的复杂度增大时，训练误差会逐渐减小；而测试误差会先减小，达到最小值后又增大。 正则化与交叉验证正则化正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。 正则化一般具有如下形式： min_{f \in \digamma} \frac1N \sum_{i=1}^N L(y_i, f(x_i)) + \lambda J(f)其中，第1项是经验风险，第2项是正则化项，$\lambda \geq 0$为调整两者之间关系的系数。 回归问题中： L(w) = \frac1N \sum_{i=1}^N(f(x_i;w)-y_i)^2 + \frac {\lambda}2 ||w||^2 L(w) = \frac1N \sum_{i=1}^N(f(x_i;w)-y_i)^2 + {\lambda} ||w||_1交叉验证如果给定的样本数据充足，进行模型选择的一种简单方法是随机地将数据集分为三部分，分别为训练集、验证集和测试集。训练集用来训练模型，验证集用于模型的选择，而测试集用于最终对学习方法的评估。 但在数据不充足的情况下，为选择好的模型，可以使用交叉验证。交叉验证的基本想法是重复地使用数据，把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复地进行训练、测试以及模型选择。 简单交叉验证将数据分为训练集和测试集，然后用训练集在各种条件下训练模型从而得到不同模型，在测试集桑拿评价各个模型的测试误差，选出测试误差最小的模型。 S折交叉验证应用最多的是S折交叉验证。首先随机地将已给数据切分为S个互不相交的大小相同的子集，然后利用S-1个子集的数据训练模型，利用余下的子集测试模型，将这一过程对可能的S种选择重复进行，最后选出S次评测中平均测试误差最小的模型。 留一交叉验证S折交叉验证的特殊情形是S = N，称为留一交叉验证，往往在数据缺乏的情况下使用。这里的N是给定数据集的容量。 泛化能力泛化误差 R_{exp}(\hat f) = E_p[L(Y, \hat f(X))] = \int L(y, \hat f(x))P(x, y)dxdy可以通过比较泛化误差上界来比较学习方法的泛化能力。 泛化误差上界的性质：样本容量增加，泛化误差趋于0；假设空间容量越大，泛化误差越大。 定理（泛化误差上界）：对二分类问题，当假设空间是有限个函数的集合时$\digamma = \{f_1, f_2, …, f_d\}$，对任意一个函数$f \in \digamma$，至少以概率$1-\delta$，以下不等式成立： R(f) \leq \hat R(f) + \varepsilon (d, N, \delta) \varepsilon(d, N, \delta) = \sqrt {\frac 1{2N}(\log d+\log\frac1{\delta})}生成模型与判别模型监督学习方法可以分为生成方法和判别方法，所学到的模型分别称为生成模型和判别模型。 生成方法由数据学习联合概率分布P(X, Y)，然后求出条件概率分布P(Y | X)作为预测的模型，即生成模型： P(Y|X) = \frac {P(X, Y)}{P(X)}典型的生成模型由：朴素贝叶斯法和隐马尔可夫模型。 判别方法由数据直接学习决策函数f(X)或者条件概率分布P(Y | X)作为预测的模型，即判别模型。典型的判别模型包括：k近邻法、感知机、决策树、Logistic回归模型、最大熵模型、支持向量机、提升方法和条件随机场等。 各自的优缺点： 生成方法：可还原出联合概率分布P(X,Y),而判别方法不能。生成方法的收敛速度更快，当样本容量增加的时候，学到的模型可以更快地收敛于真实模型；当存在隐变量时，仍可以使用生成方法，而判别方法则不能用。 判别方法：直接学习到条件概率或决策函数，直接进行预测，往往学习的准确率更高；由于直接学习Y=f(X)或P(Y|X),可对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习过程。 分类问题二分类问题常用的评价指标是精确率(precision)和召回率(recall)。 精确率定义为 P = \frac {TP}{TP+FP}召回率定义为 R = \frac {TP}{TP+FN}其中：TP (true positive) 将正类预测为正类FN (false negative) 将正类预测为负类FP (false positive) 将负类预测为正类TN (true negative) 将负类预测为负类 $F_1$是精确率和召回率的调和均值，即 \frac 2{F_1} = \frac 1P+ \frac1R F_1 = \frac {2TP}{2TP+FP+FN}标注问题评价标注模型的指标与评价分类模型的指标一样，可以认为标记问题是分类问题的一个推广。 标注问题的输入是一个观测序列，输出的是一个标记序列或状态序列。也就是说，分类问题的输出是一个值，而标注问题输出是一个向量，向量的每个值属于一种标记类型。 标注常用的统计学习方法有：隐马尔可夫模型、条件随机场。 回归问题回归模型是表示输入变量到输出变量之间映射的函数，回归问题的学习等价于函数拟合。 最常用的损失函数是平方损失函数，在此情况下，回归问题可以由著名的最小二乘法求解。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决package'' is not available(for R version 3.4.1)的问题]]></title>
    <url>%2Fposts%2F8933%2F</url>
    <content type="text"><![CDATA[在R中使用 install.packages(“ “)的时候有时会提示 package’’ is not available(for R version 3.4.1)，此时可使用以下的代码安装： 12source(&quot;http://bioconductor.org/biocLite.R&quot;)biocLite(&quot; &quot;)]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>R语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性可分支持向量机与硬间隔最大化]]></title>
    <url>%2Fposts%2F44274%2F</url>
    <content type="text"><![CDATA[支持向量机的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。 线性可分支持向量机 考虑一个二分类问题， 假设输入空间和特征空间是两个不同的空间。 输入空间：欧式空间或离散集合 特征空间：欧式空间或希尔伯特空间 线性可分支持向量机、线性支持向量机：假设这两个空间中的元素一一对应，并将输入空间 中的输入映射为特征空间中的特征向量 非线性支持向量机：利用一个从输入空间到特征空间的非线性映射将输入映射为特征向量 输入都由输入空间转换到特征空间，支持向量机的学习是在特征空间进行的。 假设给定一个特征空间上的训练数据集： T = {(x_1, y_1), (x_2, y_2), ...,(x_N, y_N)}样本分为正例和负例。 学习的目标：找到分离超平面 $w^{\star} \cdot x + b^{\star} = 0 $。 和感知机不同（利用误分类最小的策略求分离超平面，解有无穷个），线性可分支持向量机利用间隔最大化求最优分离超平面，解是唯一的。 线性可分支持向量机的决策函数为$f(x) = sign(w^{\star} \cdot x + b^{\star} )$。 其实，在二维特征空间的分类问题中，有许多直线能将两类数据正确划分，线性可分支持向量机对应着将两类数据正确划分并且间隔最大的直线。 函数间隔和几何间隔函数间隔定义超平面($w, b$)关于样本点($x_i, y_i$)的函数间隔为 $\hat\gamma_i = y_i(w\cdot x_i +b)$。 定义超平面($w, b$)关于训练集$T$的函数间隔为 $\hat\gamma = min_{i=1,…N} \quad \hat \gamma_i$。 函数间隔表示分类预测的正确性和确信度。 几何间隔如下图所示，点A与超平面($w, b$)的距离由线段AB给出，记作$\gamma_i$： \gamma_i =y_i (\frac w {||w||} \cdot x_i +\frac b {||w||}) 定义超平面($w, b$)关于训练集$T$的几何间隔为 $\gamma = min_{i=1,…N} \quad \gamma_i$。 函数间隔和几何间隔的关系 样本点的几何间隔 \gamma_i = \frac{\hat\gamma_i}{||w||} 训练集的几何间隔 \gamma = \frac{\hat\gamma}{||w||}如果超平面参数w和b成比例地改变（超平面没有变），函数间隔也按此比例改变，而几何间隔不变。 间隔最大化最大间隔分离超平面具体地，求一个几何间隔最大的分离超平面，即最大间隔分离超平面可以表示为下面的约束最优化问题： max_{w, b} \quad \gamma \\\\ s.t. \qquad y_i (\frac w {||w||} \cdot x_i +\frac b {||w||})\geq \gamma \quad i = 1,2,...,N根据几何间隔和函数间隔的关系： max_{w, b}\quad \frac{\hat\gamma}{||w||} s.t. \qquad y_i ( w \cdot x_i + b)\geq \hat\gamma \quad i = 1,2,...,N函数间隔 $\hat\gamma$的取值不影响最优化问题的解，可以取 $\hat \gamma = 1$。 最大化 $\frac1{||w||}$和最小化 $||w||$是等价的，为了求导方便，我们将其等价为最小化 $\frac12 ||w^2||$。 则线性可分支持向量机的最优化问题如下，这是一个求解最优解$w^{\star}, b^{\star}$的凸二次规划问题： max_{w, b}\quad \frac12 ||w^2|| s.t. \qquad y_i ( w \cdot x_i + b)-1\geq 0 \quad i = 1,2,...,N 补充：凸优化问题是指约束最优化问题： min_w \quad f(w)\\ s.t. \quad g_i(w) \leq 0, \quad i = 1,2,...,k\\ \ h_i(w) = 0, i=1,2,...,l其中，目标函数$f(x)$和约束函数$g_i(w)$都是$R^n$上连续可微的凸函数，约束函数$h_i(w)$是$R^n$上的仿射函数。 当目标函数为二次函数且约束函数是仿射函数时，凸最优化问题成为凸二次规划问题。 支持向量和间隔边界在线性可分情况下，训练集的样本点中与分离超平面距离最近的样本点的实例称为支持向量。 支持向量使得约束条件式等号成立，即$y_i(wx_x+b)-1=0$。 正例：$H_1: \quad w\cdot x+b=-1$ 负例：$H_2: \quad w\cdot x+b = -1$ 如下图所示： $H_1$与$H_2$平行，它们之间形成一条长带，分离超平面位于它们中央且与之平行。长带的宽度称为间隔，等于$\frac 2 {||w||}$，$H_1$和$H_2$称为间隔边界。 在决定分离超平面时只有支持向量起作用，而其他实例点并不起作用。 线性可分支持向量机学习算法——最大间隔法输入：线性可分训练数据集线性可分训练数据集 $T={(x_1, y_1), (x_2, y_2),…,(x_N, y_N)}$，其中 $x_i \in R^n$，$y_i \in \{-1, 1\}$，$i=1,2,…,N$。 输出：最大间隔分离超平面和分类决策函数 构造并求解约束最优化问题： max_{w, b}\quad \frac12 ||w^2|| s.t. \qquad y_i ( w \cdot x_i + b)-1\geq 0 \quad i = 1,2,...,N得到最优解$w^{\star} , b^{\star}$。 得到分离超平面及分类决策函数： w^{\star} \cdot x + b^{\star} = 0 f(x) = sign(w^{\star} \cdot x +b^{\star})给出《统计学习方法》中的一个例子： 学习的对偶算法关于拉格朗日对偶的介绍可以参考我的上一篇笔记。应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解。 这样做有两个优点： 对偶问题往往更容易求解 自然引入核函数，进而推广到非线性分类问题 首先构建拉格朗日函数，为每一个不等式约束引进拉格朗日乘子$\alpha_i \geq 0, i=1,2,…,N$： \begin{align*} \\ & L \left( w, b, \alpha \right) = \dfrac{1}{2} \| w \|^{2} + \sum_{i=1}^{N} \alpha_{i} \left[- y_{i} \left( w \cdot x_{i} + b \right) + 1 \right] \\ & = \dfrac{1}{2} \| w \|^{2} - \sum_{i=1}^{N} \alpha_{i} y_{i} \left( w \cdot x_{i} + b \right) + \sum_{i=1}^{N} \alpha_{i} \end{align*}根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题： max_{\alpha}min_{w,b}L(w, b, \alpha)为了得到对偶问题的解，需要先求$L(w,b,\alpha)$对$w,b$的极小，再求对$\alpha$的极大。 求极小将拉格朗日函数$L(w,b,\alpha)$分别对$w, b$求偏导数并令其等于0： \begin{align*} \\ & \nabla _{w} L \left( w, b, \alpha \right) = w - \sum_{i=1}^{N} \alpha_{i} y_{i} x_{i} = 0 \\ & \nabla _{b} L \left( w, b, \alpha \right) = -\sum_{i=1}^{N} \alpha_{i} y_{i} = 0 \end{align*}得 \begin{align*} \\ & w ＝ \sum_{i=1}^{N} \alpha_{i} y_{i} x_{i} \\ & \sum_{i=1}^{N} \alpha_{i} y_{i} = 0 \end{align*}代入拉格朗日函数，得 \begin{align*} \\ & L \left( w, b, \alpha \right) = \dfrac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} \left( x_{i} \cdot x_{j} \right) - \sum_{i=1}^{N} \alpha_{i} y_{i} \left[ \left( \sum_{j=1}^{N} \alpha_{j} y_{j} x_{j} \right) \cdot x_{i} + b \right] + \sum_{i=1}^{N} \alpha_{i} \\ & = - \dfrac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} \left( x_{i} \cdot x_{j} \right) - \sum_{i=1}^{N} \alpha_{i} y_{i} b + \sum_{i=1}^{N} \alpha_{i} \\ & = - \dfrac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} \left( x_{i} \cdot x_{j} \right) + \sum_{i=1}^{N} \alpha_{i} \end{align*}即 \begin{align*} \\ & \min_{w,b}L \left( w, b, \alpha \right) = - \dfrac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} \left( x_{i} \cdot x_{j} \right) + \sum_{i=1}^{N} \alpha_{i} \end{align*}求极大求$max_{\alpha}min_{w,b}L(w, b, \alpha)​$，即是对偶问题： \begin{align*} \\ & \max_{\alpha} - \dfrac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} \left( x_{i} \cdot x_{j} \right) + \sum_{i=1}^{N} \alpha_{i} \\ & s.t. \sum_{i=1}^{N} \alpha_{i} y_{i} = 0 \\ & \alpha_{i} \geq 0, \quad i=1,2, \cdots, N \end{align*}将上式由求极大转换成求极小，得到下面与之等价的对偶最优化问题： \begin{align*} \\ & \min_{\alpha} \dfrac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} \left( x_{i} \cdot x_{j} \right) - \sum_{i=1}^{N} \alpha_{i} \\ & s.t. \sum_{i=1}^{N} \alpha_{i} y_{i} = 0 \\ & \alpha_{i} \geq 0, \quad i=1,2, \cdots, N \end{align*}对线性可分训练数据集，假设上面对偶最优化问题对$\alpha​$的解为$\alpha^{\star} = \left( \alpha_{1}^{\star}, \alpha_{1}^{\star}, \cdots, \alpha_{N}^{\star} \right)^T​$，可以由$\alpha^{\star} ​$求得原始最优化问题对(w, b)的解$w^{\star}, b^{\star}​$。有下面的定理： \begin{align*} \\ & w^{*} = \sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i} \end{align*} \begin{align*} \\ & b^{*} = y_{j} - \sum_{i=1}^{N} \alpha_{i}^{*} y_{i} \left( x_{i} \cdot x_{j} \right) \end{align*}线性可分支持向量机学习算法输入：线性可分训练数据集线性可分训练数据集 $T={(x_1, y_1), (x_2, y_2),…,(x_N, y_N)}​$，其中 $x_i \in R^n​$，$y_i \in \{-1, 1\}​$，$i=1,2,…,N​$。 输出：分离超平面和分类决策函数 构造并求解约束最优化问题： \begin{align*} \\ & \min_{\alpha} \dfrac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} \left( x_{i} \cdot x_{j} \right) - \sum_{i=1}^{N} \alpha_{i} \\ & s.t. \sum_{i=1}^{N} \alpha_{i} y_{i} = 0 \\ & \alpha_{i} \geq 0, \quad i=1,2, \cdots, N \end{align*}​ 得到最优解$\alpha^{\star} = \left( \alpha_{1}^{\star}, \alpha_{1}^{\star}, \cdots, \alpha_{N}^{\star} \right)​$。 计算 \begin{align*} \\ & w^{*} = \sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i} \end{align*}并选择$\alpha^{\star} $的一个正分量$\alpha_{j}^{\star} &gt; 0$，计算 \begin{align*} \\ & b^{*} = y_{j} - \sum_{i=1}^{N} \alpha_{i}^{*} y_{i} \left( x_{i} \cdot x_{j} \right) \end{align*} 求得分类超平面： \begin{align*} \\ & w^{*} \cdot x + b^{*} = 0 \end{align*}分类决策函数： \begin{align*} \\& f \left( x \right) = sign \left( w^{*} \cdot x + b^{*} \right) \end{align*} 同样给出一个对偶形式算法的例子，训练数据和上一个例子相同：]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo添加Aplayer播放器]]></title>
    <url>%2Fposts%2F30334%2F</url>
    <content type="text"><![CDATA[感谢开源！！让我们用上了这么好的插件！ 创建歌单页面由于我想在单独的页面加入歌单，所以额外创了个页面，也可以直接在文章中插入，原理都是一样的。 新建页面，命名为playlist： 1hexo new page playlist 这时候在 /Hexo/source 文件夹下会生成一个playlist文件夹，打开里面的index.md，修改如下： 123title: 歌单date: 2019-02-21 16:14:00type: &quot;playlist&quot; 打开主题的 _config.yml文件，在menu下新建一个名为playlist的类（注意这里使用的图标是图标库中的图标，网址为 http://www.fontawesome.com.cn/faicons/ 。可以选择自己喜欢的图标，我这里选择的是music）。完成后如下所示： 1234567menu: home: / || home categories: /categories/ || th tags: /tags/ || tags archives: /archives/ || archive playlist: /playlist/ || music about: /about/ || user 打开/Hexo/themes/hexo-theme-next/languages/zh-Hans.yml，添加对应的中文翻译： 12menu: playlist: 歌单 这样歌单就创建完成啦~ 使用 hexo-tag-aplayer 插件 hexo-tag-aplayer 是Aplayer在hexo上的插件，这里的配置参考的是官方文档 ，第一步安装 hexo-tag-aplayer： 1npm install --save hexo-tag-aplayer 最新版的 hexo-tag-aplayer 已经支持了MetingJS的使用，可以直接解析网络平台的歌曲（简直是神器），首先要在站点配置文件中开启meting模式，添加以下代码在配置文件的最后： 12aplayer: meting: true 复制歌单的链接，然后复制歌单的id，例如 https://music.163.com/playlist?id=523845661&amp;userid=46562117 ，这个歌单的id就是523845661，公司名可以是tencent、netease或是其他公司，下面给出一个例子，打开 /Hexo/source/playlist/index.md文件，输入： 1&#123;% meting &quot;523845661&quot; &quot;netease&quot; &quot;playlist&quot; &quot;theme:#FF4081&quot; &quot;mode:circulation&quot; &quot;mutex:true&quot; &quot;listmaxheight:340px&quot; &quot;preload:auto&quot; %&#125; 效果还是很不错的：]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建Jupyter Notebook远程云服务器]]></title>
    <url>%2Fposts%2F37756%2F</url>
    <content type="text"><![CDATA[将Jupyter部署到课题组的深度学习服务器上，主要的考虑是可以避免在不同电脑上使用jupyter时数据不一致的问题，而且能够完美利用服务器的性能。 完成后的界面如下： 还是可以的，等之后有时间再把本地电脑上的那些插件都装上就完美啦~ 安装过程1、安装Jupyter Notebook1pip install jupyter 2、生成配置文件1jupyter notebook --generate-config 3、设置密码打开Python终端，输入 1234In [1]: from IPython.lib import passwdIn [2]: passwd()Enter password: Verify password: 复制得到的密码。 4、设置配置文件1vim /home/zsh/.jupyter/jupyter_notebook_config.py 在末尾增加配置信息（直接修改配置文件也可以，但这样看着更简洁，且便于之后的修改） 123456c.NotebookApp.ip = &apos;0.0.0.0&apos; # 所有IP都可以访问c.NotebookApp.port = 8888 # 默认的端口是8888c.NotebookApp.open_browser = False # 禁止在服务器上打开jupyterc.NotebookApp.notebook_dir = &apos;/home/zsh/jupyter&apos; # 设置Jupyter的根目录c.NotebookApp.allow_root = True # 以root权限启动jupyter c.NotebookApp.password = &quot;粘贴在第三步中得到的密码&quot; 5、启动Jupyter Notebook1jupyter notebook 这样就搭建成功啦，可以在本地的浏览器上输入 服务器的ip地址:8888，这样就可以远程打开jupyter了。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令使用纪录]]></title>
    <url>%2Fposts%2F37930%2F</url>
    <content type="text"><![CDATA[ssh登陆的命令行方法： 1ssh amax@10.10.1.110 显示所有文件： 1ls -a 复制文件夹下的所有内容： 1cp -r dir1 dir2 删除文件夹： 1rm -rf dir 临时更改pip源 1sudo python -m pip install pyradiomics -i https://pypi.tuna.tsinghua.edu.cn/simple 安装g++： 1aptitude install build-essential 查找： 1find -name &apos;google*&apos; 解压tar.xz压缩包： 1tar -xvf nameofpackage.tar.xz 解压tar.gz压缩包： 1tar -zxvf nameofpackage.tar.gz 杀进程： 1killall -9 R 查看IP： 1hostname -I 查看版本： 123cat /usr/local/cuda/version.txt # 查看CUDA版本cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 # 查看CUDNN版本cat /etc/issue # 查看Ubuntu版本 切换用户： 12sudo su root # 切换root用户su user # 切换普通用户，也可以使用 sudo -i 压缩文件夹： 1zip -r filename.zip filename 打开Jupyter Notebook： 1jupyter notebook --config /home/zsh/.jupyter/config_zsh.py 查看文件或文件夹大小： 1du -sh 文件名或文件夹名]]></content>
      <categories>
        <category>Linux命令使用纪录</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[调整hexo页面宽度]]></title>
    <url>%2Fposts%2F13179%2F</url>
    <content type="text"><![CDATA[博客在浏览器上的留白太多，因此想增加文章的宽度。对于Pisces Scheme，修改页面宽度的方式与其他三个主题不太一样，因此列出Pisces Scheme的修改方式。 打开/Hexo/themes/hexo-theme-next/source//css/_variables/custom.styl 添加两行代码即可： 12$main-desktop = 1200px $content-desktop = 900px]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用DNSPOD进行域名解析]]></title>
    <url>%2Fposts%2F27735%2F</url>
    <content type="text"><![CDATA[目前的域名解析有两种方法，一种是直接利用域名注册商的解析，另一种就是更换DNS用第三方的解析。为了便于管理，也为了更好的速度，将域名的解析方式由namesilo默认的解析方式改为使用国内的DNSPOD进行解析。 首先在namesilo上更换域名DNS（即Change Name Servers），将原来默认的Name Servers改为 12f1g1ns1.dnspod.netf1g1ns1.dnspod.net 删除默认的A记录和CNAME记录，等待DNSPOD的免费DNS更新完毕后就可以放心地关闭namesilo了。 然后打开DNSPOD控制台，添加域名即可。由于本博客是用Github托管的，因此这里记录了连接到Github博客的设置，也可以连接到自己的VPS，IP改为自己的VPS地址即可。 123@ A 192.30.252.153@ A 192.30.252.154www CNAME shenghaishxt.github.io A： 用来指定域名为 IPv4 的地址（如：8.8.8.8），如果需要将域名指向一个IP地址，就需要添加 A 记录。CNAME： 如果需要将域名指向另一个域名，再由另一个域名提供 ip 地址，就需要添加 CNAME 记录。 最后在本地的/Hexo/source文件夹下新建CNAME文件，注意不要带任何后缀，如果想要自己的网站是带有www的二级域名，那么就输入带有www的域名，否则的话直接添加自己购买的域名就好。我这里添加的是带有www的二级域名。 1www.zhangshenghai.com 接下来就等待DNS的解析啦，不知道为什么我的解析好像挺慢的。十几分钟后zhangshenghai.com这个域名是可以ping通，但加了www的域名就总是ping不通。貌似是过了一个晚上，我第二天打开才可以登陆上。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>DNS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CHAPTER10 线性规划在近似算法中的应用]]></title>
    <url>%2Fposts%2F20007%2F</url>
    <content type="text"><![CDATA[使用线性规划来近似顶点覆盖问题ILP（整数线性规划）对于每一个顶点$v\in V$，有$x(v) \in \{0, 1\}$，$x(v)=1$意为v在顶点覆盖中，$x(v)=0$意为v不再顶点覆盖中。那么，对于顶点覆盖问题的任意边$(u, v)$，u和v至少有一个必须在顶点覆盖中，即$x(u)+x(v)\geq1$。这样就引出了以下用于寻找最小顶点覆盖的0-1整数规划 。 min \sum_{v\in V} x(v)\\ s.t.\qquad x(u)+x(v) \geq 1 \quad \forall (u,v)\in E \\ \quad x(v)\in \{0, 1\} \quad \forall v \in Vrelax to LP（线性规划松弛）假设去掉了$x(v) \in \{0, 1\}​$这一限制，并代之以$0\leq x(v) \leq 1​$，就可以得到以下的线性规划，称为线性规划松弛。 min \sum_{v\in V} x(v)\\ s.t.\qquad x(u)+x(v) \geq 1 \quad \forall (u,v)\in E \\ \quad 0\leq x(v) \leq 1 \quad \forall v \in VRounding（使用Rounding的方法来构造近似算法）对于每一个顶点v，都会求得一个$x(v)$的值$x^*(v)$，对$x(v)$做以下的rounding： 若$x^*(v)\geq 1/2$，则将该顶点加入到点覆盖集合中（$x(v)= 1$），否则舍去顶点v（$x(v) = 0​$），直至图中的所有顶点处理完毕。 由以上算法可看出： 故此算法是一个近似度为2的近似算法。]]></content>
      <categories>
        <category>算法设计与分析</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CHAPTER9 线性规划]]></title>
    <url>%2Fposts%2F55620%2F</url>
    <content type="text"><![CDATA[线性规划概述在给定有限的资源和竞争约束情况下，很多问题都可以表达为最大化或最小化某个目标。如果可以把目标指定为某些变量的一个线性函数，而且如果可以将资源的约束指定为这些变量的等式或不等式，则得到一个线性规划问题（Linear-Programming Problem）。 在求解线性规划时又两种有用的格式：标准型和松弛型。在标准型中所有的约束都是不等式，而在松弛型中所有的约束都是等式。 下面给出一个将实际问题转换为线性规划形式的例子。 有m种不同的食物$F_1, …, F_m$，这些食物能够提供n种营养$N_1, …, N_n$，营养$N_j$每天的最低需求量是$c_j$，$b_i$是$F_i$的单位价格。$a_{ij}$代表食物$F_i$单位体积所含的营养$N_j$。问题是求在满足营养需求下的最小花费。 假设每种食物的数量为$x_i$，则使用线性规划的形式可表示为： min \sum_i b_i x_i \\ s.t. \sum_i a_{ij}x_i \geq c_j这个问题的目标是求满足营养需求的条件下最小化价格，接下来我们会看到，其实它的对偶问题就是最大化营养的需求量。 单纯形算法解决线性规划问题主要用三种算法： 单纯形算法：指数时间的复杂度，但是在实际中应用广泛，当它被精心实现时，通常能够快速地解决一般的线性规划问题。 椭圆算法：第一个指数时间算法，但是在实际中运行缓慢。 内点法：在理论和实际中都能比较有效率地解决线性规划问题。 本章我们主要讨论在实际问题中应用广泛的单纯形算法。 首先从一个例子开始，考虑下列松弛型的线性规划并将等式重写后可得到一个tableau： 现在，$x_3, x_4, x_5$是基本解，令$x_1=x_2=0$，可得$x_3=1, x_4=3, x_5=2$，且$z=0$。 我们当然希望改善$z$的值，很明显需要增加$x_1$或者$x_2$。令$x_1=0$，由于$x_1, …, x_5 \geq 0$，$x_2$最大可取至1，此时$x_3=0$。现在基本解变为$x_2, x_4, x_5$，重写tableau： 重复上面的过程，为了增加$z$的值，我们可以增加$x_1$（由于$x_3$的系数是负数，因此增加$x_3$是无效的）。令$x_3=0$，$x_1$最大可取至1，此时$x_5=0$。这时基本解变为$x_1, x_2, x_4$，重写tableau： 重复上面的过程，为了增加$z$的值，我们可以增加$x_3$（由于$x_5$的系数是负数，因此增加$x_5$是无效的）。令$x_5=0$，$x_3$最大可取至2，此时$x_4$变为0。这时基本解变为$x_1, x_2, x_3$，重写tableau： 此时可看出$z$的取值已达最优，因此解是$x_1=3, x_2=2, x_3=2, x_4=0, x_5=0$，目标值$z=5$。 对偶性对偶性是个非常重要的性质。在一个最优化问题中，一个对偶问题的识别几乎总是伴随着一个多项式时间算法的发现。 在线性规划的形式下，对偶问题可互相转化，具体如下图所示： 下面给出一个实际例子，照着原问题的线性规划形式，我们即可写出对偶问题的线性规划形式。 原问题有多少个未知数，对偶问题就有多少个式子；原问题有多少个式子，对偶问题就有多少个未知数。 如下图所示，原问题给出了对偶问题的可行解的下界，对偶问题给出了原问题的可行解的上界。 总结几个经典的对偶问题： 最大流的对偶问题是最小割 最大匹配的对偶问题是最小顶点覆盖 最优匹配的对偶问题是最小定标和 最大流与最小割的线性规划表示最大流满足两个性质：反对称性、容量限制。最大流是满足这两个约束和最大化流量值的流，其中流量值是从源流出的总流量。因此，流满足线性约束，且流的值是一个线性函数。可以将最大流问题表示为线性规划并作如下的转换： 其对偶问题的实际意义是最小割：]]></content>
      <categories>
        <category>算法设计与分析</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CHAPTER8 近似算法]]></title>
    <url>%2Fposts%2F63466%2F</url>
    <content type="text"><![CDATA[目前，所有的NP完全问题都没有能够在多项式时间内求解的算法，我们通常可以采用以下几种解题策略： 只对问题的特殊实例求解 用动态规划法或分支限界法求解 用概率算法求解 只求近似解 用启发式方法求解 本节主要讨论的是解NP完全问题的近似算法。 近似算法的性能若一个最优化问题的最优值为$C^{OPT}$，求解该问题的一个近似算法的一个近似最优解相应的目标函数值为$C$，则将近似算法的性能比定义为： \eta = max({\frac{C}{C^{OPT}}, \frac{C^{OPT}}{C}})通常情况下，该性能比是问题输入规模$n$的一个函数$\rho(n)$，即 max({\frac{C}{C^{OPT}}, \frac{C^{OPT}}{C}}) \leq \rho(n)顶点覆盖问题的近似算法问题描述无向图$G=(V,E)$的顶点覆盖是它的顶点集$V$的一个子集$V’⊆V$，使得若$(u,v)$是$G$的一条边，则$v∈V’$或$u∈V’$。顶点覆盖V’的大小是它所包含的顶点个数$|V’|$。 下面给出一个近似比为2的算法的伪代码： 123456789101112VertexSet approxVextexCover(Graph g)&#123; cset = NULL; e = g.e; while (e != NULL) &#123; 从e中任取一条边(u, v); 将顶点u,v加入cset; 从e中删去与u和v相关联的边; &#125; return cset;&#125; 算法运行过程下图是《算法导论》中顶点覆盖问题近似算法的图例，说明了算法的运行过程和结果。 图(e)表示近似算法产生的近似最优顶点覆盖cset，它由顶点b,c,d,e,f,g所组成。图(f)是图G的一个最小顶点覆盖，它只含有3个顶点：b,d和e。 性能分析假定算法选取的边集为A，则返回的顶点个数为2A。即$|C| = 2|A|$。图G的任一顶点覆盖都至少包含A中各条边中的一个顶点，即$|C^{OPT}| \geq |A|$。 则 \rho = \frac{|C|}{|C^{OPT}|} \leq 2旅行商问题的近似算法问题描述给定一个完全无向图$G=(V,E)$，其每一边$(u,v)∈E$有一非负整数费用$c(u,v)$。要找出$G$的最小费用哈密顿回路。 费用函数c往往具有三角不等式性质，即对任意的3个顶点$u,v,w∈V$，有：$c(u,w)≤c(u,v)+c(v,w)$。 在费用函数不一定满足三角不等式的一般情况下，不存在具有常数性能比的解TSP问题的多项式时间近似算法，除非$P=NP$。换句话说，若$P≠NP$，则对任意常数$ρ&gt;1$，不存在性能比为ρ的解决旅行售货员问题的多项式时间近似算法。 下面给出一个解决满足三角不等式的旅行商问题的近似算法伪代码： 123456APPROX-TSP-TOUR(G, c) 任意选择V中的一个顶点r，作为树根节点 调用Prim算法得到图G的最小生成树T 先序遍历T，得到顶点序列L 删除L中的重复顶点形成哈密顿环C 输出C 算法运行过程下图是APPROX-TSP-TOUR的操作过程，(a)示出了给定点的集合，(b)示出了一个最小生成树T，它是由MST-PRIM计算出来的，根为a节点，(c)是对T进行先序遍历时的顶点序列，(d)是近似算法得到的路线。 性能分析假设$H ^ {opt}​$是一个最优游程，如图e所示。由于我们通过删除一个游程路线中的任一边而得到一棵生成树，故最小生成树$T​$的权值是最优游程代价的一个下界，即$c(T) \leq c(H^{opt})​$。 假设图c中的遍历的代价为$c(W)$，该遍历经过了$T$的每条边两次，则有$c(W) = 2c(T)$，两式联立有$c(W) \leq 2c(H^{opt})$。由于$H$是从完全遍历$W$中删除了某些顶点得到的，故有$c(H) \leq c(W)$，则$c(H) \leq2c(H^{opt})$。 则 \frac{C(H)}{C({H^{opt}})} \leq 2]]></content>
      <categories>
        <category>算法设计与分析</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CHAPTER7 NP完全性]]></title>
    <url>%2Fposts%2F64398%2F</url>
    <content type="text"><![CDATA[P问题、NP问题、NPC问题的概念老师上课时强调过本章对于概念要特别清楚，因此首先给出这三个问题的概念： P问题：能够在多项式时间内解决的问题。 NP问题：能够在多项式时间内验证一个解的正确性的问题。 NPC问题：当一个问题满足下面两个条件的时候，那么这个问题是NPC问题。 首先，它是NP问题。 其次，所有其他的NP问题都能够在多项式时间内归约到此问题上（NP-hard）。 NPC问题是NP问题的子集。 NP-hard问题：问题A不一定是一个NP问题，但是所有的NPC问题都可以在多项式时间内转化为A，则称A为NP-hard问题。 NPC问题一定是NP-hard问题。 一些典型的NP完全问题通过问题变换的技巧，可以将2个不同问题的计算复杂性联系在一起。这样就可以将一个问题的计算复杂性归约为另一个问题的计算复杂性，从而实现问题的计算复杂性归约。 证明一个问题是NP完全问题分为两个步骤： 证明该问题是NP问题。 证明NP问题中的每一个问题都能在多项式时间内归约为该问题。 由于多项式问题具有传递性，因此只需证明一个已知的NP完全问题能够在多项式时间内归约到该问题即可。 下图给出了进行NP完全证明的结构，树的根为CIRCUIT-SAT。 电路可满足性问题（CIRCUIT-SAT）由《算法导论》第二版引理34.5：电路可满足性问题属于NP类，以及引理34.6：电路可满足性问题是NP难度的，结合NP完全性的定义可直接推出结论： 电路可满足性问题是NP完全的。 合取范式的可满足性问题（SAT）证明： SAT ∈ NP: 给定该问题的一个实例，用证书y={1, 1, 1}作为输入，对于给定的布尔公式x，我们都能按照布尔公式的数学运算规则在多项式时间内求解出来。 在多项式时间内将Circuit-SAT归约到SAT： 将电路中的输入用布尔变元表示，将电路中的每一个逻辑门用布尔公式来对应，就可以将Circuit-SAT问题归约到SAT问题。 显然，这是在多项式时间内完成的，因此SAT是NP完全问题。 三元合取范式的可满足性问题（3-CNF-SAT） 在这个问题的证明之前先补充一下合取范式（CNF）的定义。 如果一个布尔公式可表示为所有字句的“与”，且每个字句都是一个或多个文字的“或”，则称该布尔公式为合取范式（CNF）。 如果公式中的每个字句恰好都有三个不同的文字，则称该布尔公式为3-CNF。 例如，布尔公式 (x_1 \vee -x_1 \vee -x_2)\wedge(x_3 \vee x_2 \vee x_4)\wedge(-x_1 \vee -x3 \vee -x_4)就是一个3-CNF，其三个字句中的第一个为$(x_1 \vee -x_1 \vee -x_2)$，它包含3个文字$x_1$，$- x_1$，$-x_2$。 证明： 3-CNF-SAT ∈ NP： 这里的证明与上面的SAT问题是类似的。同样给定一个证书y作为输入，对于一个给定的合取范式，都可以在多项式时间内验证合取范式的值是1还是0。 在多项式时间内将SAT归约为3-CNF-SAT： 由于这里的证明对离散数学的要求较高，且老师上课时也没有细讲这部分内容，故只给出步骤，详细的证明可参考《算法导论》第二版定理34.10。 Ⅰ. 为输入公式画出一棵二叉“语法分析”树，文字作为树叶，连接词作为内部顶点。 Ⅱ. 把每个字句变换为合取范式。 Ⅲ. 继续对公式进行变换，使每个字句恰好有三个不同的文字。 从上面的步骤可以看出，SAT可以在多项式时间内归约到3-CNF-SAT，因此3-CNF-SAT是NP完全问题。 团问题（CLIQUE）证明： CLIQUE∈NP： 对于给定的图G(V, E)，如果给定顶点集V’作为证书，我们可以验证对于任意一对顶点u, v∈V’, 通过检查边(u, v)是否属于E，从而验证V’是否是一个团。显然，这是在多项式时间内完成的。 在多项式时间内将3-CNF-SAT归约到CLIQUE： 给定一个含有k个字句的3-CNF，假定其是可满足的，即3-CNF的结果为1。我们总是可以构造一个3k个顶点的图，构造方法是在不同的三元组、且不是自己的非的节点之间连线。 一共有k个分组，每个分组中的节点之间不能互相连接，而且这个3-CNF是可满足的。那么每个分组都会有一个节点与其他分组的节点相连，将每个分组的选出的那个节点互相连接起来，就是最大团。显然，最大团有k个节点。 下面给出《算法导论》中的一个k=3的实例，图中浅色的节点为每个分组中选出的节点。即3-CNF-SAT的一个可满足赋值为$x_2=0, x_3=1, x_1=0 或 1$。 归约过程在多项式内可以完成，因此团问题是NP完全问题。 顶点覆盖问题（VERTEX-COVER）证明： VERTEXT-COVER∈NP： 对于给定的图G(V, E)，如果给定顶点集V’作为证书，对于每条边(u, v)∈E，我们可以检查是否有u∈V’或v∈V’。这一验证在多项式时间内即可完成。 在多项式时间内将CLIQUE归约到VERTEX-COVER： 设G=(V, E)是CLIQUE的一个实例，设G的最大团为V’，取图G的补图，设补图上的边为E’，补图上的点为V-V’，那么V-V’是一个顶点覆盖。 原理：从E’中取任意的一条边(u, v)，那么在G中，边(u, v)是不存在的，那么u或v至少有一个在V-V’中。由于边(u, v)是任意取自E’的，即在补图中，任意一条边上都至少有一个点是属于V-V’的，即满足顶点覆盖的定义。 下面给出一个例子，V’ = {u, v, x, y}，V-V’ = {z, w}。 同样，这个归约过程能够在多项式时间内完成，因此顶点覆盖问题是NP完全问题。 哈密顿回路问题（HAM-CYCLE）证明： DIR-HAM-CYCLE∈NP： 对于一个给定的图G(V, E)，我们可以简单验证一个顶点序列是否经过所有顶点一次且仅一次，而且最后能够回到源点。这个过程显然是在多项式时间内完成的。 在多项式时间内将3-CNF-SAT归约到DIR-HAM-CYCLE： 构造一个能够表达3-CNF中的文字和子句的图结构，每行中的节点代表3-CNF中的每个文字，如第一行中的节点都代表了x1，那么跨行之间的节点之间即可代表3-CNF中的字句。 如果不清楚文字和字句的概念，可以参考上面3-CNF中关于合取范式的定义。 首先进行一个定义：当$x_i=1$时，遍历的顺序是从左到右，当$x_i=0$时，遍历的顺序是从右到左。从最上面的源点出发，以某种方式连接这些点以满足3-CNF，只要满足图中的3-CNF，那么这个图就是有向哈密顿回路。这个过程是在多项式时间内完成的。 接下来还要进行另外一个归约，才可以达到我们的目标： HAM-CYCLE∈NP： 与DIR-HAM-CYCLE的验证方法相同，这里不再赘述。 在多项式时间内将DIR-HAM-CYCLE归约到HAM-CYCLE： 给定一个具有n个顶点的有向图G=(V, E)，可以在多项式时间内构建一个具有3n个顶点的无向图G’ 以上两个归约都是在多项式时间内完成的，故哈密顿回路是NP完全的。 旅行商问题（TSP）证明： TSP∈ NP： 给定该问题的一个实例，用n个顶点组成的回路作为证书，我们可以验证该回路是否只包含每个顶点一次，并且检查各边费用之和是否小于k。这个过程能够在多项式时间内完成。 在多项式时间内将哈密顿回路归约到TSP： 设G=(V, E)是HAM-CYCLE的一个实例，可以构造对应的一个TSP实例。建立一个完全图G‘=(V, E’)，定义费用函数c为： 然后求解完全图G’上最大限定花费为0的路线即可。 下面来说明当且仅当图G’中有一个费用至多为0的回路的时候，G中才具有一个哈密顿回路： 假定图G中有一个哈密顿回路h。h中的每条边都属于E，因此在G’中的费用为0。因此，h在G‘中是费用为0的回路。 反之，假定图G’中有一个费用h‘至多为0的回路，回路上每条边的费用必为0。因此，h’仅包含E中的边。 这样，我们就得出结论，h’是图G中的一个哈密顿回路。 上述归约过程在多项式时间内完成，故旅行商问题是NP完全的。]]></content>
      <categories>
        <category>算法设计与分析</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CHAPTER6 动态规划]]></title>
    <url>%2Fposts%2F40122%2F</url>
    <content type="text"><![CDATA[概念分治法将问题划分成一些独立的子问题，递归地求解各子问题，然后合并子问题的解而得到原问题的解。 动态规划适用于子问题不是独立的情况，也就是个子问题包含公共的子子问题。动态规划对每个子子问题只求解一次，将其结果保存在一张表中，从而避免每次遇到各个子问题时重新计算答案。 动态规划的算法可分为以下4个步骤： 描述最优解的结构。 递归定义最优解的值。 按自底向上的方式计算最优解的值。 由计算的结果构造一个最优解。 0-1背包问题假设有n个物品，它们的重量分别为$w_1, w_2, …, w_n$，价值分别为$v_1, v_2, …, v_n$，给一承重为W的背包，求装入的物品具有最大的价值总和。 首先给出利用动态规划计算0-1背包问题的递归式： 在这个递归式中，$OPT(i, W)$表示前i件物品放入容量为W的背包中的最大价值。 初始化时，$OPT(0, j) = 0$，意为当没有物品放入时，不管背包容量多少，其最大价值为0；$OPT(i, 0)$意为当背包容量为0时，不管从前i件物品中怎么取，最大价值都是0。 接着进行动态规划，在已知$f(i-1, j)​$时，即已知在前$i-1​$件物品放入容量为j的背包时的最大价值情况下，求$f(i, j)​$。 在求$f(i, j)$时，首先判断物品i的重量是否超过目前背包的重量j，如果超过，则这个物品i放不进背包，则$f(i, j) = f(i-1, j)$。如果背包可以放下物品i，则尝试把背包中的重量减去物品i的重量$w_i$，这样$f(i-1, j-w_i)$表示前i-1件物品在背包容量为$i-w_i$下的最大价值，此时如果放入物品i，那么价值就变为$f(i-1, j-w_i)+v_i$。判断$f(i-1, j-w_i)+v_i$与$f(i-1, j)$的大小，选择较大的一个作为在背包容量为i下的最大价值。以上就是对于这个递归式的算法描述。 相对应的伪代码如下图所示： 可以将这个二维数组可视化，有如下图所示的五个物品和其对应的价值，且背包容量为11，那么如何让背包中装入的物品有最大的价值？ 遍历过程如下所示，从上到下代表i的遍历，从左到右代表j的遍历，最后可得到最大价值为40。 0-1背包问题已经被证明是NP完全问题，而它却有着一个动态规划解法，该解法有着O(nW)的时间复杂度，其中n是物品的个数，W是背包限制的最大负重。但是这种动态规划的算法称为伪多项式时间算法，这种算法不能真正意义上实现多项式时间内解决问题。 最长公共子序列（LCS问题）以最长公共子序列为例，将动态规划的四个步骤按部就班地走一遍。 Step1 描述最优解的结构（分析问题）假设$X = A, B, C, B$，$Y = B, D, C, A, B$，我们可以容易地想到暴力解法，即将枚举出X中的所有子序列，然后检查每个子序列是否是Y的子序列。假设X和Y的长度分别是m和n，那么暴力解法的时间复杂度是$O(2^mn)$。 我们可以观察到，LCS问题具有最优子结构，设$X=和Y=$为两个序列，$Z = $为X和Y的任意一个LCS，则有LCS的最优子结构定理： 如果$x_m=y_n$，那么$z_k = x_m = y_n$，而且$Z_{k-1}$是 $X_{m-1}$和$Y_{n-1}$的一个LCS。 如果$x_m \neq y_n$，那么$z_k \neq x_m$意味着$Z$是 $X_{m-1}$和$Y$的一个LCS。 如果$x_m \neq y_n$，那么$z_k \neq y_n$意味着$Z$是 $X$和$Y_{n-1}$的一个LCS。 Step2 递归定义最优解的值（递归解决）用$C[i,j]$表示$X_i$和$Y_j$的最长公共子序列LCS的长度，则有公式： Step3 按自底向上的方式计算最优解的值（计算LCS的长度）LCS_LENGTH以两个序列为输入，将LCS的长度保存到二维数组c中，将构造过程保存到另一个二维数组b中，伪代码如下所示： 1234567891011121314151617181920212223def LCS_LENGTH(X,Y): m = length(X) n = length(Y) # 初始化 for i = 1 to m: c[i][0] = 0 for j = 1 to n: c[0][j] = 0 # 计算LCS的长度 for i = 1 to m: for j = 1 to n: if x[i] == y[j]: c[i, j] = c[i-1, j-1] + 1 b[i, j] = '\' elif c[i-1, j] &gt;= c[i, j-1]: c[i, j] = c[i-1, j] b[i, j] = '|' else: c[i, j] = c[i, j-1] b[i, j] = '-'return c, b Step4 由计算的结果构造一个最优解（构建LCS）根据LCS_LENGTH返回的表b，可以构建一个LCS序列，输出所有值为’\’的元素，即可得到LCS，PRINT_LCS的伪代码如下所示： 123456789def PRINT_LCS(b, X, i, j): if i==0 or j==0: return 0 if b[i, j] == '\': PRINT_LCS(b, X, i-1, j-1) print X[i] elif b[i, j] == '|': PRINT_LCS(b, X, i-1, j) elif PRINT_LCS(b, X, i, j-1) 为了加深理解，使用C++实现了以上伪代码，使用PPT上的例子，最终得出结果如下图所示： 全部C++代码如下所示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172#include "stdafx.h"#include &lt;iostream&gt;using namespace std;#define m 4#define n 5#define skew 0#define up 1#define level 2void lcs_length(char*X, char*Y, int c[m+1][n+1], int b[m+1][n+1])&#123; int i, j; for (i=0; i&lt;m; i++) c[i][0] = 0; for (j=0; j&lt;n; j++) c[0][j] = 0; for (i=1; i&lt;=m; i++) for (j=1; j&lt;=n; j++) &#123; if (X[i] == Y[j]) &#123; c[i][j] = c[i-1][j-1] + 1; b[i][j] = skew; &#125; else if (c[i-1][j] &gt;= c[i][j-1]) &#123; c[i][j] = c[i-1][j]; b[i][j] = up; &#125; else &#123; c[i][j] = c[i][j-1]; b[i][j] = level; &#125; &#125;&#125;void print_lcs(int b[m+1][n+1], char* X, int i, int j)&#123; if (i==0 || j==0) return; if (b[i][j] == skew) &#123; print_lcs(b, X, i-1, j-1); cout &lt;&lt; X[i] &lt;&lt; ' '; &#125; else if(b[i][j] == level) print_lcs(b, X, i, j-1); else print_lcs(b, X, i-1, j);&#125;int main()&#123; char X[m+1] = &#123;' ','A','B','C','B'&#125;; char Y[n+1] = &#123;' ','B','D','C','A','B'&#125;; int c[5][6] = &#123;0&#125;; int b[5][6] = &#123;0&#125;; int i, j; cout &lt;&lt; "The X is ABCB" &lt;&lt; endl; cout &lt;&lt; "The Y is BDCAB" &lt;&lt; endl &lt;&lt; endl; lcs_length(X, Y, c, b); for(i=0; i&lt;=m; i++) &#123; for (j=0; j&lt;=n; j++) cout &lt;&lt; c[i][j] &lt;&lt; " "; cout &lt;&lt; endl; &#125; cout &lt;&lt; endl &lt;&lt; "The length of LCS is: " &lt;&lt; c[m][n] &lt;&lt; endl; cout &lt;&lt; "The LCS is: "; print_lcs(b, X, m, n); return 0;&#125; 编辑距离（Edit Distance）假设X和Y是两个字符串，我们要用最少的操作将X转换为Y，三种操作可供使用，分别是删除、插入和替换，最少操作的数目称为编辑距离。 与LCS类似，也可得到编辑距离的公式： 下面给出一个例子，具体的实现与LCS类似，这里不再赘述。 矩阵连乘问题（Chain MatrixMultiplication）给定n个矩阵$｛A1,A2,…,An｝$，其中$Ai$与$Ai+1$是可乘的，$i=1,2 ,…,n-1$。确定计算矩阵连乘积的计算次序，使得依此次序计算矩阵连乘积需要的数乘次数最少。 假如我们要得到计算从$A_i$到$A_j$的最优计算次序。首先假设这个计算次序在矩阵$A_k$和$A_{k+1}$之间断开，且$i \leq k &lt; j$，则计算量为前一部分的计算量、后一部分的计算量以及两部分相乘的计算量之和。 可以递归地定义$C[i, j]​$为： 可以看到，$k$的位置只有$j-i$种可能，此算法的时间复杂度为$O(n^3)​$，给出一个这种动态规划算法的计算实例。]]></content>
      <categories>
        <category>算法设计与分析</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CHAPTER5 分治法]]></title>
    <url>%2Fposts%2F57540%2F</url>
    <content type="text"><![CDATA[分治法是将一个复杂的问题分成一些规模较小而结构与原问题相似的子问题，递归地解这些子问题，然后将各子问题的解合并得到原问题的解。 分治法在每一层的递归上都有三个步骤： 分解（Divided）：将原问题分解成一系列子问题。 解决（Conquer）：递归地解各子问题。 合并（Combine）：将子问题的结果合并成原问题的解。 假设我们将原问题分解成a个子问题，每一个的大小是原问题的1/b。如果分解和合并的时间各为D(n)和C(n)，则可得到递归式： 合并排序（Merge Sort）当合并排序的运行时间如下分解： 分解：仅计算出子数组的中间位置，需要常量时间，故$D(n) = \Theta(1)$。 解决：递归地解两个规模为n/2的子问题，时间为$2T(n/2)$。 合并：在一个含有n个元素的子数组上，MERGE过程的运行时间为$\Theta(n)$，则$C(n) = \Theta(n)$。 因此，合并排序的最坏运行时间$T(n)$的递归表示是： 由之前第一章介绍的主定理，我们可以得到$T(n) = \Theta(nlgn)$，这里的$lgn$代表$log_2n$。同样，我们也可以通过递归树得到相同的答案。 最大子数组问题（Maximum Subarray Problem）对于一个具有连续连续值的数组A，寻找A的和最大的非空连续子数组，我们称这样的连续子数组为最大子数组。例如，对于下图的数组，A[1…16]的最大子数组是A[8…11]，其和为43。在实际的例子中可表示在第8天买入股票，并在第11天卖出，获得的收益为43美元。 使用分治法可以求解最大子数组问题。首先找到数组的中间点，将数组分为左右两个数组，那么最大子数组可能存在于下列三种情况之一： 完全位于左边的数组中。 完全位于右边的数组中。 跨越了左右两个数组。 对于前两种情况，使用同样的方式递归地划分为规模最小的子数组求解即可。对于第三种情况，我们采用的算法是从中间点向两边遍历，分别求出两边的最大子数组，然后将左右两边的子数组相加即为跨越中点的最大子数组。 下面给出算法的伪代码： 对FIND-MAXIMUM-SUBARRAY算法的运行时间进行分析： 首先计算出子数组的中间位置，需要常量时间$\Theta(1)$，然后递归地解两个规模为n/2的子问题，时间为$2T(n/2)$，调用FIND-MAX-CROSSING-SUBARRAY花费了$\Theta(n)$时间，则$T(n) = \Theta(1) + 2T(n/2) + \Theta(n) $，用主方法或递归树求解此递归式可得$T(n) = \Theta(nlgn)$。 斐波那契数列（Fibonacci Number）对于斐波那契数，我们有$F_0=0, F_1=1, F_n=F_{n-1}+F_{n-2}$。利用分治策略，我们可以将斐波那契数列转换为矩阵乘幂的问题，如下图所示： 使用分治法，我们将矩阵递归地分解成两个相同的矩阵，再将这两个矩阵相乘即可。 故$T(n) = T(n/2)+O(1) = T(n)/4+2O(1) = … = T(n/2^{logn})+O(logn)×O(1)$。 整数乘法（Integer Multiplication）假设x, y分别为两个n-bit的整数，如果要将它们相乘，模拟使用手动乘法得到的时间复杂度是$\Theta(n^2)$，考虑分治法。 令$x = (10^ma+b), y=(10^mc+d)$，如$x=1234567890, m=5, a=12345, b=67890$。那么$x×y = (10^ma+b)(10^mc+d)= 10^{2m}ac+10^m(bc+ad)+bd$，这里的时间复杂度$T(n) = 4T(n/2)+O(n)$，使用主方法可得$T(n) = O(n^2)$。 Anatolii, Karatsuba在1962年提出了一个只需要三次子乘法就可以完成运算的算法，其时间复杂度的递归表示为$T(n) = 3T(n/2)+O(n)$，使用主方法可得$T(n) = O(n^{lg3})$。 矩阵乘法（Matrix multiplication）给定一个n维矩阵X和Y，计算Z=XY。我们可以使用分治法求解这个问题。 分解：将X和Y分解为n/2维的矩阵。 解决：使用8次矩阵乘法递归地将这些n/2维的矩阵相乘。 合并：使用4次矩阵加法将矩阵合并。 下面给出一个例子： 1969年Strassen提出了一个只需要7次矩阵乘法就可以完成运算的算法，算法将原矩阵分为7个新的子矩阵如下图所示： 然后进行计算： 此算法的时间复杂度为$T(n) = 7T(n/2)+\Theta(n^2)=O(n{log_27})=O(n^{2.81})$。 凸包问题（The Convex Hull Problem）假设平面上有一系列点，过某些点作一个多边形，使这个多边形能把所有点都“包”起来，当这个多边形是凸多边形的时候，我们就叫它凸包，凸包问题就是求构成凸包的点，如下图所示。 使用蛮力法是最容易想到的，思路是由两点确定一条直线，如果剩余的点都在这条直线的同一侧，那么认为这两个点是构成凸包的点。蛮力法的时间复杂度为$O(n^3)$。 下面我们介绍解决凸包问题的分治法，下面为具体步骤和图例： 将所有点放在二维坐标系里，那么横坐标最大的两个点$p_1、p_n$一定是凸包上的点（具体可以用反证法证明，这里不展开说）。直线$p_1p_n$将点集合分为了两部分，分别叫上包和下包。 对于上包，求距离直线$p_1p_n$最远的点，即下图中的$p_{max}$。 作直线$p_1p_{max}$和$p_np_{max}$，把$p_1p_{max}$左侧的点当作上包，把$p_np_{max}$右侧的点也当作是上包。 重复步骤2、3。 对下包也做类似的操作。 分治法的时间复杂度为$T(n) = 2T(n/2) + O(n) = O(nlogn)$。 三格骨牌问题（Tromino Tiling）对于三格骨牌问题，同样可以用分治法求解，解决这个问题的思想是每次都将平板分成四块同等大小的子平板。 例如，在插入三格骨牌时，将平板分成四块，由于洞位于左上方的子平板，因此将三个骨牌放置为如图所示的位置，以确保四块子平板的大小相等。递归地进行这个过程即可得出结果。 最邻近点问题（Finding the Closest Pair of Points）顾名思义，最邻近点问题即在平面点集中找出距离最近的两个点。使用时间复杂度为$O(n^2)$的蛮力法可以解决这个问题。 下面我们介绍解决这个问题的分治法，首先将点集划分为两个部分，然后递归地寻找最近的点，若找到最近的两个点之间的距离为$\delta$，则寻找是否存在分别属于两个部分的点之间的距离小于$\delta$，算法的时间复杂度为$T(n) = O(n)+2T(n/2)+O(n)=O(nlgn)$。]]></content>
      <categories>
        <category>算法设计与分析</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CHAPTER4 贪心算法]]></title>
    <url>%2Fposts%2F17200%2F</url>
    <content type="text"><![CDATA[贪心算法即不从整体最优考虑，只是做出在当前看来最好的选择，它所做出的选择只是在某种意义上的局部最优选择。贪心算法对于大多数优化问题都能得到整体最优解（如单源最短路径问题、最小生成树问题等），虽然说并不总是这样的。在一些情况下，即使贪心算法不能得到整体最优解，但是其最终的结果是最优解的近似。 动态规划算法通常以自底向上的方式解各子问题，而贪心算法则通常以自顶向下的方式进行，每作一次贪心选择就将所求问题简化为规模更小的子问题。 下面来看一些具体的例子。 区间调度（Interval Scheduling）问题描述：假如我们有多个任务，每个任务都具有各自的开始时间和结束时间，求在任务不重叠的情况下任务的最大组合数。 根据贪心算法，我们可以从不同的角度分析这个问题。 Rule1：选择开始最早的任务 每次选择当前最先开始的任务，并依次选到最后一个。 从这个例子看出，失败！ Rule2：选择区间最短的任务 从最短的任务开始选择，并依次选择不重复的当前最短的任务，按照所用时间长短排序。 从这个例子看出，失败！ Rule3：选择冲突最少的任务对于每一个任务，计算与它冲突的任务个数，每次选择当前与其冲突最少的任务。 从这个例子看出，还是失败了。 Rule4：选择结束最早的任务每次选择当前最早结束的任务，并依次选到最后一个。 可以看出，在上面的三个例子中，本算法都是可行的。事实上，Rule4的算法是解决区间调度问题的一个可行算法。直观上来说，按这种方法选取的任务为未安排的任务留下了尽可能多的时间。也就是说，该算法的贪心选择使剩余的可安排时间段极大化，以安排尽可能多的不重叠活动。这个算法的时间复杂度是$O(nlogn)$。 集合覆盖（Set Cover）问题描述：在一个集合$B$以及$B$内元素构成的若干集合$S_1, S_2, … , S_m$中，找到数目最少的$S_i$使得$S_i$中的所有元素都包含了$B$中所有元素。为便于理解，给一个具体的例子。例如，$B$={1,2,3,4,5}，$S_1$={1,2,3}，$S_2$={2,4}，$S_3$={3,4}，$S_4$={4,5}，可以找到一个集合覆盖$S_1, S_4$。 其实集合覆盖问题是一个NP难的问题，但是我们仍然可以用贪心算法得到这个问题的近似解。 假设我们要在城镇里建几所学校，需要满足两点：1、每个学校都要建在一个城镇里。2、城镇离学校的距离不能超过30英里。 根据贪心算法，我们可以得到非常接近的解： 选出这样一个学校，即它覆盖了数量最多的城镇。 重复第一步，直到覆盖所有的城镇。 这是一种近似算法，贪心算法的运行时间为$O(n^2)​$，可得到如下图所示的结果，图中点的位置代表城镇的位置。 最小生成树（Minimum Spanning Tree）用贪心算法设计策略可以设计出构造最小生成树的有效算法，如Prim算法和Kruskal算法，尽管它们做贪心选择的方式不同。 关于MST的这两个算法相信大家应该都十分熟悉了，这里便不再展开。]]></content>
      <categories>
        <category>算法设计与分析</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CHAPTER3 最大匹配问题]]></title>
    <url>%2Fposts%2F63093%2F</url>
    <content type="text"><![CDATA[关于匹配的几个定义匹配（Matching）一个匹配是一个边的子集合$M\subseteq E$，且满足对所有顶点$v\in V$，$M$中至多有一条边与$v$相关联。也可以简单地说，一个匹配就是一个边的集合，其中任意两条边之间都没有公共顶点。 下面给出一个例子： 最大匹配（Maximum Matching）简单地说，最大匹配是一个图的所有匹配中边数最多的那个匹配。给出一个例子： 二分图（Bipartite Graph）二分图是图论中的一种特殊模型，设$G=(V,E)$是一个无向图，如果顶点$V$可分割为两个互不相交的子集$(A,B)$，并且图中的每条边$(i，j)$所关联的两个顶点$i$和$j$分别属于这两个不同的顶点集，则称图$G$为一个二分图。 完全匹配（Perfect Matching）简单地说，当一个图的某个匹配中所有的顶点都是匹配点，那么这个匹配就是完美匹配。同样给出一个例子： 在二分图中寻找最大匹配从本质上来说，二分图匹配其实是最大流的一种特殊情况。是解决这个问题的关键技巧在于建立一个流网络，其中流对应于匹配，如下图所示。 可以看出，图中添加了源点s和汇点t，它们是不属于V的新顶点。令已有边的容量为无穷大，且令s和t分别连接二分图，并设置其容量为1。这时，我们通过Ford-Fulkerson方法计算得到的最大流就等于最大二分匹配。 在一般图中寻找最大匹配在一般图中，我们使用增广路径（Agumenting path）来寻找最大匹配。如果一条路径的首尾是非匹配点，路径中除此之外（如果有）其他的点均是匹配点，那么这条路径就是一条增广路径。 如下图所示，我们从非匹配点9出发，经过匹配点4、8、1、6，最后在非匹配点2停止。所以，9-&gt;4-&gt;8-&gt;1-&gt;6-&gt;2 就是一条增广路径。 由于增广路径的首尾是非匹配点，那么增广路径的首尾边必为非匹配边。由于增广路径中匹配边与非匹配边一次交替，所以非匹配边的数目比匹配边多一条。我们可以利用这个特性来改进匹配，只要将匹配边与非匹配边互换即可，如下图所示。 所以，只要不断地迭代这个过程，直至找不到增广路径为止，就可以找到一般图的最大匹配。 补充：若要寻找带权一般图上的最大匹配，则在上面算法的基础上加个权重和判断即可。]]></content>
      <categories>
        <category>算法设计与分析</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CHAPTER2 最大流与最小割]]></title>
    <url>%2Fposts%2F16099%2F</url>
    <content type="text"><![CDATA[流网络$G=(V, E)​$是一个简单有向图，在V中指定顶点s和t，分别称为源点和汇点，有向图G中的每一条边$(u, v) \in E​$，对应有一个值$cap(u, v) \geq0​$，称为边的容量，这样的有向图G称作一个流网络，下图是一个例子。 $f(v, u)​$称作是从顶点u到顶点v的流，它满足以下性质： 容量限制：对所有$u, v \in V$，要求$f(u, v) \leq c(u, v)$。 反对称性：对所有$u, v \in V$，要求$f(u, v) = -f(v, u)$。 如果有一组流满足以下条件，那么这组流就成为一个可行流： 源点s：流出量 = 整个网络的流量 汇点t：流入量 = 整个网络的流量 中间点：总流入量 = 总流出量 最大流即网络G所有的可行流中，流量最大的一个可行流。 Ford-Fulkerson方法之所以称为Ford-Fulkerson方法而不是算法，是由于它包含具有不同运行时间的几种实现。Ford-Fulkerson方法依赖于三种重要思想：残留网络、增广路径、割。这三种思想是最大流最小割定理的精髓，该定理用流网络的割来描述最大流的值，我们将会在后面谈到。以下给出Ford-Fulkerson方法的伪代码： 12345Ford-Fulkerson-Method(G, s, t): initialize flow f to 0 while there exists an augmenting path p: do augment flow f along p return f 最大流最小割定理首先来介绍割的概念，一个割会把图G的顶点分成两个不相交的集合，其中s在一个集合中，t在另外一个集合中。割的容量就是从A指向B的所有边的容量和，最小割问题就是要找到割的容量最小的情况。下面给出两个例子，割的容量分别为30和62。 接着介绍残留网络和增广路径的概念，给定一个流网络$G$和一个可行流，流的残留网络$G_f$拥有与原网相同的顶点。流网络$G$中每条边将对应残留网中一条或者两条边，对于原流网络中的任意边(u, v)，流量为f(u, v)，容量为c(u, v)： 如果f(u, v) &gt; 0，则在残留网中包含一条容量为f(u, v)的边(v, u); 如果f(u, v) &lt; c(u, v)，则在残留网中包含一条容量为c(u, v) - f(u, v)的边(u, v)。 下图为一个例子： 对于一个已知的流网络$G=(V, E)$和流$f$，增广路径$p$为残留网络$G_f$中从s到t的一条简单路径。 最大流最小割定理：网络的最大流等于某一最小割的容量，并且下列条件是等价的： $f$是$G$的一个最大流。 残留网络$G_f$不包含增广路径。 对$G$的某个割$(S, T)$，有$|f| = c(S, T)$。 基本的Ford-Fulkerson算法根据，我们可以求给定有向图的最大流。下面给出《算法导论》中的一个实例： 上图中的左边表示开始时的残留网络，右边表示将增广路径加入残留网络后得到的新的可行流，通过三次迭代即可得到最大流，根据最大流最小割定理，我们同样可以得到最小割。 再通过本课程课件上的一个例题进行练习。 同样通过基本的Ford-Fulkerson算法，可得到答案如下。 Edmonds-Karp算法Edmonds和Karp曾经证明了如果每步的增广路径都是最短，那么整个算法会执行$O(mn)$步，Edmonds-Karp算法是用广度优先搜索来实现对增广路径p的计算的，实现的伪代码如下图所示。 由于在广度优先搜索时最坏情况下需要$O(m)$次操作，所以此算法的复杂度为$O(m^2n)$。之后，Dinitz改进了Edmonds-Karp算法，得到一个时间复杂度为$O(mn^2)​$的算法，下面给出一张关于最短增广路径算法研究历史的表格，这里就不再展开了。]]></content>
      <categories>
        <category>算法设计与分析</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CHAPTER1 函数的增长与递归式]]></title>
    <url>%2Fposts%2F36105%2F</url>
    <content type="text"><![CDATA[渐进记号用来表示算法的渐进运行时间的记号是用定义域为自然数集$N=\{ 0, 1, 2, …\}$的函数来定义的，这些记号便于用来表示最坏情况运行时间$T(n)$，因为$T(n)​$一般仅定义于整数的输入规模上。 $\Theta$记号（紧渐进界）对于$\Theta​$记号有如下的定义： $\Theta$记号限制一个函数在常数因子内，如图所示，$n_0$是最小的可能值。如果存在正常数$n_0, c_1, c_2$使得在$n_0$右边$f(n)$的值永远在$c_1g(n)$与$c_2g(n)$之间，那么可以写成$f(n) = \Theta(g(n))​$。 O记号（渐进上界）对于$O$记号有如下的定义： $O$记号给出一个函数在常数因子内的上限。如图所示，$n_0$是最小的可能值。如果存在正常数$n_0, c$使得在$n_0$右边$f(n)$的值永远等于或小于$cg(n)$，那么可以写成$f(n) = O(g(n))$。 Ω记号（渐进下界）对于$Ω​$记号有如下的定义： $Ω$记号给出一个函数在常数因子内的下限。如图所示，$n_0$是最小的可能值。如果存在正常数$n_0, c$使得在$n_0$右边$f(n)$的值永远等于或大于$cg(n)$，那么可以写成$f(n) = Ω(g(n))$。 o记号（渐进非紧上界）$O$记号所提供的渐进上界可能是紧的，但也有可能不是。例如，$2n^2=O(n^2)$是一个紧的上界，但$2n=O(n^2)$却不是一个紧的上界。于是，我们使用$o$记号来表示一个紧的上界。 对于$o​$记号有如下的定义： 例如，$2n=o(n^2)$，但$2n^2 \neq o(n^2)$。 $O$记号与$o$记号的定义是类似的，主要区别在于对于$f(n)=O(g(n))$，界$0 \leq f(n) \leq cg(n)$对某个常数$c&gt;0$成立即可，而对于$f(n)=o(g(n))$，界$0 \leq f(n) \leq cg(n)$对所有常数$c&gt;0$成立。 $ω$记号（渐进非紧下界）$ω$记号与$Ω$记号的关系就与前面小o和大o之间的关系是类似的，我们用$ω$记号表示一个紧的下界。 对于$ω$记号有如下的定义： 例如，$n^2/2=ω(n)$，但$n^2/2\neqω(n^2)$。 函数间的比较实数的许多关系属性可以用于渐进比较，以上的记号之间具有传递性和对称性，下面假设$f(n)$和$g(n)$是渐进正值函数。 解递归式的三种方法求解递归式，即找出解的渐进“$\Theta$”或“$O$”界的方法主要有三种： 代换法：先猜某个界存在，然后用数学归纳法证明该猜测的正确性。 递归树方法：将递归式转换成树形结构，树中的节点代表在不同递归层次付出的代价。 主方法：给出递归形式$T(n) = aT(n/b)+f(n)$的界，其中$a \geq 1, b&gt;1$，$f(n)$是给定的函数。这种方法要记忆三种情况，就可以确定很多简单递归式的界了。 代换法用代换法解递归式需要两个步骤： 猜测解的形式。 用数学归纳法找出使解真正有效地常数。 递归树方法虽然代换法给递归式的解的正确性提供了一种简单的证明方法，但是有的时候很难得到一个好的猜测。此时，画出一个递归树是一种得到好猜测的直接方法。 设$T(n) = 3T(n/4)+n^2​$，则使用递归树求解该递归式的过程如下图所示： 主方法设$a \geq 1, b&gt;1$，$f(n) $为一函数，$T(n)$由递归式 T(n) = aT(n/b)+f(n)对非负整数定义，那么$T(n)$有如下的渐进界： 求解和式时有一个比较常用的公式，假设$f(k)$是单调递增的函数，那么有如下的性质：]]></content>
      <categories>
        <category>算法设计与分析</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CHAPTER8 神经网络3]]></title>
    <url>%2Fposts%2F38226%2F</url>
    <content type="text"><![CDATA[神经网络共有三个部分，在前两个部分中，我们讨论了神经网络的静态部分：如何创建网络的连接、数据和损失函数。本节将致力于讲解神经网络的动态部分，即神经网络学习和搜索最优超参数的过程。 梯度检查使用中心化公式： \frac{df(x)}{dx} = \frac{f(x+h)-f(x-h)}{2h}使用相对误差来比较：如何比较数值梯度$f’_n$和$f’_a$？使用相对误差是更加合适的： \frac{\left|f'_a - f'_n\right|}{max(\left|f'_a\right|,\left|f'_n\right|)}上式考虑了差值占两个梯度绝对值的比例。在实践中： 相对误差 &gt; 1e-2，通常意味着梯度可能出错。 1e-4 &lt; 相对误差 &lt; 1e-2，我们要对这个值感到不舒服。 相对误差 &lt; 1e-4，这个值对于有不可导点的目标函数是OK的，但是目标函数中如果没有kink（目标函数的不可导点），那么相对误差还是太高。 相对误差 &lt; 1e-7，这个值是个好结果。 要知道的是网络的深度越深，相对误差就越高。如果是在对一个10层网络的输入数据做梯度检查，那么1e-2的相对误差值可能就OK了，因为误差一直在累积。 使用双精度：一个常见的错误是使用单精度浮点数来进行梯度检查。这样会导致即使梯度实现正确，相对误差值也会很高（比如1e-2）。 目标函数的不可导点（kinks）：在进行梯度检查时，一个导致不准确的原因是不可导点问题。如ReLU函数的原点就是不可导点，比如的当$x=-1e6$时，对ReLU函数进行梯度检查。因为x&lt;0，所以解析梯度在该点的梯度为0。然而，这里的数值梯度可能会突然计算出一个非零的梯度值，因为f(x+h)可能越过了不可导点。 使用少量数据：解决上面的不可导点问题的一个办法是使用更少的数据。因为含有不可导点的损失函数的数据点越少，不可导点就越少，所以在计算有限差值近似时越过不可导点的几率就越小。 谨慎设置步长h。在实践中h并不是越小越好，因为当h特别小的时候，就可能就会遇到数值精度问题。有时候如果梯度检查无法进行，可以试试将h调到1e-4或者1e-6，然后突然梯度检查可能就恢复正常。 在特定的模式下使用梯度检查：最好让网络学习一小段时间，等到损失函数开始下降之后再进行梯度检查。在第一次迭代后就进行梯度检查的危险在于，此时可能正处在不正常的边界情况，从而掩盖了梯度没有正确实现的事实。 不要让正则化吞没数据：需要注意的危险是正则化损失可能吞没掉数据损失，在这种情况下梯度主要来源于正则化部分。这样就会掩盖掉数据损失梯度的不正确实现。因此，推荐先关掉正则化对数据损失做单独检查，然后对正则化做单独检查。对于正则化的单独检查可以是修改代码，去掉其中数据损失的部分，也可以提高正则化强度，确认其效果在梯度检查中是无法忽略的，这样不正确的实现就会被观察到了。 记得关闭随机失活（dropout）和数据扩张（augmentation）：在进行梯度检查时，记得关闭网络中任何不确定的效果的操作，比如随机失活，随机数据扩展等。不然它们会在计算数值梯度的时候导致巨大误差。 检查少量的维度。在实际中，梯度可以有上百万的参数，在这种情况下只能检查其中一些维度然后假设其他维度是正确的。 学习之前的合理性检查在进行费时费力的最优化之前，最好进行一些合理性检查： 寻找特定情况的正确损失值。在使用小参数进行初始化时，确保得到的损失值与期望一致。最好先单独检查数据损失（让正则化强度为0）。例如，对于一个跑CIFAR-10的Softmax分类器，一般期望它的初始损失值是2.302，这是因为初始时预计每个类别的概率是0.1（因为有10个类别），然后Softmax损失值正确分类的负对数概率：-ln(0.1)=2.302。对于Weston Watkins SVM，假设所有的边界都被越过（因为所有的分值都近似为零），所以损失值是9（因为对于每个错误分类，边界值是1）。如果没看到这些损失值，那么初始化中就可能有问题。 提高正则化强度时导致损失值变大。 对小数据子集过拟合。最后也是最重要的一步，在整个数据集进行训练之前，尝试在一个很小的数据集上进行训练（比如20个数据），然后确保能到达0的损失值。进行这个实验的时候，最好让正则化强度为0，不然它会阻止得到0的损失。除非能通过这一个正常性检查，不然进行整个数据集训练是没有意义的。但是注意，能对小数据集进行过拟合并不代表万事大吉，依然有可能存在不正确的实现。比如，因为某些错误，数据点的特征是随机的，这样算法也可能对小数据进行过拟合，但是在整个数据集上跑算法的时候，就没有任何泛化能力。 检查整个学习过程在训练神经网络的时候，应该跟踪多个重要数值。这些数值输出的图表是观察训练进程的一扇窗口，是直观理解不同的超参数设置效果的工具，从而知道如何修改超参数以获得更高效的学习过程。 在下面的图表中，x轴通常都是表示周期（epochs）单位，该单位衡量了在训练中每个样本数据都被观察过次数的期望（一个周期意味着每个样本数据都被观察过了一次）。相较于迭代次数（iterations），一般更倾向跟踪周期，这是因为迭代次数与数据的批尺寸（batchsize）有关，而批尺寸的设置又可以是任意的。 损失函数训练期间第一个要跟踪的数值就是损失值，它在前向传播时对每个独立的批数据进行计算。下图展示的是随着损失值随时间的变化。 左图展示了不同学习率的效果，右图展示了一个典型的随时间变化的损失函数值。 损失值的震荡程度和批尺寸（batch size）有关，当批尺寸为1，震荡会相对较大。当批尺寸就是整个数据集时震荡就会最小，因为每个梯度更新都是单调地优化损失函数（除非学习率设置得过高）。 训练集和验证集准确率在训练分类器的时候，需要跟踪的第二重要的数值是训练集和验证集的准确率。 两者之间的空隙表示模型过拟合的程度。 图中蓝色的曲线表示此模型有很强的过拟合，此时，我们应该增大正则化强度（更强的L2权重惩罚，更多的随机失活等）或收集更多的数据。 另一种可能是训练集曲线和验证集曲线很贴近，这说明模型容量还不够大，应该增加参数数量增大模型容量。 权重更新比例最后一个应该跟踪的是权重中更新值的数量和全部值的数量之间的比例。一个经验性的结论是这个比例应该在1e-3左右。如果更低，说明学习率可能太小，如果更高，说明学习率可能太高。下面是具体例子： 123456# 假设参数向量为W，其梯度向量为dWparam_scale = np.linalg.norm(W.ravel())update = -learning_rate*dW # 简单SGD更新update_scale = np.linalg.norm(update.ravel())W += update # 实际更新print update_scale / param_scale # 要得到1e-3左右 每层的激活数据及梯度分布一个不正确的初始化可能让学习过程变慢，甚至彻底停止。一个方法是输出网络中所有层的激活数据和梯度分布的柱状图。直观地说，就是如果看到任何奇怪的分布情况，那都不是好兆头。比如，对于使用tanh的神经元，我们应该看到激活数据的值在整个[-1,1]区间中都有分布。如果看到神经元的输出全部是0，或者全都饱和了往-1和1上跑，那肯定就是有问题了。 第一层可视化如果数据是图像，那么把第一层特征可视化可能会有帮助，以下是一个将神经网络第一层的权重可视化的例子。 左图中的特征充满了噪音，这表明网络可能出现了问题：网络没有收敛，学习率设置不恰当，正则化惩罚的权重过低等。 右图的特征不错，平滑、干净且种类繁多，说明训练过程进行良好。 参数更新一旦能使用反向传播计算解析梯度，梯度就能够被用来进行参数更新了，参数更新有以下的好几种方法。深度网络的最优化是现在非常活跃的研究领域，本节将介绍一些公认有效的常用技巧。 随机梯度下降及各种更新方法随机梯度下降（SGD）最简单的更新形式是沿着负梯度方向改变参数，其最简单的更新形式是： 12# 普通更新x += -learning_rate * dx 动量方法（Momentum）另一个有效的方法是动量更新（Momentum），这个方法在深度网络上几乎总能得到更好的收敛速度，可以看作是从物理角度对于最优化问题得到的启发。损失值可以理解为山的高度，用随机数字初始化参数等同于在某个位置给质点设定初始速度为0。这样最优化过程可以看做是模拟参数向量（即质点）在地形上滚动的过程。 在SGD中，梯度直接影响位置。而在Momentum中，物理观点建议梯度只影响速度，然后速度再影响位置： 123# 动量更新v = mu * v - learning_rate * dx # 与速度融合x += v # 与位置融合 这里引入了一个初始化为0的变量v和超参数mu。mu在最优化的过程中被看作是动量（一般设为[0.5, 0.9, 0.95, 0.99]中的一个），但其物理意义与摩擦系数更一致。这个变量有效地抑制了速度，降低了系统的动能。和学习率退火类似，动量随时间变化的设置有时能略微改善最优化的结果，动量在后期应该上升。一个典型的例子是刚开始时将mu设为0.5，在后面的周期中慢慢提升到0.99。 通过动量更新，参数向量会在任何有持续梯度的方向上增加速度。 NAG（Nesterov Accelerated Gradient）与普通动量方法有些许不同，理论上NAG对于凸函数能得到更好地收敛，在实际中也比普通动量方法表现的更好一些。 NAG的核心思路是，当参数向量位于某个位置x时，Momentum会通过mu*v稍微改变参数向量。因此，如果要计算梯度，可以直接计算x + mu*v而不是x，可以将x + mu*v看作是未来的近似位置，这样的计算就更有意义了。 如上图所示，既然我们知道动量会把我们带到绿色箭头指向的点，我们就不要在原点那里计算梯度了。使用Nesterov向量，我们就在这个x_ahead的地方计算梯度： 1234x_ahead = x + mu * v# 计算dx_ahead(在x_ahead处的梯度，而不是在x处的梯度)v = mu * v - learning_rate * dx_aheadx += v 然而在实践中，人们更喜欢使用与SGD、Momentum类似简单的表达式，通过改写可得到： 123v_prev = v # 存储备份v = mu * v - learning_rate * dx # 速度更新保持不变x += -mu * v_prev + (1 + mu) * v # 位置更新变了形式 学习率衰减方式如果学习率很高，系统的动能就过大，参数向量就会无规律地跳动，不能够稳定到损失函数更深更窄的部分去。通常，实现学习率退火有3种方式： 随步数衰减：每进行几个周期就根据一些因素降低学习率。典型的值是每过5个周期就将学习率减少一半，或者每20个周期减少到之前的0.1。这些数值的设定是严重依赖具体问题和模型的选择的。在实践中可能看见这么一种经验做法：使用一个固定的学习率来进行训练的同时观察验证集错误率，每当验证集错误率停止下降，就乘以一个常数（比如0.5）来降低学习率。 指数衰减：数学公式是$\alpha = \alpha_0e^{-kt}$，其中$\alpha_0, k$是超参数，$t$是迭代次数。 1/t衰减：数学公式是$\alpha = \alpha_0/(1+kt)$。其中$\alpha_0, k$是超参数，$t$是迭代次数。 在实践中，我们发现随步数衰减的随机失活（dropout）更受欢迎，因为它使用的超参数（衰减系数和以周期为时间单位的步数）比k更有解释性。最后，如果你有足够的计算资源，可以让衰减更加缓慢一些，让训练时间更长些。 二阶方法第二类常用的最优化方法是基于牛顿法的，其迭代如下： x \leftarrow x - [H f(x)]^{-1} \nabla f(x)这里的$Hf(x)$是Hessian矩阵，它是函数的二阶偏导数的平方矩阵。直观理解上，就是乘以Hessian转置矩阵可以让最优化过程在曲率小的时候大步前进，在曲率大的时候小步前进。这个公式中没有学习率这个超参数，这相较于一阶方法是一个巨大的优势。 然而此方法很难用到实际中去，因为计算（以及求逆）Hessian矩阵操作非常耗费时间和空间。举例来说，假设一个有一百万个参数的神经网络，其Hessian矩阵大小就是[1,000,000 x 1,000,000]，将占用将近3,725GB的内存。在这些方法中最流行的是L-BFGS，该方法使用随时间的梯度中的信息来隐式地近似。 然而，即使解决了存储空间的问题，L-BFGS的一个巨大劣势是要对整个训练集进行计算，而整个训练集一般包含几百万的样本。而与mini-batch SGD不同，让L-BFGS在小批量上运行起来很需要技巧。 在实践中，使用L-BFGS之类的二阶方法并不常见。相反，基于Nesterov的动量更新的各种随机梯度下降方法更加常用，因为它们更简单而且更容易扩展。 逐参数适应学习率方法前面讨论的所有方法都是对学习率进行全局的操作，对于所有的参数都是一样的。学习率调参是很耗费计算资源的过程，所以很多工作都在研究能够适应性地对学习率调参的方法，甚至是逐个参数适应学习率调参。 AdagradAdagrad是一个适应性学习率算法： 123# 假设有梯度和参数向量xcache += dx**2x += -learning_rate * dx / (np.sqrt(cache) + eps) cache的尺寸和梯度矩阵的尺寸是一样的，还跟踪了每个参数梯度dx的平方和。注意，接收到高梯度值的权重更新的效果被减弱，而接收到低梯度值的权重更新的效果将会增强。eps（一般设为1e-4到1e-8之间）是防止出现除以0的情况。Adagrad的一个缺点是，单调的学习率通常过于激进且过早停止学习。 RMSPropRMSprop是一个非常高效但没有公开发表的适应性学习率算法。这个方法用一种很简单的方式修改了Adagrad方法，让它不那么激进： 12cache = decay_rate * cache + (1 - decay_rate) * dx**2x += - learning_rate * dx / (np.sqrt(cache) + eps) decay_rate是一个超参数，常用的值是[0.9, 0.99, 0.999]。RMSProp仍然是基于梯度的大小来对每个权重的学习率进行修改，这样的效果同样不错。但与Adagrad不同的是，RMSProp的更新不会让学习率单调变小。 AdamAdam是最近才提出的一种更新方法，它看起来像是RMSprop的动量版： 123m = beta1*m + (1-beta1)*dxv = beta2*v + (1-beta2)*(dx**2)x += -learning_rate * m / (np.sqrt(v) + eps) 这个方法和RMSProp很像，只是把原始梯度向量dx替换成了平滑版的梯度m。论文中推荐的参数值eps=1e-8，beta1=0.9，beta2=0.999。 在实践中，推荐Adam作为默认的算法，它一般比RMSProp的效果好一些。但是也可以试试SGD+Nesterov动量。 第一张动画是一个损失函数的等高线图，基于动量的方法出现了走偏了的情况。 第二张动画展示了一个马鞍状的最优化地形，其中对于不同维度它的曲率不同（在一个维度下降，而在另一个维度上升）。SGD、Momentum、NAG很难突破对称性，一直卡在顶部。而逐参数适应性学习率算法可以看到马鞍另一个维度上具有很低的梯度。 经验之谈 对于稀疏数据，尽量使用学习率可自适应的优化方法，不用手动调节，而且最好采用默认值 SGD通常训练时间更长，但是在好的初始化和学习率调度方案的情况下，结果更可靠 如果在意更快的收敛，并且需要训练较深较复杂的网络时，推荐使用学习率自适应的优化方法。 Adadelta，RMSprop，Adam是比较相近的算法，在相似的情况下表现差不多。 在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果 超参数调优我们已经看到，训练一个神经网络会遇到很多超参数设置。神经网络最常用的设置有： 初始学习率。 学习率衰减方式（例如一个衰减常量）。 正则化强度（L2惩罚，随机失活强度）。 实现。更大的神经网络需要更长的时间去训练，所以调参可能需要几天甚至几周。一个具体的设计是用worker持续地随机设置参数然后进行最优化。 比起交叉验证最好使用一个验证集。在大多数情况下，一个尺寸合理的验证集可以让代码更简单，不需要用几个数据集来交叉验证。 超参数范围。在对数尺度上进行超参数搜索。例如，一个典型的学习率应该看起来是这样：learning_rate = 10 \ uniform(-6, 1)。对于正则化强度，可以采用同样的策略。但是有一些参数（比如随机失活）还是在原始尺度上进行搜索（例如：dropout=uniform(0,1)**）。 随机搜索优于网格搜索。Bergstra和Bengio在文章Random Search for Hyper-Parameter Optimization中说“随机选择比网格化的选择更加有效，而且在实践中也更容易实现。 上面是Random Search for Hyper-Parameter Optimization的核心说明图。通常，有些超参数比其余的更重要，通过随机搜索，可以让你更精确地发现那些比较重要的超参数的好数值。 对于边界上的最优值要小心。这种情况一般发生在你在一个不好的范围内搜索超参数（比如学习率）的时候。比如，假设我们使用learning_rate = 10 \ uniform(-6,1)**来进行搜索。一旦我们得到一个比较好的值，一定要确认它不在这个范围的边界上，不然你可能错过更好的其他搜索范围。 从粗到细地分阶段搜索。在实践中，先进行初略范围（比如10 ** [-6, 1]）搜索，然后根据好的结果出现的地方，缩小范围进行搜索。进行粗搜索的时候，让模型训练1个周期就可以了，因为很多超参数的设定会让模型没法学习，或者突然就爆出很大的损失值。第二个阶段就是对一个更小的范围进行搜索，这时可以让模型运行5个周期，而最后一个阶段就在最终的范围内进行仔细搜索，运行很多次周期。 贝叶斯超参数最优化。是一整个研究领域，主要是研究在超参数空间中更高效的导航算法，这里不展开讨论。 模型集成在实践的时候，有一个总是能提升神经网络几个百分点准确率的办法，就是在训练的时候训练几个独立的模型，然后在测试的时候将它们的预测结果进行平均。模型数量增加，算法的结果也单调提升（尽管提升效果越来越少）。模型之间的差异度越大，提升效果可能越好。进行集成有以下几种方法： 同一个模型，不同的初始化。使用交叉验证来得到最好的超参数，然后用最好的参数来训练不同初始化条件的模型。 在交叉验证中发现最好的模型。使用交叉验证来得到最好的超参数，然后取其中最好的几个（比如10个）模型来进行集成。这样就提高了集成的多样性，但风险在于可能会包含不够理想的模型。在实际操作中，这样操作起来比较简单，在交叉验证后就不需要额外的训练了。 一个模型设置多个记录点。如果训练非常耗时，那就在不同的训练时间对网络留下记录点（比如每个周期结束），然后用它们来进行模型集成。很显然，这样做多样性不足，但是在实践中效果还是不错的，这种方法的优势是代价比较小。 在训练的时候跑参数的平均值。还有一个也能得到1-2个百分点的提升的小代价方法，就是在训练过程中，如果损失值相较于前一次权重出现指数下降时，就在内存中对网络的权重进行一个备份。将最后几次得到的权重进行平均，你会发现这个“平滑”过的版本的权重总是能得到更少的误差。直观的理解就是目标函数是一个碗状的，你的网络在这个周围跳跃，所以对它们平均一下，就更可能跳到中心去。 总结训练一个神经网络需要： 利用少量数据进行梯度检查。 进行合理性检查，确认初始损失值的合理的，在小数据集上能得到100%的准确率。 在训练时，跟踪损失函数值、训练集和验证集的准确率，还可以跟踪更新的参数量相对于总参数量的比例（1e-3左右）。对于卷积神经网络，可以将第一层的权重可视化。 推荐的两个参数更新方法是SGD+Nesterov动量方法和Adam方法。 随着训练进行学习率衰减。 使用随机搜索（而不是网格搜索）来搜索最优的超参数，从粗（比较宽的超参数范围训练1-5个周期）到细（窄范围训练很多个周期）地搜索。 进行模型集成来获得额外的性能提高。 参考：https://zhuanlan.zhihu.com/p/21930884]]></content>
      <categories>
        <category>cs231n</category>
      </categories>
      <tags>
        <tag>Computer Vision</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CHAPTER7 神经网络2]]></title>
    <url>%2Fposts%2F20956%2F</url>
    <content type="text"><![CDATA[设置数据和模型在上一节中介绍了神经元的模型，具体来说，神经网络就是进行了一系列的线性映射与非线性激活函数交织的运算。本节将讨论更多的算法设计内容，比如数据预处理、批量归一化、正则化和损失函数。 数据预处理均值减法（Mean Subtraction），即零中心化，是预处理最常用的形式。它对数据中的每个独立特征都减去平均值，从几何上可以理解为在每个维度上都将数据云的中心迁移到原点。 归一化（Normalization）是指将数据的所有维度都归一化，使其数值范围都近似相等。但在图像处理中，像素的数值范围几乎是一致的（都在0-255之间），所以进行这个额外的预处理步骤并不是很必要。 上面是一般的数据预处理流程： 得到原始的2维输入数据。 在每个维度上都减去平均值后得到零中心化的数据，现在数据云是以原点为中心的。 每个维度都除以其标准差来调整其每个维度上的数值范围。 PCA和白化：这是另一种预处理形式。在这种处理中，先对数据进行零中心化处理，然后计算协方差矩阵，它展示了数据中的相关性结构。在本笔记中提到PCA和白化是为了介绍的完整性，实际上卷积神经网络并不会采用这些变换。但是零中心化和归一化都是很重要的。 注意：任何预处理策略都只能在训练集数据之上进行计算，算法训练完毕后再应用到验证集或者测试集上。我们应该先把数据分成训练/验证/测试集，只是从训练集中求图片平均值，然后各个集（训练/验证/测试集）中的图像再减去这个平均值。 权重初始化在训练网络之前，我们需要初始化网络的参数。首先，使用全零初始化是错误的，因为如果每个神经元都计算出同样的输出，然后它们就会在反向传播中计算出同样的梯度，从而进行同样的参数更新。 小随机数初始化：因此，权重初始值应该要非常接近0，但又不能等于0。因此，我们将权重初始化为很小的数值，以此来打破对称性。其思路是：如果神经元初始时是随机且不相等的，那么它们将计算出不同的更新，并将自身变成整个网络的不同部分。 警告：并不是小数值一定会得到好的结果。例如，一个神经网络的层中权重值很小，那么在反向传播的时候就会计算出非常小的梯度（因为梯度和权重值是成比例的）。这就会很大程度上减小反向传播中的“梯度信号”，在深度网络中就会出现问题。 使用1/sqrt(n)校准方差：上面的做法存在一个问题，随着输入数据量的增长，随机初始化的神经元的输出数据的分布中的方差也在增大。我们可以除以数据量的平方根来调整其数值范围，这样神经元的方差就归一化到1了。也就是说，建议将神经元的权重向量初始化为：w = np.random.randn(n) / sqrt(n) 。这样就保证了网络中所有神经元起始时有近似同样的输出分布。实践经验表明，这样做可以提高收敛的速度。在最新的论文中，神经网络算法使用ReLU激活函数时的当前最佳推荐是将权重向量初始化为：w = np.random.randn(n) * sqrt(2.0/n)。 稀疏初始化：另一个初始化方法是将所有的权重矩阵设为0，但是为了打破对称性，每个神经元都同下一层固定数目的神经元随机连接。 偏置（biases）的初始化：通常将偏置初始化为0，这是因为小随机数初始化已经打破了对称性。 实践中的做法：当前的推荐是使用ReLU激活函数，并且使用w = np.random.randn(n) * sqrt(2.0/n)来进行权重初始化。 批量归一化（Batch Normalization）批量归一化是loffe和Szegedy最近才提出的方法，这个方法减轻了在初始化神经网络方面的困难。其做法是让激活数据在训练前通过一个网络，网络处理数据使其服从标准高斯分布。在实现层面，应用这个技巧通常意味着全连接层与激活函数之间添加了一个BN层。使用BN层的网络对于不好的初始值有更强的鲁棒性。 总结：批量归一化可以理解为在网络的每一层之前都做预处理，只是这种操作以另一种方式与网络集成在了一起。 正则化L2正则化：是最常用的正则化方法，通过惩罚目标函数中所有参数的平方得以实现。即对于网络中的每个权重$w$，向目标函数中增加一个$\frac12 \lambda w^2$，其中$\lambda$是正则化强度。L2正则化可以直观理解为对于大数值的权重向量进行严厉惩罚，倾向于更加分散的权重向量。由于输入与权重之间的乘法操作，这样就有了一个优良的特性：使网络更倾向于使用所有输入特征，而不是严重依赖输入特征中某些小部分特征。 L1正则化：是另一个常用的正则化方法，对于每个$w​$，我们都向目标函数增加一个$\lambda \left|w\right|​$。L1和L2也可以进行组合：$\lambda_1\left|w\right|+\lambda_2 w^2 ​$，这也被称作弹性网正则化。L1正则化有一个有趣的性质，它会让权重向量在最优化的过程中非常接近0。在实践中，如果不是特别关注某些明确的特征选择，一般来说L2正则化效果更好。 最大范式约束：另一种形式的正则化是给每个神经元中权重向量的量级设定上限，并使用投影梯度下降来确保这一约束。这种正则化即使在学习率设置过高的时候，在网络中也不会出现数值爆炸，这是因为它的参数更新始终是被限制着的。 随机失活（Dropout）：是一个简单又极其有效的正则化方法，与以上三种方法互为补充，实现方法是让神经元以超参数p的概率被激活。 使用随机失活可以被认为是对完整的神经网络抽样出一些子集，每次只更新子网络的参数。 不使用随机失活可以理解为是对数量巨大的子网络们做了模型集成，以此来计算出一个平均的预测。 实际中更倾向于使用反向随机失活。 前向传播中的噪音：随机失活属于网络在前向传播中有随机行为的方法。测试时，通过分析法（在使用随机失活的例子中就是乘以p）或数值法（通过抽样出很多子网络，随机选择不同的子网络进行前向传播，最后对它们取平均）将噪音边缘化。另一个相关的研究是DropConnect，它在前向传播的时候，一系列权重被随机设置为0。 偏置正则化：对于偏置的正则化并不常见，因为它们在矩阵乘法中和输入数据并不产生互动，所以并不需要控制其在数据维度上的效果。 实践中的做法：通过交叉验证获得一个全局使用的L2正则化强度是比较常见的，在使用L2正则化的同时在所有层后面使用随机失活也很常见，p值一般默认设为0.5，也可能在验证集上调参。 损失函数损失函数有两个部分，一个是已经讨论过的正则化部分，它可以看作是对模型复杂程度的某种惩罚。损失函数的第二个部分是数据损失，它是一个有监督学习问题，用于衡量分类算法的预测结果和真实标签结果之间的一致性，数据损失是对所有样本的数据损失求平均，即$L = \frac1N \sum_i L_i​$。 分类问题假设有一个装满样本的数据集，每个样本都有一个唯一的正确标签。在这类问题中，一个最常见的损失函数是SVM（Weston Watkin公式）： L_i = \sum_{j \neq y_i}max(0, f_j-f_{y_i}+1)第二个常用的损失函数是Softmax分类器，它使用交叉熵损失： L_i = -log(\frac{e^{f_{y_i}}}{\sum_j e^{f_j}})回归问题回归问题通常是预测实数的值的问题，比如预测房价。对于这种问题，通常是计算预测值和真实值之间的损失，然后用L2平方范式度量差异： L_i = \left|\left|f - y_i \right|\right|^2之所以在目标函数中要进行平方，是因为梯度算起来更加简单。因为平方是一个单调运算，所以不用改变最优参数。L1范式是要将每个维度上的绝对值加起来： L_i = \left|\left|f - y_i \right|\right| = \sum_j\left|f_j-(y_i)_j\right|注意：比起较为稳定的Softmax损失，L2损失的最优化过程要困难得多。 直观而言，它需要网络具备一个特别的性质，即对于每个输入（和增量）都要输出一个确切的正确值。 L2损失鲁棒性不好，因为异常值可以导致很大的梯度。 分类可以给出关于回归的输出的分布，而不是一个简单地毫无把握的输出值。 小结 推荐的预处理操作是对数据的每个特征都进行零中心化，然后将其数值范围都归一化到[-1, 1]范围之内。 使用标准差为$\sqrt \frac2n$的高斯分布来初始化权重，其中n是输入的神经元数。 使用正则化和反向随机失活。 使用批量归一化。 讨论了实践中可能要面对的不同任务，以及每个任务对应的常用损失函数。 参考：https://zhuanlan.zhihu.com/p/21930884]]></content>
      <categories>
        <category>cs231n</category>
      </categories>
      <tags>
        <tag>Computer Vision</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CHAPTER6 神经网络1]]></title>
    <url>%2Fposts%2F20573%2F</url>
    <content type="text"><![CDATA[快速简介在线性分类一节中，在给出图像的情况下，是使用$s = Wx$来计算不同视觉类别的评分。在使用数据库CIFAR-10的案例中，$x$是一个[3072x1]的列向量，$W$是一个[10x3072]的矩阵，所以输出的评分是一个包含10个分类评分的向量。 神经网络算法则不同，它的计算公式是$s = W_2max(0, W_1x)$，一个三层的神经网络可以类比地看作$s = W_3max(0, W_2max(0, W_1x))$，其中$W_1, W_2, W_3$是需要学习的参数。 单个神经元建模神经网络起源于生物神经系统，但随后成为了一个工程问题，并在机器学习领域取得了很好的效果。然而，讨论还是从对生物系统的一个高层次简略描述开始，毕竟神经网络是从这里得到了启发。 生物动机和连接在神经元的计算模型中，沿着轴突传播的信号（比如$x_0$）将基于突触的突触强度（比如$w_0$），与其他神经元的树突进行相乘（$w_0x_0$）。突触的强度（也就是权重$w$）是可学习的，且可以控制一个神经元对另一个神经元的影响强度。 树突将信号传递到细胞体，信号在细胞体中相加，如果最终之和高于某个阈值，那么神经元将会被激活，向其轴突输出一个峰值信号。由于历史原因，神经元的激活函数常常使用sigmoid函数，该函数将输入值压缩到0-1之间。 如上图所示，左边是生物神经元，右边是数学模型。 作为线性分类器的单个神经元只要在神经元的输出端有一个合适的损失函数，就能让单个神经元变成一个线性分类器。 二分类Softmax分类器。举例来说，可以把$\sigma(\sum w_ix_i+b)$看作是其中一个分类的概率$P(y_i=1|x_i;w)$，其他分类的概率为$P(y_i=0|x_i;w) = 1 - P(y_i=1|x_i;w)$。根据这种理解，也可以得到交叉熵损失，这在线性分类一节中已经介绍。 二分类SVM分类器。在神经元的输出外增加一个最大边界折叶损失（max-margin hinge loss）函数，将其训练成一个二分类的支持向量机。 理解正则化。正则化损失从生物学角度可以看作逐渐遗忘，因为它的效果是让所有突触权重$W$在参数更新过程中逐渐朝0变化。 常用激活函数每个激活函数的输入都是一个数字，然后对其进行某种固定的数学化操作。 Sigmoid。数学公式为$\sigma(x) =1/(1+e^{-x})​$，函数图像如上图左边所示。现在sigmoid函数很少使用，这是因为它有两个主要缺点： Sigmoid函数饱和使梯度消失。在神经元的激活在接近0或1处时会饱和，在这些区域，梯度几乎为0。回忆一下，在反向传播的时候，这个局部梯度将会与整个损失函数关于该门单元输出的梯度相乘。因此，如果局部梯度非常小，那么相乘的结果也会接近0，这样就几乎没有信号通过神经元传到权重再到数据了。还有，在初始化权重矩阵时要特别留意，防止初始化权重过大，否则网络就几乎不学习了。 Sigmoid函数的输出不是零中心的。神经网络后面层得到的数据将不是零中心的，这将会影响梯度下降的运作。因为如果输入神经元的数据总是正数，那么关于$w$的梯度在反向传播的过程中，将会全是正数，或全是负数。这将会导致梯度下降权重更新时出现z子型的下降。但是整个批量的数据加起来后，对于权重的最终更新就会有不同的正负，这样就一定程度减轻了这个问题。 Tanh。函数图像如上图右边所示。它也存在饱和问题，但是输出是零中心的。因此，tanh比sigmoid更受欢迎。注意，tanh是一个简单放大的神经元，具体说来：$tanh(x) = 2\sigma(2x) - 1​$。 ReLU。数学公式为$f(x) = max(0, x)$，近些年ReLU变得非常流行，具体有以下几条优缺点： 优点1：对于SGD的收敛有巨大的加速作用（Krizhevsky的论文指出使用ReLU比使用tanh的收敛快6倍）。 优点2：ReLU的运算比较简单。 缺点：在训练的时候，ReLU单元比较脆弱。例如，如果学习率设置得太高，可能会发现网络中40%的神经元都会死掉（在整个训练集中这些神经元都不会被激活）。 Leaky RuLU。为了解决ReLU的缺点，人们提出了Leaky ReLU。当x&lt;0时，Leaky ReLU给出一个很小的负数梯度值，比如0.01。 Maxout。一些其他单元被提了出来，数学公式为：$max(w_1^Tx+b_1, w_2^Tx+b_2)$。ReLU和Leaky ReLU都是这个公式的特殊情况，（比如ReLU就是当$w_1,b_1 = 0$的时候）。这样就解决了ReLU的缺点，但同时神经元的参数增加了一倍。 那么，我们到底应该使用哪种激活函数？答案是ReLU。设置好学习率，或许可以减轻神经元死亡的情况。如果单元死亡问题很严重，试试Leaky ReLU或Maxout，不要再用Sigmoid了。也可以试试tanh，但是效果应该不如ReLU或Maxout。 神经网络结构层组织 命名规则。上图左边是一个2层神经网络，隐藏层由4个神经元（也可称为单元）组成，输出层由2个神经元组成，输入层是3个神经元。右边是一个3层神经网络，两个含有4个神经元的隐藏层。当我们说N层神经网络的时候，没有把输入层算入。因此，单层的神经网络是没有隐层的。因此，有的研究者会说逻辑回归或SVM只是单层神经网络的一个特例。 网络尺寸。用来度量神经网络尺寸的标准主要有两个：神经元的个数、参数的个数。现代卷积神经网络能包含约1亿个参数，可由10-20层构成。有多少个神经元就有多少个偏置，以上图的两个网络举例： 第一个网络有4+2=6个神经元（输入层不算），[3×4]+[4×2]=20个权重，还有[4+2]=6个偏置，共26个可学习的参数。 第二个网络有4+4+1=9个神经元，[3×4]+[4×4]+[4×1]=32个权重，4+4+1=9个偏置，共41个可学习的参数。 前向传播计算例子将神经网络组织成层状的一个主要原因，就是这个结构让神经网络算法使用矩阵向量操作变得简单和有效。用一个三层神经网络举例，输入是[3×1]的向量。一个层所有连接的强度可以存在一个单独的矩阵中，如第一个隐藏层的权重$W_1$是[4×3]，所有单元的偏置存储在$b_1$中，尺寸[4×1]。这样，每个神经元的权重都在$W_1$的一个行中，于是矩阵乘法$np.dot(W_1, x)$就能够计算该层所有神经元的激活数据。类似的，$W_2$将会是[4×4]矩阵，存储着第二个隐藏层的连接，$W_3$是[1×4]的矩阵，用于输出层。完整的3层神经网络的前向传播就是简单的3次矩阵乘法，其中交织着激活函数的应用。 123456# 一个3层神经网络的前向传播：f = lambda X: 1.0/(1.0+np.exp(-x)) # 激活函数（用的sigmoid）x = np.random.randn(3, 1) # 含3个数字的随机输入向量(3×1)h1 = f(np.dot(W1, x) + b1) # 计算第一个隐藏层的激活数据(4×1)h2 = f(np.dot(w2, h1) + b2) # 计算第二个隐藏层的激活数据(4×1)out = np.dot(W3, h2) + b3 # 神经元输出(1×1) 注意，x并不是一个单独的列向量，而是一个可批量训练的数据（其中每个输入样本将会是$x$中的一列），所有的样本将会被并行化地高效计算出来。 表达能力理解具有全连接层的神经网络的一个方式是，可以认为它们定义了一个由一系列函数组成的函数簇，网络的权重就是每个函数的参数。那么，这个函数簇的表达能力如何？ 研究已经证明，对于给出的任意函数$f(x)$和任意$\epsilon&gt;0$，均存在一个至少含1个隐藏层的神经网络$g(x)$，对于$\forall x$，使得$\left|f(x) - g(x)\right| &lt; \epsilon$。换句话说，神经网络可以近似任何连续函数。 既然含1个隐藏层的神经网络就可以近似任何函数，那为什么还要构建更多层来将网络做得更深？ 答案是：虽然一个2层网络在数学理论上能够完美近似任何连续函数，但是在实际操作中效果很差。深度神经网络在实践中非常好用，是因为它们表达出的函数不仅平滑，而且对于数据的统计特性有很好的拟合。同时，网络通过最优化算法能比较容易地学习到这个函数。因此，虽然在理论上深层网络和单层网络的表达能力是一样的，但是就实践经验而言，深层网络的效果更好。 在全连接神经网络中，3层的网络比2层的网络表现好，然而继续加深（做到4，5，6层）很好有太大帮助。但在卷积神经网络中，深度是一个极其重要的因素。对于该现象的一种观点是：由于图像具有多层次结构（比如脸是由眼睛等组成，眼睛又是由边缘组成），所以多层处理对于这种数据就有直观意义（这里我理解为每层网络都提取特定的特征）。 设置层的数量和尺寸当我们增加层的数量和尺寸时，网络的容量上升了。即神经元们可以合作表达许多复杂函数，所以表达函数的空间增加。例如，如果有一个在二维平面上的二分类问题，我们可以训练3个不同的神经网络，每个网络都只有一个隐藏层，但是每层的神经元数目不同： 在上图中，我们看出更多神经元能够表达更复杂的函数。但这既是优势也是不足，优势是可以分类更复杂的数据，不足是可能造成对训练数据的过拟合。 那么，如果数据不是足够复杂，似乎小一点的网络更好，因为可以防止过拟合。然而并非如此，防止神经网络的过拟合有很多种方法（L2正则化，dropout和输入噪音等），后面会详细讨论。在实践中，使用这些方法来控制过拟合比减少网络神经元数目更好。 不要减少神经元数目的主要原因是小网络更难使用梯度下降等局部方法进行训练： 虽然小型网络的损失函数的局部最小值更少，也比较容易收敛到这些局部最小值，但是这些最小值一般都很差，损失值很高。 相反，大网络具有更多的局部最小值，但是这些局部最小值的损失更小。 在实际中，你将发现如果你训练的是一个小网络，那么在运气好的情况下，损失值会收敛到一个好的地方，运气不好的话就会收敛到一个不好的极值。如果训练的是大网络，你将会发现许多不同的解决方法。 总之，应该尽可能使用大网络，然后使用正则化技巧来控制过拟合。 小结 介绍了生物神经元的粗略模型。 讨论了几种不同类型的激活函数，其中ReLU是最佳推荐。 介绍了神经网络，神经元通过全连接层连接，层间神经元两两相连，但是层内神经元不连接。 理解了分层的结构能够让神经网络高效地进行矩阵乘法和激活函数运算。 理解了神经网络是一个通用函数近似器。 讨论了更大网络总是更好的这一事实，大容量的模型一定要和更强的正则化配合，以免出现过拟合。后面我们会学习更多的正则化的方法，尤其是dropout。 参考：https://zhuanlan.zhihu.com/p/21930884]]></content>
      <categories>
        <category>cs231n</category>
      </categories>
      <tags>
        <tag>Computer Vision</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CHAPTER5 卷积神经网络]]></title>
    <url>%2Fposts%2F37575%2F</url>
    <content type="text"><![CDATA[卷积神经网络依然是一个可导的评分函数：输入是原始的图像像素，输出是不同类别的评分。在最后一层（通常是全连接层），网络依旧有一个损失函数（比如SVM或Softmax）。在介绍CNN之前，先提出三个观点，正是这三个观点使得CNN能够真正起作用，它们分别对应着CNN中的三种思想。 局部性。对于一张图片而言，需要检测图片中的特征来决定图片的类别，通常情况下这些特征都不是由整张图片决定的，而是由一些局部的区域决定的。 相同性。对于不同的图片，如果它们具有出现在不同位置的相同的特征，也同样可以使用相同的检测模式去检测。 不变性。对于一张大图片，如果我们进行下采样，图片的性质基本保持不变。 结构概述普通神经网络对于大尺寸图像效果不是很好。比如在MNIST数据集中，图像的大小是28×28=784，由于这是一张小的灰度图，所以看起来不是很大。若图像尺寸为200×200×3，会让卷积层中每个神经元包含200×200×3=120000个权重值，这种全连接方式效率低下。 卷积神经网络的各层中的神经元是3维排列的，层中的神经元只与前一层中的一小块区域连接，而不是采取全连接方式。对于用来分类CIFAR-10的卷积网络来说，最后的输出层的维度是1×1×10，因为CNN的最后部分会将全尺寸的图像压缩为包含分类评分的一个在深度方向排列的向量，下图是例子： 左边是一个3层的神经网络，右边是一个卷积神经网络。 CNN中的每个层使用一个可以微分的函数将激活数据从一个层传递到另一个层，主要由三种类型的层构成：卷积层，池化层和全连接层（全连接层和普通神经网络中的一样）。 一个用于CIFAR-10图像分类的CNN的结构可以是【输入层 - 卷积层 - ReLU层 - 池化层 - 全连接层】： 假设输入层的输入图像原始像素为[32×32×3]。 卷积层中，每个神经元都与输入层的一个局部相连，每个神经元都计算这个局部与自己权重的内积。如果使用12个滤波器，得到的维度是[32×32×12]。 ReLU层逐个元素进行激活函数操作，比如使用$max(0, x)$作为激活函数，该层对数据的尺寸没有改变，还是[32×32×12]。 池化层在空间维度（宽度和高度）上进行降采样(downsampling)操作，数据尺寸变为[16×16×12]。 全连接层将会计算分类评分，数据尺寸变为[1×1×10]，10个数字对应CIFAR-10中10个类别的分类评分值。全连接层和普通神经网络一样，每个神经元都与前一层中所有的神经元相连接。 由此看来，CNN一层一层地将图像从原始像素变换为最终的分类评分值。其中有的层含有参数，而有的层没有。具体地说，卷积层和全连接层不仅会用到激活函数，还会用到一些参数，这些参数会随着梯度下降被训练。ReLU层和池化层则是进行一个固定不变的函数操作。 这个例子中的结构是一个小型的VGG网络。 用来构建卷积神经网络的各种层卷积层概述和直观介绍卷积层是CNN的核心层，网络中的大部分计算量从中产生。卷积层的参数是由一些可学习的滤波器集合构成的，每个滤波器在空间上（宽度和高度）都比较小，但是深度和输入数据的深度保持一致。当滤波器沿着输入数据的宽度和高度滑过后，会生成一个2维的激活图(activation map)，激活图上的每个空间位置表示了原图片对于该滤波器的反应。直观地说，在滤波器学习到某些视觉特征后，网络就会让其激活。 在每个卷积层上，我们会有一整个集合的滤波器，比如20个，这样就会生成20个不同的二维激活图，将这些激活图在深度方向上层叠起来就形成了卷积层的输出。 Krizhevsky构架赢得了2012年的ImageNet挑战，其输入图像的尺寸是[227×227×3]。在第一个卷积层，神经元使用的感受野尺寸F=11，步长S=4，零填充P=0。因为$(227-11)/4+1=55$，卷积层的深度K=96，则卷积层的输出数据体尺寸是[55×55×96]。在这55×55×96个神经元中，每个神经元都和输入数据体中一个[11×11×3]的区域连接，但是权重不同。 局部连接在处理图像这样的高维度输入时，想要让每个神经元与前一层中所有神经元进行全连接是不现实的。相反，我们让每个神经元只与输入数据的一个局部区域连接，该连接的空间大小叫做神经元的感受野(receptive field)，它的尺寸是一个超参数（滤波器的宽和高），在深度方向上，该连接的大小总是和输入数据的深度保持一致。 感受野的推导是一个递归的过程： RF_n = RF_{n-1} + (K_n-1)×S_n其中$K_n$是第n层卷积核的大小，$S$是第n层的滑动步长。 左边：红色的是输入数据体，蓝色的部分是第一个卷积层中的神经元。卷积层中的每个神经元都只是与输入数据体的一个局部在空间上相连。深度列上的每个神经元都是与输入数据体中同一个感受野连接，但是权重不同。举例说明，比如输入数据的尺寸为32×32×3，如果感受野（滤波器尺寸）是5×5，卷积层中每个神经元与输入数据之间就有5×5×3=75个连接。这里再次强调感受野深度的大小必须是3，和输入数据保持一致。 右边：神经元还是计算权重和输入的内积，然后进行激活函数运算，只是它们的连接被限制在一个局部空间。 空间排列首先，卷积层的输出深度是一个超参数K，它与使用的滤波器数量一致，每种滤波器都在输入数据中寻找一种特征。比如说输入一张原始图片，卷积层输出的深度是20，这说明有20个滤波器对数据进行处理，每种滤波器寻找一种特征进行激活。 其次，在滑动滤波器的时候，必须指定步长S，表示滤波器每次移动S个像素点。 最后，可以将输入数据在边界上用0进行填充，边界填充的数量P也作为一个超参数，可以用来控制输出数据尺寸的大小。 通过对卷积过程的计算，我们可以总结出一个通用公式： W_{output} = \frac{W_{input} - W_{filter} + 2P}S + 1 H_{output} = \frac{H_{input} - H_{filter} + 2P}S + 1其中$W$和$H$分别表示图像的宽度和高度。 零填充的使用输出数据体可以通过上面的公式来计算，假设输入数组的空间形状是正方形，那么输出数据体的空间尺寸为$(W-F+2P)/F+1$。一般来说，当步长S=1时，零填充的值是$P=(F-1)/2$，这样就能保证输入和输出数据体有相同的空间尺寸。 步长的限制通过上面的公式我们知道步长的选择是有所限制的，举例来说，当输入尺寸$W=10$的时候，如果不使用零填充，即$P=0$，滤波器尺寸$F=3$，这样步长$S=2$就行不通，因为$(10-3+0)/2+1=4.5$，结果不是一个整数，说明神经元不能整齐对称地滑过输入数据体。 参数共享在卷积层中使用参数共享是为了控制参数的数量，这样之所以行得通，是因为之前介绍的相同性。比如说一个卷积层的输出是20×20×32，那么神经元的个数就是20×20×32=12800如果感受野的大小是3×3，而输入数据体的深度是10，那么每个神经元就有3×3×10=900个参数，这样卷积层总共就有12800×900=11520000个参数，单单一层卷积就有这么多参数，这样运算速度显然是特别慢的。 根据之前介绍的，一个滤波器检测出一个空间位置(x1, y1)处的特征，那么也能够检测出(x2, y2)位置的特征，所以就可以用相同的滤波器来检测相同的特征，基于这个假设，我们就能够有效减少参数个数。比如上面这个例子，一共有32个滤波器，这使得输出体的厚度是32，每个滤波器的参数为3×3×10=900，总共的参数就有32×900=28800个，极大减少了参数的个数。 将深度维度上的一个单独的2维切片看作是深度切片(depth slice)，由参数共享我们知道输出体数据在深度切片上所有的权重都使用同一个权重向量。如一个数据体尺寸为[55×55×96]的就有96个深度切片，每个尺寸为[55×55]。在每个深度切片上的神经元都使用同样的权重和偏差。在这样的参数共享下，一个权重集对应一个深度切片。在每个深度切片中55×55个权重使用的都是同样的参数。 如果在一个深度切片中的所有权重都使用同一个权重向量，那么卷积层的前向传播中，每个深度切片都可以看作是计算神经元权重对输入数据体的卷积（这就是“卷积层”名字的由来），这也是为什么总是将这些权重集合称为滤波器(filter)，或卷积核(kernel)。 需要注意的是，参数共享之所以有效，是因为一个特征在不同位置的表现是相同的。比如一个滤波器检测到了水平边界这个特征，如果这个特征具有平移不变性，那么在其他位置也能检测出来。但是有时候参数共享可能没意义，特别是当卷积神经网络的输入图像是一些明确的中心结构的时候。一个具体的例子就是人脸识别，人脸一般位于图片的中心，我们希望不同的特征能够在不同的位置被学习到，比如眼睛特征或者头发特征，正是由于这些特征在不同的地方，才能够对人脸进行识别。 总结最后总结一下卷积层的一些性质。 输入数据体的尺寸是$W_1 ×H_1×D_1$ 4个超参数： 滤波器数量$K$ 滤波器空间尺寸$F$ 滑动步长$S$ 零填充的数量$P$ 输出数据体的尺寸为$W_2×H_2×D_2$，其中： W_2 = \frac{W_1 - F + 2P}S + 1 H_2 = \frac{H_1 - F + 2P}S + 1 D_2 = K 由于参数共享，每个滤波器包含的权重数目为$F×F×D_1$，卷积层一共有$F×F×D_1×K$个权重和$K$个偏置。 在输出数据体中，第$d$个深度切片（空间尺寸是$W_2×H_2$），用第$d$个滤波器和输入数据进行有效卷积运算的结果，再加上第$d$个偏置。 池化层我们通常会在卷积层之间周期性插入一个池化层，其作用是逐渐降低数据体的空间尺寸，这样就可以： 减少网络中参数的数量 减少计算资源耗费 有效控制过拟合 池化层和卷积层一样也有一个空间窗口，通常采用的是取这些窗口中的最大值作为输出结果，然后不断滑动窗口，对输入数据体每一个深度切片单独处理，如下图所示。 左边：池化层能够有效降低数据体空间的大小 右边：形象地说明了窗口大小是2，滑动步长是2的最大值池化是如何计算的 池化层之所以有效，是因为之前介绍的图片特征具有不变性，也就是通过下采样不会丢失图片拥有的特征。由于这种特性存在，我们可以将图片缩小再进行卷积，这样能够大大降低卷积运算的时间。 最常用的池化层形式是尺寸为2×2的窗口，滑动步长为2，对图像进行下采样，将其中75%的激活信息都丢掉，选择其中最大的保留下来，这其实是因为我们希望能够更加激活里面的数值大的特征，去除一些噪声信息。 池化层有一些和卷积层类似的性质： 输入数据体的尺寸是$W_1 ×H_1×D_1$ 2个超参数： 池化窗口空间尺寸$F$ 滑动步长$S$ 输出数据体的尺寸为$W_2×H_2×D_2$，其中： W_2 = \frac{W_1 - F}S + 1 H_2 = \frac{H_1 - F}S + 1 D_2 = D_1 池化层很少引入零填充 一般来说，应该谨慎使用比较大的池化窗口，以免对网络有破坏性。除了最大值池化之外，还有一些其他的池化函数，比如平均池化，或者L2范数池化。在实际中证明，在卷积层之间引入最大池化的效果是最好的，而平均池化一般放在卷积神经网络的最后一层。 有发现认为，在训练一个良好的生成模型时，弃用池化层也是很重要的，比如VAEs和GANs。现在看起来，未来的卷积网络结构中，可能会很少甚至不实用池化层。 全连接层全连接层和之前介绍的一般的神经网络的结构是一样的，每个神经元与前一层所有的神经元连接，而卷积神经网络只和输入数据中的一个局部区域连接，并且输出的神经元每个深度切片共享参数。 在这个过程中为了防止过拟合会引入Dropout。最近的研究表明，在进入全连接层之前，使用全局平均池化能够有效地降低过拟合。 卷积神经网络的结构层的排列规律卷积神经网络最常见的形式就是将一些卷积层和ReLU层放在一起，其后紧跟池化层，然后重复如此直到图像在空间中被缩小到一个足够小的尺寸，在某个地方过渡成全连接层也较为常见。最后的全连接层得到输出。换句话说，最常见的卷积神经网络结构如下： INPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?]M -&gt; [FC -&gt; RELU]K -&gt; FC 其中*指的是重复次数，POOL?指的是一个可选的池化层。其中N&gt;=0，通常N&lt;=3，M&gt;=0，K&gt;=0，通常K&lt;3。 一般而言，几个小滤波器卷积层的组合要比一个大滤波器卷积层要好。比如层层堆叠了3个3×3的卷积层，中间含有非线性激活层。在这种排列下面，第一个卷积层中每个神经元对输入数据的感受野是3×3，由于第二层卷积层对第一层卷积层的感受野也是3×3，这样第二层对于输入数据的感受野就是3+(3-1)×1=5，即5×5。同样，第三层卷积层对第二层卷积层的感受野是3×3，这样第三层对于第一层输入数据的感受野就是5+(3-1)×1=7，即7×7。 如果不使用3个3×3的感受野，而是直接单独使用一个7×7大小的卷积层，那么所有神经元的感受野也是7×7。总体说来，使用小滤波器的卷积组合的优点有： 多个卷积层与非线性激活层交替的结构，比单一卷积层的结构更能提取出深层的特征 假设输入、输出数据体的深度都是$C$，那么单独的7×7卷积层会有$7×7×C×C=49C^2$的参数个数，而使用3个3×3的卷积层的组合，仅仅含有$3×(3×3×C×C)=27C^2$的参数。 唯一不足的是反向传播更新参数时，中间的卷积层可能会占用更多的内存。 层的尺寸设置规律对于卷积神经网络的尺寸设计，没有严格的数学证明，这是根据经验制定出来的规则： 输入层：一般而言，输入层的大小应该能够被2整除很多次，常用的数字有32, 64, 96和224。 卷积层：应该使用尽可能小的滤波器，比如3×3或者5×5，滑动步长取1。如果必须使用更大的滤波器尺寸，比如7×7，通常用在第一个面对原始图像的卷积层上。 池化层：负责对输入的数据空间维度进行下采样，常用的设置是用2×2的感受野做最大值池化，滑动步长取2。另外一个不常用的设置是使用3×3的感受野，步长设置为2。一般而言，池化层的感受野很少超过3，因为这样会使得池化过程过于激烈，造成信息的丢失。 零填充：零填充的使用可以让卷积层的输入和输出在空间上的维度保持一致。 案例学习下面是卷积神经网络领域比较有名的几种结构： LeNet：第一个成功的卷积神经网络应用，是Yann LeCun在上世纪90年代实现的，它最著名的应用是识别数字和邮政编码。 AlexNet：AlexNet卷积神经网络在计算机视觉领域中受到欢迎，它由Alex Krizhevsky，Ilya Sutskever和Geoff Hinton实现。AlexNet在2012年的ImageNet ILSVRC 竞赛中夺冠，性能远远超出第二名（16%的top5错误率，第二名是26%的top5错误率）。这个网络的结构和LeNet非常类似，但是更深更大，并且使用了层叠的卷积层来获取特征（之前通常是只用一个卷积层并且在其后马上跟着一个汇聚层）。 ZFNet：Matthew Zeiler和Rob Fergus发明的网络在ILSVRC 2013比赛中夺冠，它被称为ZFNet（Zeiler &amp; Fergus Net的简称）。它通过修改结构中的超参数来实现对AlexNet的改良，具体说来就是增加了中间卷积层的尺寸，让第一层的步长和滤波器尺寸更小。 GoogLeNet：ILSVRC 2014的胜利者是谷歌的Szeged等实现的卷积神经网络。它主要的贡献就是实现了一个奠基模块，它能够显著地减少网络中参数的数量（AlexNet中有60M，该网络中只有4M）。还有，这个论文中没有使用卷积神经网络顶部使用全连接层，而是使用了一个平均池化，把大量不是很重要的参数都去除掉了。 VGGNet：ILSVRC 2014的第二名是Karen Simonyan和 Andrew Zisserman实现的卷积神经网络，现在称其为VGGNet。它主要的贡献是展示出网络的深度是算法优良性能的关键部分。他们最好的网络包含了16个卷积/全连接层。网络的结构非常一致，从头到尾全部使用的是3x3的卷积和2x2的汇聚。他们的预训练模型是可以在网络上获得并在Caffe中使用的。VGGNet不好的一点是它耗费更多计算资源，并且使用了更多的参数，导致更多的内存占用（140M）。其中绝大多数的参数都是来自于第一个全连接层。后来发现这些全连接层即使被去除，对于性能也没有什么影响，这样就显著降低了参数数量。 ResNet：残差网络（Residual Network）是ILSVRC2015的胜利者，由何恺明等实现。它使用了特殊的跳跃链接，大量使用了批量归一化（batch normalization），这个结构同样在最后没有使用全连接层。ResNet当前最好的卷积神经网络模型（2016年五月）。 参考：https://zhuanlan.zhihu.com/p/21930884]]></content>
      <categories>
        <category>cs231n</category>
      </categories>
      <tags>
        <tag>Computer Vision</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CHAPTER4 反向传播]]></title>
    <url>%2Fposts%2F39784%2F</url>
    <content type="text"><![CDATA[简介反向传播是利用链式法则递归计算表达式的梯度的方法，理解反向传播的精妙之处，对于理解、实现、设计和调试神经网络非常关键。 问题陈述：这节的核心问题是：给定函数$f(x)$，其中$x$是输入数据的向量，需要计算函数$f$关于$x$的梯度，也就是$\nabla f(x)$。 使用链式法则计算复合表达式考虑包含多个函数的复杂函数，如$f(x,y,z) = (x+y)z$。将这个公式分成两部分：$q=x+y$和$f=qz$。链式法则指出将这些梯度表达式链接起来的正确方式是相乘，比如$\frac{\partial f}{\partial x} = \frac{\partial f}{\partial q}\frac{\partial q}{\partial x}$。在实际操作中，这只是简单地将两个梯度数值相乘，示例代码如下： 1234567891011121314# 设置输入值x = -2; y = 5; z = -4# 进行前向传播q = x + y # q becomes 3f = q * z # f becomes -12# 进行反向传播:# 首先回传到 f = q * zdfdz = q # df/dz = q, 所以关于z的梯度是3dfdq = z # df/dq = z, 所以关于q的梯度是-4# 现在回传到q = x + ydfdx = 1.0 * dfdq # dq/dx = 1. 这里的乘法是因为链式法则dfdy = 1.0 * dfdq # dq/dy = 1 这是最简单的一个反向传播，最终得到变量的梯度[dfdx, dfdy, dfdz]。以后我们会用更简洁的表达符号，这样就不用写df了，如用dq来代替dfdq。 上图展示了前向传播和反向传播的过程，绿色代表前向传播，红色代表反向传播。 反向传播的直观理解反向传播是一个优美的局部过程。在整个计算线路图中，每个门单元都会得到一些输入并立即计算两个东西： 这个门的输出值 其输出值关于输入值的局部梯度 门单元完成这两件事是完全独立的，它不需要知道计算线路中的其他细节。链式法则指出，门单元应该将回传的梯度乘上它的局部梯度，从而得到整个网络的输出对该门单元的每个输入值的梯度。 看一个模块化的Sigmoid例子，它的表达式是$f(w, x) = \frac1{1+e^{-(w_0x_0+w_1x_1+w_2)}}$，这个表达式描述了一个含输入x和权重w的2维的神经元，该神经元使用了Sigmoid函数，整个计算线路如下： 因此，反向传播可以看作是门单元直接在通过梯度信号相互通信。 反向传播实践：分段计算假如有如下函数： f(x, y) = \frac{x + \sigma(y)}{\sigma(x) + (x+y)^2}如果对$x$或$y$进行微分运算，运算结束后会得到一个巨大而复杂的表达式，然而这是没有必要的，因为我们不需要一个明确的函数来计算梯度，只需知道如何使用反向传播计算梯度即可。下面是构建前向传播的代码： 123456789101112x = 3 # 例子数值y = -4# 前向传播sigy = 1.0 / (1 + math.exp(-y)) # 分子中的sigmoid #(1)num = x + sigy # 分子 #(2)sigx = 1.0 / (1 + math.exp(-x)) # 分母中的sigmoid #(3)xpy = x + y #(4)xpysqr = xpy**2 #(5)den = sigx + xpysqr # 分母 #(6)invden = 1.0 / den #(7)f = num * invden # 搞定！ #(8) 在构建代码时我们创建了多个中间变量，每个都是比较简单地表达式，它们计算局部梯度的方法是已知的。这样计算反向传播就简单了：我们对前向传播时产生的每个变量(sigy, num, sigx, xpy, xpysqr, den, invden)进行回传。我们会有同样数量以d开头的变量，用来存储对应变量的梯度。 注意： 在反向传播的每一小块中都将包含了表达式的局部梯度，然后根据链式法则乘以上游梯度。 以下的代码都简写了梯度的名称，如用dnum代替了dfdnum，用dinvden代替了dfdinvden。 1234567891011121314151617181920# 回传 f = num * invdendnum = invden # 分子的梯度 #(8)dinvden = num #(8)# 回传 invden = 1.0 / den dden = (-1.0 / (den**2)) * dinvden #(7)# 回传 den = sigx + xpysqrdsigx = (1) * dden #(6)dxpysqr = (1) * dden #(6)# 回传 xpysqr = xpy**2dxpy = (2 * xpy) * dxpysqr #(5)# 回传 xpy = x + ydx = (1) * dxpy #(4)dy = (1) * dxpy #(4)# 回传 sigx = 1.0 / (1 + math.exp(-x))dx += ((1 - sigx) * sigx) * dsigx # Notice += !! See notes below #(3)# 回传 num = x + sigydx += (1) * dnum #(2)dsigy = (1) * dnum #(2)# 回传 sigy = 1.0 / (1 + math.exp(-y))dy += ((1 - sigy) * sigy) * dsigy #(1) 注意： 对前向传播变量进行缓存：在计算反向传播时，前向传播过程中得到的一些中间变量非常有用。 在不同分支的梯度要相加：如果变量x，y在前向传播的表达式中走向不同，那么进行反向传播的时候需要使用+=而不是=来累计这些变量的梯度。这是遵循了微积分中的多元链式法则。 回传流中的模式神经网络中最常用的加法、乘法和取最大值这三个门单元在反向传播的行为都有非常简单的解释，看下面这个例子： 如图所示，加法操作将梯度相等地分发给它的输入。取最大操作将梯度路由给更大的输入。乘法门拿取输入激活数据，对它们进行交换，然后乘以梯度。 用向量化操作计算梯度上述内容考虑的都是单个变量情况，但所有的概念都适用于矩阵和向量操作。 有些读者可能觉得向量化操作的梯度计算比较困难，建议是写出一个很小很明确的向量化例子，在纸上演算梯度，然后对其一般化，得到一个高效的向量化操作形式。 小结 本节直观理解了梯度的含义，知道了梯度是如何在网络中反向传播的。 讨论了分段计算在反向传播的实现中的重要性。应该将函数分成不同的模块，这样计算局部梯度相对容易。重要的是，只需要将表达式分成不同的可以求导的模块，然后在反向传播中一步一步地计算梯度。 参考：https://zhuanlan.zhihu.com/p/21930884]]></content>
      <categories>
        <category>cs231n</category>
      </categories>
      <tags>
        <tag>Computer Vision</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CHAPTER3 最优化]]></title>
    <url>%2Fposts%2F12650%2F</url>
    <content type="text"><![CDATA[简介图像分类任务有三个关键部分，分别是评分函数、损失函数和最优化。在上一节中，我们介绍了图像分类任务中的两个关键部分： 基于参数的评分函数，将原始图像像素映射为分类评分值。 损失函数，用于衡量某个具体参数集的好坏。 现在介绍最后一个关键部分：最优化Optimization，最优化是寻找能使损失函数值最小化的参数的过程。 铺垫：一旦理解了这三个部分，我们将会回到第一个部分（基于参数的函数映射），然后将其拓展为一个远比线性函数复杂的函数：首先是神经网络，然后是卷积神经网络。而损失函数和最优化这两个部分将会保持相对稳定。 最优化当前使用的例子(SVM损失函数)是一个凸函数问题，但我们的目标不仅仅是对凸函数做最优化，而是能够最优化一个神经网络，而对神经网络是不能简单的使用凸函数的最优化技巧的。 策略#1：一个差劲的初始方案：随机搜索验证集上最好的$W$跑测试集的准确率是15.5%，至少比随机猜的准确率10%高。 核心思路：迭代优化。我们的方法从一个随机的$W$开始，然后对其迭代取优，每次都让它的损失值变得更小一些。 策略#2：随机局部搜索这次我们从一个随机$W$开始，然后生成一个随机的扰动$\delta W$，只有当$W+\delta W$的损失值变低的时候才更新。 验证集上最好的$W$跑测试集的准确率是21.4%，比随机搜索稍微好一些，但是依然过于浪费计算资源。 策略#3：计算梯度的方向其实不需要随机寻找方向，因为可以直接计算出最好的方向，这个方向就是损失函数的梯度(gradient)。 当函数有多个参数时，我们称导数为偏导数。而梯度就是在每个维度上偏导数所形成的向量。 梯度计算计算梯度有两个方法： 数值梯度法：速度缓慢，但实现相对简单。 分析梯度法：计算迅速，结果精确，但实现时容易出错，且需要使用微分。 利用有限差值计算梯度实际上是利用导数的定义计算导数，从而计算梯度。实际中用中心差值公式$[f(x+h) - f(x - h)] / 2h​$的效果较好。 计算数值梯度的复杂性和参数数量线性相关。在本例中有30730个参数，所以损失函数每走一步就需要计算30731次损失函数的梯度。显然这个策略不适合大规模数据，我们需要更好地策略。 微分分析计算梯度为了解决实现时容易出错的问题，在实际操作中常常将分析梯度法的结果和数值梯度法的结果作比较，以此来检查其实现的正确性，这个步骤叫做梯度检查。 用SVM的损失函数在某个数据点上的计算来举例： L_i = \sum_{j \neq y_i} [max(0, w^T_jx_i - w^T_{y_i}x_i + \Delta)]可以对函数进行微分。比如，对$w_{y_i}$进行微分得到： \nabla_{w_{y_i}}L_i = -(\sum_{j \neq y_i}1(w^T_jx_i - w^T_{y_i}x_i + \Delta > 0))x_i注：原公式中1为空心字体，为示性函数，括号中为真则函数值为1，否则为0 一旦将梯度的公式微分出来，代码实现公式并用于梯度更新就比较顺畅了。 梯度下降小批量梯度下降(Mini-batch gradient descent)：在大规模的应用中，如果训练整个数据集来获得仅仅一个参数的更新就太浪费了，一个常用的方法是计算训练集中的小批量数据。例如，在目前最高水平的卷积神经网络中，一个典型的小批量包含256个例子，而整个训练集是120万个例子。 随机梯度下降(Stochastic Gradient Descent)：也被称为在线梯度下降，在实际情况中相对少见。是小批量梯度下降的极端情况，即每个批量中只有1个数据样本。 小结 将损失函数比作了一个高维度的最优化地形，并尝试到达它的最底部。 提出了迭代优化的思想，从一个随机的权重开始，然后一步步地将损失值变到最小。 梯度给出了该函数最陡峭的上升方向。 参数更新需要有技巧地设置步长，也叫学习率。 实际中通常使用分析梯度法，然后使用数值梯度法来检查正确与否。 参考：https://zhuanlan.zhihu.com/p/21930884]]></content>
      <categories>
        <category>cs231n</category>
      </categories>
      <tags>
        <tag>Computer Vision</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CHAPTER2 线性分类]]></title>
    <url>%2Fposts%2F16361%2F</url>
    <content type="text"><![CDATA[线性分类器简介概述：我们将使用一种更强大的方法来解决图像分类问题，该方法自然地延伸到神经网络和卷积神经网络上。这种方法主要由两部分组成： 评分函数(score function)，它是原始图像数据到类别分值的映射。 损失函数(loss function)，它用来量化预测标签与真实标签之间的一致性。 此方法可转化为一个最优化问题，在最优化过程中，将通过更新评分函数的参数来最小化损失函数值。 线性评分函数（从图像到标签分值的参数化映射）该方法的第一部分就是定义一个评分函数。假设有一个包含很多图像的训练集$x_i \in R^D$，每个图像都有一个对应的分类标签$y_i$ ，这里$i=1,2,…,N$并且$y_i \in 1…K$。这就是说，我们有$N$个图像样例，每个图像的维度是$D$，共有$K​$个不同的分类。 我们定义评分函数为：$f = R^D \rightarrow R^K$，该函数是原始图像像素到分类分值的映射。 线性分类器：在本模型中，我们从最简单的概率函数开始，一个线性映射： f(x_i, W, b) = Wx_i + b需要注意的几点： 首先，一个单独的矩阵乘法$Wx_i$就高效地并行评估10个不同的分类器，W的每一行都是一个分类类别的分类器。 此方法的优势是训练数据是用来学习参数W和b的，一旦训练完成，训练数据就可以丢弃。 只需做一个矩阵乘法和一个矩阵加法就能对一个测试数据分类，这比kNN中将测试图像和所有训练数据做比较的方法快多了。 CNN映射图像像素值到分类分值的方法和上面一样，但是映射f就要复杂多了，包含的参数也最多。 理解线性分类器举一个图像映射到分类分值的例子。为了便于可视化，假设图像只有4个像素（都是黑白像素，不考虑RGB通道），有3个分类分别代表猫、狗、船。需要注意的是，这个W一点也不好。 将图像看做高维度的点：既然图像被伸展成为了一个高维度的列向量，那么我们可以把图像看作是这个高维度空间中的一个点（即每张图像是3072维空间中的一个点）。 将线性分类器看作模版匹配：关于权W的另一个解释是它的每一行对应着一个分类的模版。一张图像对应不同分类的得分，是通过内积来比较图像和模板，然后找到和哪个模板最相似。 偏差和权重的合并技巧：这是一个经常使用的技巧，将参数$W$和$b​$合二为一。即在权重矩阵中添加一个偏差列，然后在输入向量的维度中增加一个含常数1的维度。 图像数据预处理：在上面的例子中，所有图像都是使用的原始像素值（从0到255），因此不需做归一化处理。但在机器学习中，对于输入的特征做归一化处理是常见的套路。 零均值的中心化：这是十分重要的，等我们理解了梯度下降后再来详细解释。 损失函数(Loss Function)多类SVM损失函数(Loss Function)：有时也叫代价函数Cost Function或目标函数Objective Function ，来衡量我们对结果的不满意程度。对训练集数据做出良好预测与得到一个足够低的损失值，这两件事是等价的。 多类支持向量机损失(Multiclass Support Vector Machine Loss)：SVM的损失函数要求SVM在正确分类上的得分始终比不正确分类上的得分高出一个边界值 $\Delta​$。针对第i个数据的多类SVM的损失函数定义如下： L_i = \sum_{j \neq y_i} max(0, s_j-s_{y_i}+\Delta)举例：假设有3个分类，得到了分值是$s = [13, -7, 11]$。其中第一个类别是正确类别，同时假设$\Delta$是10，于是： L_i = max(0, -7-13+10) + max(0, 11-13+10)第一个部分结果是0，第二个部分结果是8。虽然正确分类的得分比不正确分类的得分要高(13&gt;11)，但是比10的边界值还是较小。简而言之，SVM的损失函数想要正确分类类别$y_i$的分数比不正确类别的分数高，而且至少要高$\Delta$。 在这个模型中，评分函数为$f(x_i, W) = Wx_i$，所以可以稍微改写损失函数的公式： L_i = \sum_{j\neq y_i}max(0, w^T_jx_i-w^T_{y_i}x_i+\Delta)其中$w_j$是权重$W$的第j行，被变形为列向量。 多类SVM想要正确分类的分数比不正确分类的分数至少高出delta，如果其他分类分数进入了红色的区域，那么就开始计算损失。如果没有这些情况，损失值为0。 正则化：我们希望向某些特定的权重$W$添加一些偏好，对其他权重则不添加，以此来消除模糊性。这一点是能够实现的，方法是向损失函数增加一个正则化惩罚(regularization penalty)： R(W) = \sum_k\sum_l W^2_{k,l}正则化不是数据的函数，仅基于权重。包含正则化惩罚后，可以给出完整的多类SVM损失函数了，它由两部分组成：数据损失(data loss)和正则化损失(regulatization loss)： L = \frac1N \sum_iL_i + \lambda R(W)将其展开为完整公式是： L = \frac1N\sum_i\sum_{j\neq y_i}[max(0, f(x_i;W)_j-f(x_i;W)_{y_i}+\Delta) + \lambda\sum_k\sum_l W^2_{k,l}]引入正则化惩罚带来很多良好的性质，如引入L2惩罚后，SVM就有了最大边界(max margin)。 其中最好的性质就是对大数值权重进行惩罚，可以提升其泛化能力，这就意味着没有哪个维度能够独自对整体分值有过大影响。例如，$x = [1, 1, 1, 1]$，$w_1 = [1, 0, 0, 0]$，$w_2 = [0.25, 0.25, 0.25, 0.25]$。那么$w_1^Tx = w_2^Tx = 1$，但是$w_1$的L2惩罚是1.0，而$w_2​$的L2惩罚是0.25。因此，这鼓励分类器将所有维度上的特征都用起来，而不是强烈依赖于其中少数几个维度。 与二元支持向量机(Binary Support Vector Machine)的关系：它对于第i个数据的损失计算公式是： L_i = Cmax(0, 1-y_iw^Tx_i) + R(W)可以认为本章节介绍的SVM公式包含了上述公式，此公式是只有两个类别的特例。 备注：其他多类SVM公式：本课中展示的多类SVM只是多种SVM公式中的一种。另一种常用的公式是One-Vs-All(OVA) SVM，它针对每个类和其他类训练一个独立的二元分类器。还有一种更少用的叫做All-Vs-Al(AVA)策略。最后一个需要知道的公式是Structured SVM，它将正确分类的分类分值和非正确分类中的最高分值的边界最大化。 Softmax分类器最常用的分类器有两个，一个是SVM，另一个就是Softmax。对于学习过二元逻辑回归的读者来说，Softmax分类器就可以理解为逻辑回归分类器面对多个分类的一般化归纳。在Softmax分类器中，函数映射$f(x_i;W) = Wx_i$保持不变，但将这些评分值视为每个分类的未归一化的对数概率，并且将折叶损失(hinge loss)替换为交叉熵损失(cross-entropy loss)。公式如下： L_i = -log(\frac{e^{f_{y_i}}} {\sum_j e^{f_j}})保证数值稳定的小技巧：编程实现softmax函数的时候，由于指数函数的存在，数值可能会变得非常大，因此归一化技巧非常重要。可以加入常数C，得到一个从数学上等价的公式： \frac{e^{f_{y_i}}}{\sum_je^{f_j}} = \frac{Ce^{f_{y_i}}}{C\sum_je^{f_j}} = \frac{e^{f_{y_i}+logC}}{\sum_je^{f_j+logC}}通常将$C$设为$logC = -maxf_j$。 该技巧简单地说，就是将向量$f$中的数值进行平移，使得最大值为0。代码实现如下： 123456f = np.array([123, 456, 789]) # 例子中有3个分类，每个评分的数值都很大p = np.exp(f) / np.sum(np.exp(f)) # 不妙：数值问题，可能导致数值爆炸# 那么将f中的值平移到最大值为0：f -= np.max(f) # f becomes [-666, -333, 0]p = np.exp(f) / np.sum(np.exp(f)) # 现在OK了，将给出正确结果 SVM和Softmax的比较下图有助于区分这两种分类器。 两个分类器都计算了同样的分值向量f。不同的是对于f中分值的解释： SVM把它们看作是分类评分，它的损失函数鼓励正确分类的分值比其他分类的分值高出至少一个边界值。 Softmax把它们看作是没有归一化的对数概率，鼓励正确分类的归一化的对数概率变高，其余的变低。 在实际使用中，SVM和Softmax经常是相似的：通常说来，两种分类器的表现差别很小。但相对于Softmax，SVM更加“局部目标化”，这既可以看作是一个特性，也可以看作是一个劣势。例如分数是[10, -100, -100]和[10, 9, 9]，对SVM($\Delta = 1​$)来说没什么不同，两者的损失值都是0。但是Softmax计算出[10, 9, 9]的损失值就远远高于[10, -100, -100]的。 小结总结如下： 定义了从图像像素映射到不同类别的分类评分的评分函数。在本节中，评分函数是一个基于权重$W$和偏差$b$的线性函数。 相对于kNN分类器，参数方法的预测非常快，因为其只需要与权重$W​$进行一个矩阵乘法运算。 介绍了偏差技巧，将偏差向量和权重矩阵合二为一。 定义了损失函数（SVM和Softmax）。 参考：https://zhuanlan.zhihu.com/p/21930884]]></content>
      <categories>
        <category>cs231n</category>
      </categories>
      <tags>
        <tag>Computer Vision</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CHAPTER1 图像分类]]></title>
    <url>%2Fposts%2F58641%2F</url>
    <content type="text"><![CDATA[本节介绍图像分类问题和数据驱动方法。 图像分类目标：输入一个元素为像素值的数组，然后给它分配一个分类标签。 例子：以下图为例，图像是由数字组成的巨大的3维数组，图片大小是248*400，该图像就包括了248×400×3=297600个数字，每个数字都是在范围0-255之间的整型，其中0表示全黑，255表示全白。 困难和挑战：识别一幅猫的图片对人来说很简单，但从计算机视觉的角度来看就值得深思了。以下列举了一些在图像识别方面的困难。 视角变化(Viewpoint variation) 大小变化(Scale variation) 形变(Deformation) 遮挡(Occlusion) 光照条件(Illumination conditions) 背景干扰(Background clutter) 类内差异(Intra-class variation) 数据驱动方法：给计算机很多数据，然后实现学习算法，让计算机学习到每个类的外形。 图像分类流程：完整流程如下： 输入：输入是包含N个图像的集合，这个集合称为训练集。 学习: 该步骤叫做训练分类器或者学习一个模型。 评价：让分类器来预测它未曾见过的分类标签，并以此来评价分类器的质量。 Nearest Neighbor分类器虽然此分类器和卷积神经网络没有任何关系，实际中也极少使用，但实现它可以让读者对于解决图像分类问题的方法有个基本的认识。 L1和L2比较：在面对两个向量之间的差异时，L2比L1更加不能容忍这些差异。也就是说，相对于一个巨大的差异，L2距离更加倾向于接收多个中等程度的差异。 更高的k值可以让分类的效果更平滑。 用于超参数调优的验证集注意：绝不能使用测试集来进行调优 测试数据集只使用一次，即在训练完成后评价最终的模型时使用 把训练集分为训练集和验证集。使用验证集来对所有超参数调优，最后只在测试集上跑一次并报告结果。 交叉验证。当训练集数量较小的时候，使用交叉验证。如将训练集平均分为5份，其中4份用来训练，1份用来验证，每份轮流作为验证集。取5次验证结果的平均值作为算法验证结果。一般分为3、5和10份。 Nearest Neighbor的优劣优点： 易于理解，实现简单。 算法训练不需要花时间，因为其训练过程只是将训练集数据存储起来。 缺点： 分类器需要存储所有的训练数据，这在存储空间上是低效的。 对一个测试图像进行分类需要和所有训练图像进行比较，算法计算资源耗费高。 在实际的图像分类工作中很少使用，因为高维度向量之间的距离通常是反直觉的。 基于距离的图像分类是基于背景的，而不是基于图片的语义主体。 小结 介绍了图像分类问题。 介绍了一个简单的分类器：最近邻分类器(Nearest Neighbor Classifier)，分类器中存在不同的超参数（如k值或距离类型）。 选取超参数的正确方法：将原始训练集分为训练集和验证集，在验证集上尝试不同的超参数，最后保留表现最好的超参数。 如果训练数据量不够，使用交叉验证。 一旦在验证集上找到最优的超参数，就让算法以该参数在测试集上跑且只跑一次，根据结果评价算法。 kNN分类器能够在CIFAR-10上得到近40%的准确率，尽管kNN算法简单易实现，但是需要存储所有训练数据，并且在测试的时候过于耗费计算资源。 仅仅使用L1和L2范数来进行像素比较是不够的，这样做的话，图像更多是按照背景和颜色被分类，而不是语义主体本身。 六、小结：实际应用kNN 预处理你的数据：对数据中的特征进行归一化，使其具有零平均值和单位方差。 如果是高维数据，考虑使用降维，如PCA或随机投影。 如果需要预测的超参数较多，那么就应该使用更大的验证集来有效地估计。如果担心验证集不够，那么就使用交叉验证。在计算资源足够的情况下，交叉验证更加安全。 在验证集上调优的时候，尝试使用更多的k值，以及尝试L1和L2两种范数计算方式。 如果分类器跑得太慢，可以尝试使用Approximate Nearest Neighbor库来加速。 对验证集上得到的最优超参数做记录，直接使用测试集来测试最优模型。 参考：https://zhuanlan.zhihu.com/p/21930884]]></content>
      <categories>
        <category>cs231n</category>
      </categories>
      <tags>
        <tag>Computer Vision</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
</search>
