<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Logistic 回归模型]]></title>
    <url>%2Fposts%2F48429%2F</url>
    <content type="text"><![CDATA[Logistic 回归是统计学习中的经典方法，属于对数线性模型。 二项 Logistic 回归模型将线性回归函数和Logistic函数复合起来，称为逻辑回归函数，二项Logistic回归模型是一种分类模型，二项Logistic回归模型是如下的条件概率分布： \begin{align*} \\& P \left( Y = 1 | x \right) = \dfrac{1}{1+\exp{-\left(w \cdot x + b \right)}} \\ & \quad\quad\quad\quad = \dfrac{\exp{\left(w \cdot x + b \right)}}{\left( 1+\exp{-\left(w \cdot x + b \right)}\right) \cdot \exp{\left(w \cdot x + b \right)}} \\ & \quad\quad\quad\quad = \dfrac{\exp{\left(w \cdot x + b \right)}}{1+\exp{\left( w \cdot x + b \right)}}\\& P \left( Y = 0 | x \right) = 1- P \left( Y = 1 | x \right) \\ & \quad\quad\quad\quad=1- \dfrac{\exp{\left(w \cdot x + b \right)}}{1+\exp{\left( w \cdot x + b \right)}} \\ & \quad\quad\quad\quad=\dfrac{1}{1+\exp{\left( w \cdot x + b \right)}}\end{align*}其中，$x \in R^{n}$是输入，$Y \in \left\{ 0, 1 \right\}$是输出，$w \in R^{n}$和$b \in R$是参数，$w$称为权值向量，$b$称为偏置，$w \cdot x$为$w$和$b$的内积。 Logistic回归比较两个条件概率值的大小，将实例$x$分到概率值较大的那一类。 可将权值权值向量和输入向量加以扩充，即$w = \left( w^{\left(1\right)},w^{\left(2\right)},\cdots,w^{\left(n\right)},b \right)^{T}$，$x = \left( x^{\left(1\right)},x^{\left(2\right)},\cdots,x^{\left(n\right)},1 \right)^{T}$，则逻辑斯谛回归模型如下： \begin{align*} \\& P \left( Y = 1 | x \right) = \dfrac{\exp{\left(w \cdot x \right)}}{1+\exp{\left( w \cdot x \right)}}\\& P \left( Y = 0 | x \right) =\dfrac{1}{1+\exp{\left( w \cdot x \right)}}\end{align*}模型参数估计Logistic回归模型学习时，对于给定训练数据集$T = \left\{ \left( x_{1}, y_{1} \right), \left( x_{2}, y_{2} \right), \cdots, \left( x_{N}, y_{N} \right) \right\} $，其中，$x_{i} \in R^{n+1}, y_{i} \in \left\{ 0, 1 \right\}, i = 1, 2, \cdots, N$，可以应用极大似然估计法估计模型参数，从而得到Logistic回归模型。 设： \begin{align*} \\& P \left( Y =1 | x \right) = \pi \left( x \right) ,\quad P \left( Y =0 | x \right) = 1 - \pi \left( x \right) \end{align*}似然函数为： \begin{align*} \\& l \left( w \right) = \prod_{i=1}^{N} P \left( y_{i} | x_{i} \right) \\ & = P \left( Y = 1 | x_{i} , w \right) \cdot P \left( Y = 0 | x_{i}, w \right) \\ & = \prod_{i=1}^{N} \left[ \pi \left( x_{i} \right) \right]^{y_{i}}\left[ 1 - \pi \left( x_{i} \right) \right]^{1 - y_{i}}\end{align*}对数似然函数为： \begin{align*} \\& L \left( w \right) = \log l \left( w \right) \\ & = \sum_{i=1}^{N} \left[ y_{i} \log \pi \left( x_{i} \right) + \left( 1 - y_{i} \right) \log \left( 1 - \pi \left( x_{i} \right) \right) \right] \\ & = \sum_{i=1}^{N} \left[ y_{i} \log \dfrac{\pi \left( x_{i} \right)}{1- \pi \left( x_{i} \right)} + \log \left( 1 - \pi \left( x_{i} \right) \right) \right] \\ & = \sum_{i=1}^{N} \left[ y_{i} \left( w \cdot x_{i} \right) - \log \left( 1 + \exp \left( w \cdot x \right) \right) \right]\end{align*}对$L(w)$求极大值，得到$w$的估计值。这样，问题就变成了以对数似然函数为目标函数的最优化问题。Logistic回归学习中通常采用的方法是梯度下降法和拟牛顿法。 假设$w$的极大似然估计值是$\hat{w}$，则学得的Logistic回归模型为： \begin{align*} \\& P \left( Y = 1 | x \right) = \dfrac{\exp{\left(\hat{w} \cdot x \right)}}{1+\exp{\left( \hat{w} \cdot x \right)}}\\& P \left( Y = 0 | x \right) =\dfrac{1}{1+\exp{\left( \hat{w} \cdot x \right)}}\end{align*}多项 Logistic 回归模型可将上面介绍的二项分类Logistic回归模型推广为多项Logistic回归模型，用于多类分类。 假设离散型随机变量$Y$的取值集合$\left\{ 1, 2, \cdots, K \right\}$，则多项逻辑斯谛回归模型为： \begin{align*} \\& P \left( Y = k | x \right) = \dfrac{\exp{\left(w_{k} \cdot x \right)}}{1+ \sum_{k=1}^{K-1}\exp{\left( w_{k} \cdot x \right)}}, \quad k=1,2,\cdots,K-1 \\ & P \left( Y = K | x \right) = 1 - \sum_{k=1}^{K-1} P \left( Y = k | x \right) \\ & = 1 - \sum_{k=1}^{K-1} \dfrac{\exp{\left(w_{k} \cdot x \right)}}{1+ \sum_{k=1}^{K-1}\exp{\left( w_{k} \cdot x \right)}} \\ & = \dfrac{1}{1+ \sum_{k=1}^{K-1}\exp{\left( w_{k} \cdot x \right)}}\end{align*}二项Logistic回归的参数估计法也可以推广到多项Logistic回归。]]></content>
      <categories>
        <category>《统计学习方法》读书笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Logistic回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker命令使用记录]]></title>
    <url>%2Fposts%2F41898%2F</url>
    <content type="text"><![CDATA[容器启动Docker容器： 注：# tensorcomprehensions是用户名，tc-cuda9.0-cudnn7.1-ubuntu16.04-devel是仓库名，latest是标签 1sudo nvidia-docker run -ti tensorcomprehensions/tc-cuda9.0-cudnn7.1-ubuntu16.04-devel:latest bash 查看当前系统中容器的列表： 12docker ps -a # 查看包括所有容器docker ps # 查看正在运行的容器 删除容器： 1docker rm CONTAINER ID 重新启动已经停止的容器： 12sudo docker start CONTAINER ID # 也可以使用 docker restartsudo docker attach CONTAINER ID # 附着到容器的会话上 重启Docker： 1systemctl restart docker 镜像列出镜像： 1sudo docker images]]></content>
      <categories>
        <category>Docker命令使用纪录</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu 16.04更换apt源为国内源]]></title>
    <url>%2Fposts%2F9484%2F</url>
    <content type="text"><![CDATA[直接使用国外源的话太慢了，试了几个国内的镜像源，感觉还是阿里源最快。现在北京时间2019年3月12日有效。 首先备份 sources.list： 1cp /etc/apt/sources.list /etc/apt/sources.list.bak 删除 /etc/apt/sources.list 下的内容，修改为： 12345678910deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse 然后更新源试试： 12sudo apt-get updatesudo apt-get upgrade]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树模型]]></title>
    <url>%2Fposts%2F51507%2F</url>
    <content type="text"><![CDATA[决策树模型通常包括3个步骤： 特征选择 决策树的生成 决策树的修剪 特征选择特征选择在于选取对训练数据具有分类能力的特征，这样可以提高决策树学习的效率。通常特征选择的准则是信息增益或信息增益比。 信息增益熵表示随机变量不确定性的度量。设$X$是一个取有限个值的离散随机变量，其概率分布为 \begin{align} P \left( X = x_{i} \right) = p_{i}, \quad i =1, 2, \cdots, n \end{align}则随机变量$X​$的熵定义为 \begin{align} H \left( X \right) = H \left( p \right) = - \sum_{i=1}^{n} p_{i} \log p_{i} \end{align}其中，若$p_{i}=0$，则定义$0 \log 0 = 0$。 若$p_i = \frac1n​$，则 \begin{align*} \\ & H \left( p \right) = - \sum_{i=1}^{n} p_{i} \log p_{i} \\ & = - \sum_{i=1}^{n} \dfrac{1}{n} \log \dfrac{1}{n} \\ & = \log n\end{align*}从定义可验证 \begin{align*} \\ & 0 \leq H \left( p \right) \leq \log n\end{align*}在随机变量$X$给定的条件下随机变量$Y$的条件熵为： \begin{align*} \\ & H \left( Y | X \right) = \sum_{i=1}^{n} p_{i} H \left( Y | X = x_{i} \right) \end{align*}即，$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望。其中，$p_{i}=P \left( X = x_{i} \right), i= 1,2,\cdots,n$，条件熵$H \left( Y | X \right)$表示在已知随机变量$X$的条件下随机变量$Y​$的不确定性。 特征$A$对训练集$D$的信息增益为： \begin{align*} \\ & g \left( D, A \right) = H \left( D \right) - H \left( D | A \right) \end{align*}一般地，熵$H(Y)$与条件熵$H(Y|X)$之差称为互信息。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。 设训练数据集为$D$，$\left| D \right|$表示其样本容量，即样本个数。设有$K$个类$C_{k}, k=1,2,\cdots,K$，$\left| C_{k} \right|$为属于类$C_{k}$的样本的个数，$\sum_{k=1}^{K} \left| C_{k} \right| = \left| D \right|$。设特征$A$有$n$个不同的特征取值$\left\{ a_{1},a_{2},\cdots,a_{n}\right\}$，根据特征$A$的取值将$D$划分为$n$个子集$D_{1},D_{2},\cdots,D_{n}$，$\left| D_{i} \right|$为$D_{i}$的样本数，$\sum_{i=1}^{n}\left| D_{i} \right| = \left| D \right|$。记子集$D_{i}$中属于类$C_{k}$的样本的集合为$D_{ik}$，即$D_{ik} = D_{i} \cap C_{k}$，$\left| D_{ik} \right|$为$D_{ik}$的样本个数。 信息增益算法输入：训练数据集$D$和特征$A$ 输出：特征$A$对训练数据集$D$的信息增益$g(D, A)$ 计算训练数据集$D$的经验熵$H(D)$ \begin{align*} \\ & H \left( D \right) = -\sum_{k=1}^{K} \dfrac{\left|C_{k}\right|}{\left| D \right|}\log_{2}\dfrac{\left|C_{k}\right|}{\left| D \right|} \end{align*} 计算特征$A$对 数据集$D$的经验条件熵$H(D|A)$ \begin{align*} \\ & H \left( D | A \right) = \sum_{i=1}^{n} \dfrac{\left| D_{i} \right|}{\left| D \right|} H \left( D_{i} \right) \\ & = \sum_{i=1}^{n} \dfrac{\left| D_{i} \right|}{\left| D \right|} \sum_{k=1}^{K} \dfrac{\left| D_{ik} \right|}{\left| D_{i} \right|} \log_{2} \dfrac{\left| D_{ik} \right|}{\left| D_{i} \right|}\end{align*} 计算信息增益 \begin{align*} \\ & g \left( D, A \right) = H \left( D \right) - H \left( D | A \right) \end{align*} 信息增益比以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比可以对这一问题进行校正，这是特征选择的另一准则。 特征$A$对训练集$D$的信息增益比定义为其信息增益$g\left( D, A \right)$与训练数据集$D$关于特征$A$的经验熵$H_{A}\left(D\right)​$之比： \begin{align*} \\ & g_{R} \left( D, A \right) = \dfrac{g \left( D, A \right)}{H_{A} \left(D\right)}\end{align*}其中， \begin{align*} \\ & H_{A} \left( D \right) = -\sum_{i=1}^{n} \dfrac{\left|D_{i}\right|}{\left|D\right|}\log_{2}\dfrac{\left|D_{i}\right|}{\left|D\right|}\end{align*}决策树的生成ID3 算法ID3算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。 输入：训练数据集$D$，特征$A$，阈值$\varepsilon$输出：决策树$T$ 若$D$中所有实例属于同一类$C_{k}$，则$T$为单结点树，并将类$C_{k}$作为该结点的类标记，返回$T$； 若$A = \emptyset$，则$T$为单结点树，并将$D$中实例数最大的类$C_{k}$作为该结点的类标记，返回$T​$； 否则，计算$A$中各特征$D$的信息增益，选择信息增益最大的特征$A_{g}$ \begin{align} \ & A{g} = \arg \max{A} g \left( D, A \right) \end{align} 如果$A_{g}$的信息增益小于阈值$\varepsilon$，则置$T$为单结点树，并将$D$中实例数量最大的类$C_{k}$作为该结点的类标记，返回$T$; 否则，对$A_{g}​$的每一个可能值$a_{i}​$，依$A_{g}=a_{i}​$将$D​$分割为若干非空子集$D_{i}​$，将$D_{i}​$中实例数对大的类作为标记，构建子结点，由结点及其子结点构成树$T​$，返回$T​$； 对第$i$个子结点，以$D_{i}$为训练集，以$A-\left\{A_{g}\right\}$为特征集，递归地调用步1.～步5.，得到子树$T_{i}$，返回$T_{i}$。 C4.5 的生成算法C4.5算法与ID3算法相似，只是将信息增益改为信息增益比来选择特征。 决策树的剪枝在决策树学习中将已生成的树进行简化的过程称为剪枝。决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。 设树$T$的叶结点个数为$\left| T \right|$，$t$是树$T$的叶结点，该叶结点有$N_{t}$个样本点，其中$k$类的样本点有$N_{tk}$个，$k=1,2,\cdots,K$，$H_{t}\left(T\right)$为叶结点$t$上的经验熵，$\alpha \geq 0$为参数，则决策树的损失函数可以定义为： \begin{align*} \\ & C_{\alpha} \left( T \right) = \sum_{t=1}^{\left| T \right|} N_{t} H_{t} \left( T \right) + \alpha \left| T \right| \end{align*}其中经验熵为： \begin{align*} \\ & H_{t} \left( T \right) = - \sum_{k} \dfrac{N_{tk}}{N_{t}} \log \dfrac{N_{tk}}{N_{t}} \end{align*}在损失函数中，记： \begin{align*} \\ & C \left( T \right) = \sum_{t=1}^{\left| T \right|} N_{t} H_{t} \left( T \right) = - \sum_{t=1}^{\left| T \right|} \sum_{k=1}^{K} N_{tk} \log \dfrac{N_{tk}}{N_{t}} \end{align*}这时有： \begin{align*} \\ & C_{\alpha} \left( T \right) = C \left( T \right) + \alpha \left| T \right| \end{align*}其中，$C \left( T \right)$表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，$\left| T \right|$表示模型复杂度，参数$\alpha \geq 0​$控制两者之间的影响。 决策树的剪枝算法输入：决策树$T$，参数$\alpha$输出：修剪后的子树$T_{\alpha}​$ 计算每个结点的经验熵 。 递归地从树的叶结点向上回缩。 设一组叶结点回缩到其父结点之前与之后的整体树分别为$T_{B}$与$T_{A}$，其对应的损失函数值分别是$C_{\alpha} \left( T_{B} \right)$与$C_{\alpha} \left( T_{A} \right)$，如果 \begin{align} \ & C_{\alpha} \left( T_{A} \right) \leq C_{\alpha} \left( T_{B} \right) \end{align}则进行剪枝，即将父结点变为新的叶结点。 返回2.，直到不能继续为止，得到损失函数最小的子树$T_{\alpha}$。]]></content>
      <categories>
        <category>《统计学习方法》读书笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装Docker和Nvidia-Docker]]></title>
    <url>%2Fposts%2F35246%2F</url>
    <content type="text"><![CDATA[安装DockerDocker官方为了简化流程，提供了一套便捷的安装脚本，在Ubuntu上可以使用脚本自动安装Docker CE： 12curl -fsSL get.docker.com -o get-docker.shsudo sh get-docker.sh --mirror Aliyun 启动Docker CE： 12sudo systemctl enable dockersudo systemctl start docker 建立Docker用户组： 12sudo groupadd dockersudo usermod -aG docker $USER 测试Docker是否安装正确： 1docker run hello-world 安装Nvidia-Docker123456789101112131415# If you have nvidia-docker 1.0 installed: we need to remove it and all existing GPU containersdocker volume ls -q -f driver=nvidia-docker | xargs -r -I&#123;&#125; -n1 docker ps -q -a -f volume=&#123;&#125; | xargs -r docker rm -fsudo yum remove nvidia-docker# Add the package repositoriesdistribution=$(. /etc/os-release;echo $ID$VERSION_ID)curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | \ sudo tee /etc/yum.repos.d/nvidia-docker.repo# Install nvidia-docker2 and reload the Docker daemon configurationsudo yum install -y nvidia-docker2sudo pkill -SIGHUP dockerd# Test nvidia-smi with the latest official CUDA imagedocker run --runtime=nvidia --rm nvidia/cuda:9.0-base nvidia-smi]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[提升算法 AdaBoost 算法]]></title>
    <url>%2Fposts%2F48763%2F</url>
    <content type="text"><![CDATA[提升算法的基本思路提升算法是从弱学习算法出发，反复学习，得到一系列弱分类器（又称为基本分类器），然后组合这些弱分类器，构成一个强分类器。 有两个问题： 在每一轮如何改变训练数据的权值或概率分布 如何将弱分类器组合成一个强分类器 对于第一个问题，AdaBoost的做法是提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。对于第二个问题，AdaBoost采取加权多数表决的方法。即加大分类误差率小的弱分类器的权值，减小分类误差率大的弱分类器的权值。 AdaBoost 算法输入：训练数据集$T = \left\{ \left( x_{1}, y_{1} \right), \left( x_{2}, y_{2} \right), \cdots, \left( x_{N}, y_{N} \right) \right\}$，其中$x_{i} \in \mathcal{X} \subseteq R^{n}, y_{i} \in \mathcal{Y} = \left\{ +1, -1 \right\}, i = 1, 2, \cdots, N$；弱学习算法。 输出：最终分类器$G(x)$。 初始化训练数据的权值分布 \begin{align*} \\ & D_{1}=\left(w_{11},w_{12},\cdots,w_{1N}\right), \quad w_{1i} = \dfrac{1}{N}, \quad i=1,2,\cdots,N\end{align*} 对$m=1,2,\cdots,M​$ （a）使用具有权值分布$D_m$的训练数据集学习，得到基本分类器 \begin{align*} \\ & G_{m}\left(x\right): \mathcal{X} \to \left\{ -1, +1\right\} \end{align*}（b）（b）计算$G_m(x)$在训练数据集上的分类误差率 \begin{align*} \\& e_{m} = P\left(G_{m}\left(x_{i}\right) \neq y_{i}\right) \\ & = \sum_{i=1}^{N} w_{mi} I \left(G_{m}\left(x_{i}\right) \neq y_{i} \right) \end{align*}（c）计算$G_m(x)​$的系数 \begin{align*} \\ & \alpha_{m} = \dfrac{1}{2} \log \dfrac{1-e_{m}}{e_{m}} \end{align*}（d）更新训练数据集的权值分布 \begin{align*} \\ & D_{m+1}=\left(w_{m+1,1},\cdots,w_{m+1,i},\cdots,w_{m+1,N}\right) \\ & w_{m+1,i} = \dfrac{w_{mi}}{Z_{m}} \exp \left(- \alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right), \\ & \quad \quad = \left\{ \begin{aligned} \ & \dfrac{w_{mi}}{Z_{m}} \exp \left(- \alpha_{m} \right), G_{m}\left(x_{i}\right) = y_{i} \\ & \dfrac{w_{mi}}{Z_{m}} \exp \left( \alpha_{m} \right), G_{m}\left(x_{i}\right) \neq y_{i} \end{aligned} \right. \quad i=1,2,\cdots,N \end{align*}​ 其中，$Z_m​$是规范化因子 \begin{align*} \\ & Z_{m}＝ \sum_{i=1}^{N} w_{mi} \exp \left(- \alpha_{m} y_{i}, G_{m}\left(x_{i}\right)\right)\end{align*} 构建基本分类器的线性组合 \begin{align*} \\ & f \left( x \right) = \sum_{m=1}^{M} \alpha_{m} G_{m} \left( x \right) \end{align*}得到最终分类器 \begin{align*} \\ & G\left(x\right) = sign\left(f\left(x\right)\right)=sign\left(\sum_{m=1}^{M} \alpha_{m} G_{m} \left( x \right)\right) \end{align*} AdaBoost 算法的解释AdaBoost 算法可以认为是模型为加法模型、损失函数为指数函数、学习算法为前向分布算法时的二类分类学习方法。 前向分布算法前向分布算法：输入：训练数据集$T = \left\{ \left( x_{1}, y_{1} \right), \left( x_{2}, y_{2} \right), \cdots, \left( x_{N}, y_{N} \right) \right\}$，损失函数$L\left(y,f\left(x\right)\right)$；基函数集$\left\{b\left(x;\gamma\right)\right\}$输出：加法模型$f\left(x\right)​$ 初始化$f_{0}\left(x\right)=0​$ 对$m=1,2,\cdots,M$ （a） 极小化损失函数 \begin{align} \ & \left(\beta_{m},\gamma_{m}\right) = \arg \min{\beta,\gamma} \sum_{i=1}^{N} L \left( y_{i},f_{m-1} \left(x_{i}\right) + \beta b\left(x_{i};\gamma \right)\right) \end{align} 得到参数$\beta_{m},\gamma_{m}$ （b）更新 \begin{align*} \\& f_{m} \left(x\right) = f_{m-1} \left(x\right) + \beta_{m} b\left(x;\gamma_{m}\right) \end{align*} （c）得到加法模型 \begin{align*} \\ & f \left( x \right) = f_{M} \left( x \right) = \sum_{m=1}^{M} \beta_{m} b \left( x; \gamma_{m} \right) \end{align*}前向分布算法与 AdaBoostAdaBoost 算法是前向分布加法算法的特例。这时，模型是由基本分类器组成的加法模型，损失函数是指数函数。]]></content>
      <categories>
        <category>《统计学习方法》读书笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>提升算法</tag>
        <tag>集成学习</tag>
        <tag>AdaBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Linux的Jupyter Notebook中安装R内核]]></title>
    <url>%2Fposts%2F61698%2F</url>
    <content type="text"><![CDATA[普通电脑已经无法满足日益增长的计算需求了，于是在服务器上安装了R，还没有在Jupyter上写过Python之外的语言，这次来试试在Jupyter上写R。 网上很多方法是直接安装devtools包的，利用其中的的 install_github() 函数用于从Github上安装R内核。但如果直接install.packages的话，反正我是死活装不上这个包，提示各种错误。这里分享我的安装方法，给大家提供一个参考吧。 安装devtools前的准备直接在终端执行如下命令： 1234567sudo apt-get install gfortransudo apt-get install build-essential sudo apt-get install libxt-dev sudo apt-get install libcurl4-openssl-devsudo apt-get install libxml++2.6-devsudo apt-get install libssl-devsudo R # 进入R 安装devtools添加dependencies = T同时安装必要的依赖： 12install.packages(&quot;devtools&quot;, dependencies = T)library(devtools) # 如果没有任何提示就代表成功了 安装R kernel并注册从Github安装R kernel： 1devtools::install_github(&apos;IRkernel/IRkernel&apos;) 在R中注册激活R kernel： 1IRkernel::installspec() 然后打开Jupyter，看到可以新建R notebook了：]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>R语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多机使用SSH key连接至Github Page]]></title>
    <url>%2Fposts%2F35104%2F</url>
    <content type="text"><![CDATA[设置Github在本地机器中安装好Git和Hexo之后，在终端或是Git Bash中执行如下代码： 12git config --global user.name &quot;YOUR NAME&quot; # Github注册账户名git config --global user.email &quot;YOUR EMAIL ADDRESS&quot; # Github注册邮箱 验证Github输入以下命令，生成SSH key： 1ssh-keygen -t rsa -b 4096 -C &quot;YOUR EMAIL ADDRESS&quot; # Github注册邮箱 将 SSH key 添加到 ssh-agent： 123eval &quot;$(ssh-agent -s)&quot; # 开启 ssh-agentssh-add ~/.ssh/id_rsa # 添加SSH key到 ssh-agentclip &lt; ~/.ssh/id_rsa.pub # 将SSH key复制出来 登录Github，依次点击自己的头像，Settings，SSH and GPG keys， Add SSH key， 在 Title 这里输入 Key 的label，我的是Linux-PC，将SSH key添加到Github账户。 然后测试SSH连接，在终端或Git Bash中执行： 1ssh -T git@github.com 如果返回的是You’ve successfully authenticated, but GitHub does not provide shell access，那么就表示已经成功了。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[V2Ray的Linux客户端配置]]></title>
    <url>%2Fposts%2F26401%2F</url>
    <content type="text"><![CDATA[V2Ray 是 Project V 下的一个工具。Project V 是一个包含一系列构建特定网络环境工具的项目，而 V2Ray 属于最核心的一个。我们可以将其理解为类似于Shadowsocks的代理软件，但V2Ray的功能更强大，而且更不容易被墙检测到。V2Ray在win和mac下都有GUI客户端可使用，但在Linux下没有，只能通过修改配置来使用，由于客户端配置比较复杂，本文提供一种偷懒版配置方法，即在win或mac下将配置好的config文件复制一份到Linux下使用。 安装V2RayV2Ray官方提供了一个自动化安装的脚本，直接使用这个脚本安装即可，在终端中运行： 1234wget https://install.direct/go.sh # 下载脚本sudo bash go.sh # 执行脚本sudo systemctl start v2ray # 启动V2Ray，测试是否安装成功sudo systemctl stop v2ray # 停止V2Ray 配置V2Ray在win或mac中找到已配置好的config.json，复制到/etc/v2ray/config.json后执行： 1sudo systemctl restart v2ray # 重启V2Ray 现在有两种方法启动V2Ray的代理，先介绍第一种，在系统层面上启动代理，这种算是全局代理了。进入本地计算机设置中的网络——系统代理，我这里监听的端口是2333，协议是socks，如下图： 另一种方法是在Chrome或Firefox中安装SwitchyOmega来管理，使用SwitchyOmega网上已经很多了，这里就不再讲了。等全部设置完毕后，可以试试能不能上油管，享受自由的互联网吧！ 如果想访问国内网站不走代理的话，建议在Chrome下配合SwitchyOmega来使用，用自动切换模式自由度更高，而且能够设置规则列表，实时更新被墙的网址。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>V2Ray</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[序列最小最优算法(SMO算法)]]></title>
    <url>%2Fposts%2F3981%2F</url>
    <content type="text"><![CDATA[序列最小最优化算法(sequential minimal optimization, SMO)算法：1998年由Platt提出。支持向量机的学习问题可以形式化为求解凸二次规划问题，这样的凸二次规划问题具有全局最优解，并且有许多最优化算法可以用于这一问题的求解。但是当训练样本容量很大时，这些算法往往变得非常低效，以致无法使用。所以，如何高效地实现支持向量机学习就成为一个重要的问题。 SMO算法是一种启发式算法，基本思路如下： 如果所有变量的解都满足此最优化问题的KKT条件，那么得到解。 否则，选择两个变量，固定其它变量，针对这两个变量构建一个二次规划问题，称为子问题，可通过解析方法求解，提高了计算速度。 子问题的两个变量：一个是违反KKT条件最严重的那个，另一个由约束条件自动确定。 SMO算法包括两个部分： 求解两个变量二次规划的解析方法 选择变量的启发式方法 序列最小最优化（sequential minimal optimization，SMO）算法要解如下凸二次规划的对偶问题： \begin{align*} \\ & \min_{\alpha} \dfrac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} K \left( x_{i}, x_{j} \right) - \sum_{i=1}^{N} \alpha_{i} \\ & s.t. \sum_{i=1}^{N} \alpha_{i} y_{i} = 0 \\ & 0 \leq \alpha_{i} \leq C , \quad i=1,2, \cdots, N \end{align*}子问题有两个变量，一个是违反KKT条件最严重的那个，另一个由约束条件自动确定。子问题的两个变量只有一个是自由变量，如果$\alpha_2$确定，那么$\alpha_1$也随之确定，所以子问题中同时更新两个变量。 SMO算法输入：训练数据集$T = \left\{ \left( x_{1}, y_{1} \right), \left( x_{2}, y_{2} \right), \cdots, \left( x_{N}, y_{N} \right) \right\}$，其中$x_{i} \in \mathcal{X} = R^{n}, y_{i} \in \mathcal{Y} = \left\{ +1, -1 \right\}, i = 1, 2, \cdots, N$，精度$\varepsilon$； 输出：近似解$\hat \alpha​$ 取初始值$\alpha^{0} = 0​$，令$k = 0​$； 选取优化变量$\alpha_{1}^{\left( k \right)},\alpha_{2}^{\left( k \right)}​$，求解 \begin{align*} \\ & \min_{\alpha_{1}, \alpha_{2}} W \left( \alpha_{1}, \alpha_{2} \right) = \dfrac{1}{2} K_{11} \alpha_{1}^{2} + \dfrac{1}{2} K_{22} \alpha_{2}^{2} + y_{1} y_{2} K_{12} \alpha_{1} \alpha_{2} \\ & \quad\quad\quad\quad\quad\quad - \left( \alpha_{1} + \alpha_{2} \right) + y_{1} \alpha_{1} \sum_{i=3}^{N} y_{i} \alpha_{i} K_{i1} + y_{2} \alpha_{2} \sum_{i=3}^{N} y_{i} \alpha_i K_{i2} \\ & s.t. \quad \alpha_{1} + \alpha_{2} = -\sum_{i=3}^{N} \alpha_{i} y_{i} = \varsigma \\ & 0 \leq \alpha_{i} \leq C , \quad i=1,2 \end{align*} 求得最优解$\alpha_{1}^{\left( k＋1 \right)},\alpha_{2}^{\left( k+1 \right)}$，更新$\alpha$为$\alpha^{\left( k+1 \right)}$； 若在精度$\varepsilon​$范围内满足停机条件 \begin{align*} \\ & \sum_{i=1}^{N} \alpha_{i} y_{i} = 0 \\ & 0 \leq \alpha_{i} \leq C, i = 1, 2, \cdots, N \\ & \end{align*} \begin{align*} y_{i} \cdot g \left( x_{i} \right) = \left\{ \begin{aligned} \ & \geq 1, \left\{ x_{i} | \alpha_{i} = 0 \right\} \\ & = 1, \left\{ x_{i} | 0 < \alpha_{i} < C \right\} \\ & \leq 1, \left\{ x_{i} | \alpha_{i} = C \right\} \end{aligned} \right.\end{align*}其中， g(x_i) = \sum_{j=1}^N \alpha_jy_jK(x_j,x_i) + b则转(4)；否则令$k = k + 1$，转(2)； 4.取$\hat \alpha = \alpha^{\left( k + 1 \right)}​$。]]></content>
      <categories>
        <category>《统计学习方法》读书笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[非线性支持向量机与核函数]]></title>
    <url>%2Fposts%2F13688%2F</url>
    <content type="text"><![CDATA[核技巧非线性分类问题如果能用$R^n$中的一个超曲面将正负例正确分开，则称这个问题为非线性可分问题。对应下图的例子，通过变换将左图中椭圆变换称右图中的直线，将非线性分类问题变换为线性分类问题。 核技巧的基本想法是通过一个非线性变换将输入空间（欧式空间$R^n$或离散集合）对应于一个特征空间（希尔伯特空间$\mathcal{H}$），使得在输入空间$R^n$中的超曲面模型对应于特征空间$\mathcal{H}​$中的超平面模型（支持向量机）。 核函数的定义设$\mathcal{X}$是输入空间（欧式空间$R^n$或离散集合），$\mathcal{H}$是特征空间（希尔伯特空间），如果存在一个从$\mathcal{X}$到$\mathcal{H}$的映射： \begin{align*} \\& \phi \left( x \right) : \mathcal{X} \to \mathcal{H} \end{align*}使得对所有$x,z \in \mathcal{X}$，函数$K \left(x, z \right)$满足条件： \begin{align*} \\ & K \left(x, z \right) = \phi \left( x \right) \cdot \phi \left( z \right) \end{align*}则称$K \left(x, z \right)$为核函数，$\phi \left( x \right)$为映射函数，式中$\phi \left( x \right) \cdot \phi \left( z \right)$为$\phi \left( x \right)$和$\phi \left( z \right)$的内积。 核函数在支持向量机中的应用在线性支持向量机的对偶问题中，无论是目标函数还是决策函数都只涉及输入实例与实例之间的内积，它们都可以用核函数$K(x_i,x_j) = \phi(x_i) \cdot \phi(x_j)$来代替。这等价于经过映射函数$\phi$将原来的输入空间变换到一个新的特征空间，将输入空间中的内积 $x_i \cdot x_j$变换为特征空间中的内积$\phi(x_i) \cdot \phi(x_j)$，在新的特征空间里从训练样本中学习线性支持向量机。 在实际应用中，往往依赖领域知识直接选择核函数，核函数选择的有效性需要通过实验验证。 常用核函数多项式核函数 \begin{align*} \\& K \left( x, z \right) = \left( x \cdot z + 1 \right)^{p} \end{align*}对应的支持向量机是一个p次多项式分类器 高斯核函数 \begin{align*} \\& K \left( x, z \right) = \exp \left( - \dfrac{\| x - z \|^{2}}{2 \sigma^{2}} \right) \end{align*}非线性支持向量机将线性支持向量机扩展到非线性支持向量机，只需将线性支持向量机对偶形式中的内积换成核函数即可。 非线性支持向量机学习算法输入：训练数据集线性可分训练数据集 $T={(x_1, y_1), (x_2, y_2),…,(x_N, y_N)}$，其中 $x_i \in R^n$，$y_i \in \{-1, 1\}$，$i=1,2,…,N​$。 输出：分类决策函数 选取适当的核函数$K(x, z)$和适当的参数$C$，构造并求解最优化问题 \begin{align*} \\ & \min_{\alpha} \dfrac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} K \left( x_{i}, x_{j} \right) - \sum_{i=1}^{N} \alpha_{i} \\ & s.t. \sum_{i=1}^{N} \alpha_{i} y_{i} = 0 \\ & 0 \leq \alpha_{i} \leq C , \quad i=1,2, \cdots, N \end{align*} 求得最优解$\alpha^{\star} = \left( \alpha_{1}^{\star}, \alpha_{1}^{\star}, \cdots, \alpha_{N}^{\star} \right)$。 选择$\alpha^{\star}$的一个分量$0 &lt; \alpha_{j}^{\star} &lt; C​$，计算 \begin{align*} \\ & b^{*} = y_{j} - \sum_{i=1}^{N} \alpha_{i}^{*} y_{i} K \left( x_{i}, x_{j} \right) \end{align*} 构造决策函数 \begin{align*} \\& f \left( x \right) = sign \left( \sum_{i=1}^{N} \alpha_{i}^{*} y_{i} K \left( x_{i}, x_{j} \right) + b^{*} \right) \end{align*}当$K(x, z)$是正定核函数时，第一步中的式子是凸二次规划问题，解是存在的。]]></content>
      <categories>
        <category>《统计学习方法》读书笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo中渲染MathJax数学公式的问题]]></title>
    <url>%2Fposts%2F2405%2F</url>
    <content type="text"><![CDATA[在学习机器学习算法的过程中有大量的公式需要渲染，原本使用的是Hexo默认的”hexo-renderer-marked”引擎，现在感觉已经力不从心了，好多复杂的公式都无法渲染，网上关于这个问题的解决方法其实有很多，但这里还是po出我觉得最靠谱的一种方法吧。 替换默认渲染引擎hexo-renderer-kramed 是 hexo-renderer-marked 的Fork修改版，只是在渲染部分进行了修改，首先替换默认渲染引擎： 12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save 解决语义冲突的问题hexo s 启动本地服务器后看到行间公式已经渲染成功了，但一些行内公式还是无法渲染。原因是hexo-renderer-kramed 也有语义冲突的问题。于是来到hexo根目录下，打开node_modules\kramed\lib\rules\inline.js 作相应的修改： 1234//escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/, # 第11行escape: /^\\([`*\[\]()#$+\-.!_&gt;])/ //em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, # 第20行em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/ 重启本地服务器后可以看到问题基本解决了，不过行内公式中针对两个 * 的语义冲突依然存在，查了一圈，发现这个问题目前也没有什么解决方法，不过问题也不太大，直接用 \star 代替就好了。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jupyter服务器端多用户配置]]></title>
    <url>%2Fposts%2F34463%2F</url>
    <content type="text"><![CDATA[实验室的服务器目前只有一个jupyter端口，而现在有多用户使用jupyter在服务器操作的需求，大家不方便用一个账户，于是就创建了多个账户。 原理其实很简单，使用不同的配置文件运行jupyter即可，下面是启动jupyter的代码： 1jupyter notebook --config &lt;config_file_path&gt; 然后使用ipython配置密码，这里配置的是哈希之后的密码，进入ipython输入： 12from notebook.auth import passwdpasswd() 复制生成的哈希密码，然后编辑配置文件： 1vim ~/.jupyter/jupyter_notebook_config.py 添加： 1c.NotebookApp.password = &apos;sha1:...&apos; # 刚才复制的哈希密码]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[拉格朗日对偶性]]></title>
    <url>%2Fposts%2F22545%2F</url>
    <content type="text"><![CDATA[在约束最优化问题中，常常利用拉格朗日对偶性将原始问题转换为对偶问题，通过解对偶问题而得到原始问题的解。 原始问题假设$f(x),c_i(x), h_j(x)​$是定义在$R^n​$上的连续可微函数，考虑约束最优化问题 min_{x \in R^n}f(x) s.t. \qquad c_i(x) \leq 0, \qquad i = 1,2,...,k h_j(x) = 0, \quad j=1,2,...,l首先，引入拉格朗日函数： L(x, \alpha, \beta) = f(x)+\sum_{i=1}^k \alpha_i c_i(x)+ \sum_{j=1}^l \beta_jh_j(x)这里，$x = (x^{(1)},x^{(1)},…, x^{(n)} )^T \in R^n$，$\alpha_i, \beta_i$是拉格朗日乘子，$\alpha_i \geq 0$，考虑x的函数： \theta_P(x) = max_{\alpha, \beta: \alpha_i \geq0}L(x, \alpha, \beta)考虑极小化问题： min_x\theta_P(x)= min_xmax_{\alpha, \beta: \alpha_i \geq0}L(x, \alpha, \beta)这与原始最优化问题等价，即它们有着相同的解，将其称为广义拉格朗日函数的极小极大问题。 对偶问题定义： \theta_D(\alpha, \beta) = min_xL(x, \alpha, \beta)再考虑极大化$\theta_D(\alpha, \beta)​$，即原始最优化问题的对偶问题： max_{\alpha, \beta: \alpha_i \geq 0}\theta_D(\alpha, \beta)= max_{\alpha, \beta: \alpha_i \geq 0}min_xL(x, \alpha, \beta) s.t. \qquad \alpha_i \geq 0, \qquad i = 1,2,...,k原始问题和对偶问题的关系若原始问题和对偶问题都有最优值，则： max_{\alpha, \beta: \alpha_i \geq 0}min_xL(x, \alpha, \beta) \leq min_xmax_{\alpha, \beta: \alpha_i \geq0}L(x, \alpha, \beta)在某些条件下，上式的等号成立，这时可以用解对偶问题替代解原始问题。 对原始问题和对偶问题，假设函数$f(x)​$和$c_i(x)​$是凸函数，$h_j(x)​$是仿射函数，并且不等式约束$c_i(x)​$是严格可行的，则$x^{\star}​$和$\alpha^{\star}，\beta^{\star}​$分别是原始问题和对偶问题的解的充分必要条件是$x^{\star}​$，$\alpha^{\star}，\beta^{\star}​$满足下面的Karush-Kuhn-Tucker (KKT)条件： \nabla_xL(x^{\star},\alpha^{\star}，\beta^{\star}) = 0 \alpha_i^{\star}c_i(x^{\star}) = 0, \qquad i=1,2,...,k c_i(x^{\star}) \leq 0, \qquad i=1,2,...,k \alpha_i^{\star} \geq 0 , \qquad i=1,2,...,k h_j(x^{\star}) = 0, \qquad i=1,2,...,k可以看出，若$\alpha_i^{\star} \geq 0$，则$c_i(x^{\star})=0$，$\alpha_i^{\star} \geq 0$称为KKT的对偶互补条件。]]></content>
      <categories>
        <category>《统计学习方法》读书笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Python模拟鼠标(键盘)操作]]></title>
    <url>%2Fposts%2F42676%2F</url>
    <content type="text"><![CDATA[昨天在帮师兄做脑核磁共振图像的颅骨去除，使用的是一个桌面软件，需要输入所需要的文件，然后软件自动进行分割，手动做的话每例大概需要三四分钟，共两百多例，这样效率太低了，且实在是很不优雅。于是决定用python写一个脚本来自动模拟鼠标操作，使用的是pyautogui库，特此记录以便参考。 首先进入base环境安装pyautogui库： 12activate baseconda install -c jim-hart pyautogui 因为需要输入的是4个nii文件，所以首先删除不需要的文件，保证文件夹内的文件数量$\leq4$： 1234567src_dir = r"F:\0"need_file = ['t1c.nii', 't2.nii', 't1.nii', 'flair.nii']for dir in os.listdir(src_dir): dst_dir = os.path.join(src_dir, dir) for file in os.listdir(dst_dir): if (file not in need_file): os.remove(os.path.join(dst_dir, file)) 然后判断需要的4个文件是否都存在，去除不满足条件的病例。 12345678src_dir = r&quot;F:\0&quot;need_file = [&apos;t1c.nii&apos;, &apos;t2.nii&apos;, &apos;t1.nii&apos;, &apos;flair.nii&apos;]for dir in os.listdir(src_dir): dst_dir = os.path.join(src_dir, dir) filenum = len([lists for lists in os.listdir(dst_dir) if os.path.isfile(os.path.join(dst_dir, lists))]) if (filenum != 4): print(dst_dir) 使用脚本控制鼠标太快了，可能会导致应用出问题，为了减缓鼠标点击速度，pyautogui提供了中断措施，为所有的pyatutogui函数增加延迟，默认的延迟是0.1s，这里做如下调整： 1pyautogui.PAUSE = 0.6 接着确定各个按钮的坐标，坐标的位置可以用以下代码确定： 12x, y = pyautogui.position()print(x, y) 这里用到的鼠标操作主要是移动、单击、双击和多次点击： 1234pag.moveTo(x,y)pyautogui.click()pyautogui.doubleClick()pyautogui.click(clicks=10, interval=0.25) 本来是有用到滚轮操作的，但是不知道什么原因，在代码中失效，也就放弃了这个操作，用多次点击代替。pyautogui这个库还是很强大的，也支持键盘操作，只不过这里不需要键盘操作也就没有用到。 比较简单地介绍了使用python模拟鼠标（键盘）操作，以下是具体的源码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134import pyautogui as pagimport osimport timescreenWidth, screenHeight = pag.size()assert screenWidth == 1920assert screenHeight == 1080x, y = pag.position()print(x, y)pag.moveTo(700, 700)pag.PAUSE = 0.6pag.moveTo(folder_rightx, folder_righty)# pag.click(clicks=1, interval=0.25)loadx, loady = 18, 32patientx, patienty = 71, 56outputx, outputy = 1247, 666Fdiskx, Fdisky = 724, 425folderx, foldery = 695, 443folder_rightx, folder_righty = 981, 359t1_extrax, t1_extray = 712, 441t1x, t1y = 956, 380t1cx, t1cy = 956, 380+17t2x, t2y = 956, 380+35flairx, flariy = 956, 380-17load2x, load2y = 700, 700performx, performy = 121, 404next1x, next1y = 900, 724next2x, next2y = 1207, 742# set T1 as baselineblx, bly = 1230, 444for i in range(0, 1): print(&quot;current No: %d&quot; % i) try: # open add patient pag.moveTo(loadx, loady) pag.click() pag.moveTo(patientx, patienty) pag.click() # open output folder pag.moveTo(outputx, outputy) pag.click() pag.moveTo(Fdiskx, Fdisky) pag.doubleClick() pag.moveTo(folderx, foldery) pag.doubleClick() if i == 0: pag.moveTo(folder_rightx, folder_righty) else: pag.moveTo(next2x, next2y) pag.click(clicks=i, interval=0.25) pag.moveTo(folder_rightx, folder_righty) pag.doubleClick() # set T1 path pag.moveTo(blx, bly) pag.click() if i == 0: pag.moveTo(t1_extrax, t1_extray) else: pag.moveTo(next1x, next1y) pag.click(clicks=i, interval=0.25) pag.moveTo(t1_extrax, t1_extray) pag.doubleClick() pag.moveTo(t1x, t1y) pag.doubleClick() # set T1c path pag.moveTo(blx, bly+38*1) pag.click() pag.moveTo(t1cx, t1cy) pag.doubleClick() # set t2 path pag.moveTo(blx, bly+38*2) pag.click() pag.moveTo(t2x, t2y) pag.doubleClick() # set Flair path pag.moveTo(blx, bly+38*3) pag.click() pag.moveTo(flairx, flariy) pag.doubleClick() # set Template (same as T1c) pag.moveTo(blx, bly+38*4) pag.click() pag.moveTo(t1cx, t1cy) pag.doubleClick() # load pag.moveTo(load2x, load2y) pag.click() time.sleep(3) # perform pag.moveTo(performx, performy) pag.click() # performing #time.sleep(60) except: print(&quot;No.%d is Error&quot; % i) # 删除不需要的文件，保证文件夹内文件数量小于等于4src_dir = r&quot;F:\0&quot;need_file = [&apos;t1c.nii&apos;, &apos;t2.nii&apos;, &apos;t1.nii&apos;, &apos;flair.nii&apos;]for dir in os.listdir(src_dir): dst_dir = os.path.join(src_dir, dir) for file in os.listdir(dst_dir): if (file not in need_file): os.remove(os.path.join(dst_dir, file)) # 判断需要的四个文件是否都存在src_dir = r&quot;F:\0&quot;need_file = [&apos;t1c.nii&apos;, &apos;t2.nii&apos;, &apos;t1.nii&apos;, &apos;flair.nii&apos;]for dir in os.listdir(src_dir): dst_dir = os.path.join(src_dir, dir) filenum = len([lists for lists in os.listdir(dst_dir) if os.path.isfile(os.path.join(dst_dir, lists))]) if (filenum != 4): print(dst_dir)]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性可分支持向量机与软间隔最大化]]></title>
    <url>%2Fposts%2F35913%2F</url>
    <content type="text"><![CDATA[线性支持向量机线性支持向量机包含线性可分支持向量机和线性不可分支持向量机。 通常情况下，训练数据中有一些特异点，线性不可分意味着这些特异点$(x_i, y_i)​$不能满足函数间隔大于等于1的约束条件。为了解决这个问题，对每个样本点引进一个松弛变量 $\xi_i \geq 0​$，使得函数间隔加上松弛变量大于等于1，约束条件变为： y_{i} \left( w \cdot x_{i} + b \right) \geq 1 - \xi_{i}同时，对每个松弛变量 $\xi_i​$ 都支付一个代价 $\xi_i​$，目标函数就由原来的 $\frac12 ||w||^2​$变成 \dfrac{1}{2} \| w \|^{2} + C \sum_{i=1}^{N} \xi_{i}其中$C&gt;0​$为惩罚参数。 目标函数有两层含义: 使 $\frac12 ||w||^2$尽量小即间隔尽量大 使误分类点的个数尽量小 这样就将线性不可分的支持向量机的学习问题变成如下凸二次规划问题： \begin{align*} \\ & \min_{w,b,\xi} \quad \dfrac{1}{2} \| w \|^{2} + C \sum_{i=1}^{N} \xi_{i} \\ & s.t. \quad y_{i} \left( w \cdot x_{i} + b \right) \geq 1 - \xi_{i} \\ & \xi_{i} \geq 0, \quad i=1,2, \cdots, N \end{align*}学习得到分离超平面为： \begin{align*} \\& w^{*} \cdot x + b^{*} = 0 \end{align*}以及相应的分类决策函数： \begin{align*} \\& f \left( x \right) = sign \left( w^{*} \cdot x + b^{*} \right) \end{align*}称为线性支持向量机。 学习的对偶算法构建拉格朗日函数引入拉格朗日乘子$\alpha_{i} \geq 0, \mu_{i} \geq 0, i = 1, 2, \cdots, N$构建拉格朗日函数： \begin{align*} \\ & L \left( w, b, \xi, \alpha, \mu \right) = \dfrac{1}{2} \| w \|^{2} + C \sum_{i=1}^{N} \xi_{i} + \sum_{i=1}^{N} \alpha_{i} \left[- y_{i} \left( w \cdot x_{i} + b \right) + 1 - \xi_{i} \right] + \sum_{i=1}^{N} \mu_{i} \left( -\xi_{i} \right) \\ & = \dfrac{1}{2} \| w \|^{2} + C \sum_{i=1}^{N} \xi_{i} - \sum_{i=1}^{N} \alpha_{i} \left[ y_{i} \left( w \cdot x_{i} + b \right) -1 + \xi_{i} \right] - \sum_{i=1}^{N} \mu_{i} \xi_{i} \end{align*}其中，$\alpha = \left( \alpha_{1}, \alpha_{2}, \cdots, \alpha_{N} \right)^{T}$以及$\mu = \left( \mu_{1}, \mu_{2}, \cdots, \mu_{N} \right)^{T}$为拉格朗日乘子向量。 求极小对偶问题是拉格朗日函数的极大极小问题，首先求$\min_{w,b}L \left( w, b, \xi, \alpha, \mu \right)$： \begin{align*} \\ & \nabla_{w} L \left( w, b, \xi, \alpha, \mu \right) = w - \sum_{i=1}^{N} \alpha_{i} y_{i} x_{i} = 0 \\ & \nabla_{b} L \left( w, b, \xi, \alpha, \mu \right) = -\sum_{i=1}^{N} \alpha_{i} y_{i} = 0 \\ & \nabla_{\xi_{i}} L \left( w, b, \xi, \alpha, \mu \right) = C - \alpha_{i} - \mu_{i} = 0 \end{align*}得 \begin{align*} \\ & w ＝ \sum_{i=1}^{N} \alpha_{i} y_{i} x_{i} \\ & \sum_{i=1}^{N} \alpha_{i} y_{i} = 0 \\ & C - \alpha_{i} - \mu_{i} = 0\end{align*}代入拉格朗日函数，得 \begin{align*} \\ & L \left( w, b, \xi, \alpha, \mu \right) = \dfrac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} \left( x_{i} \cdot x_{j} \right) + C \sum_{i=1}^{N} \xi_{i} - \sum_{i=1}^{N} \alpha_{i} y_{i} \left[ \left( \sum_{j=1}^{N} \alpha_{j} y_{j} x_{j} \right) \cdot x_{i} + b \right] \\ & \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad + \sum_{i=1}^{N} \alpha_{i} - \sum_{i=1}^{N} \alpha_{i} \xi_{i} - \sum_{i}^{N} \mu_{i} \xi_{i} \\ & = - \dfrac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} \left( x_{i} \cdot x_{j} \right) - \sum_{i=1}^{N} \alpha_{i} y_{i} b + \sum_{i=1}^{N} \alpha_{i} + \sum_{i=1}^{N} \xi_{i} \left( C - \alpha_{i} - \mu_{i} \right) \\ & = - \dfrac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} \left( x_{i} \cdot x_{j} \right) + \sum_{i=1}^{N} \alpha_{i} \end{align*}求极大求$\max_{\alpha} \min_{w,b, \xi}L \left( w, b, \xi, \alpha, \mu \right)$： \begin{align*} \\ & \max_{\alpha} - \dfrac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} \left( x_{i} \cdot x_{j} \right) + \sum_{i=1}^{N} \alpha_{i} \\ & s.t. \sum_{i=1}^{N} \alpha_{i} y_{i} = 0 \\ & C - \alpha_{i} - \mu_{i} = 0 \\ & \alpha_{i} \geq 0 \\ & \mu_{i} \geq 0, \quad i=1,2, \cdots, N \end{align*}等价于 \begin{align*} \\ & \min_{\alpha} \dfrac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} \left( x_{i} \cdot x_{j} \right) - \sum_{i=1}^{N} \alpha_{i} \\ & s.t. \sum_{i=1}^{N} \alpha_{i} y_{i} = 0 \\ & 0 \leq \alpha_{i} \leq C , \quad i=1,2, \cdots, N \end{align*}于是，可以通过求解此对偶问题而得到原始问题的解，进而确定分离超平面和决策函数。 线性可分支持向量机学习算法输入：训练数据集线性可分训练数据集 $T={(x_1, y_1), (x_2, y_2),…,(x_N, y_N)}$，其中 $x_i \in R^n$，$y_i \in \{-1, 1\}$，$i=1,2,…,N$。 输出：分离超平面和分类决策函数 选择惩罚参数$C \geq 0$，构早并求解凸二次规划问题 \begin{align*} \\ & \min_{\alpha} \dfrac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} \left( x_{i} \cdot x_{j} \right) - \sum_{i=1}^{N} \alpha_{i} \\ & s.t. \sum_{i=1}^{N} \alpha_{i} y_{i} = 0 \\ & 0 \leq \alpha_{i} \leq C , \quad i=1,2, \cdots, N \end{align*}求得最优解 $\alpha^{\star} = \left( \alpha_{1}^{\star}, \alpha_{1}^{\star}, \cdots, \alpha_{N}^{\star} \right)$ 计算 \begin{align*} \\ & w^{*} = \sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i} \end{align*}并选择$\alpha^{\star}$的一个分量$0 &lt; \alpha_{j}^{\star} &lt; C​$，计算 \begin{align*} \\ & b^{*} = y_{j} - \sum_{i=1}^{N} \alpha_{i}^{*} y_{i} \left( x_{i} \cdot x_{j} \right) \end{align*} 求得分离超平面 \begin{align*} \\ & w^{*} \cdot x + b^{*} = 0 \end{align*}以及分类决策函数 \begin{align*} \\& f \left( x \right) = sign \left( w^{*} \cdot x + b^{*} \right) \end{align*} 支持向量实例$x_i$的几何间隔为： \begin{align*} \\& \gamma_{i} = \dfrac{y_{i} \left( w \cdot x_{i} + b \right)}{ \| w \|} = \dfrac{| 1 - \xi_{i} |}{\| w \|} \end{align*}则实例$x_i$到间隔边界的距离为： \begin{align*} \\& \left| \gamma_{i} - \dfrac{1}{\| w \|} \right| = \left| \dfrac{| 1 - \xi_{i} |}{\| w \|} - \dfrac{1}{\| w \|} \right| \\ & = \dfrac{\xi_{i}}{\| w \|}\end{align*}对于每一个 $\xi_i \geq 0​$，有： \begin{align*} \xi_{i} \geq 0 \Leftrightarrow \left\{ \begin{aligned} \ & \xi_{i}=0,\quad x_{i}在间隔边界上; \\ & 0 < \xi_{i} < 1, \quad x_{i}在间隔边界与分离超平面之间; \\ & \xi_{i}=1, \quad x_{i}在分离超平面上; \\ & \xi_{i}>1, \quad x_{i}在分离超平面误分类一侧; \end{aligned} \right.\end{align*} 合页损失函数线性支持向量机学习还有另外一种解释，就是最小化以下目标函数： \sum_{i=1}^N \left[ 1 - y \left(w \cdot x + b \right) \right]_{+} + \lambda||w||^2线性支持向量机（软间隔）的合页损失函数为： \begin{align*} \\& L \left( y \left( w \cdot x + b \right) \right) = \left[ 1 - y \left(w \cdot x + b \right) \right]_{+} \end{align*}其中，“+”为取正函数： \begin{align*} \left[ z \right]_{+} = \left\{ \begin{aligned} \ & z,\quad z > 0 \\ & 0, \quad z \leq 0 \end{aligned} \right.\end{align*}]]></content>
      <categories>
        <category>《统计学习方法》读书笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git远程仓库的使用]]></title>
    <url>%2Fposts%2F757%2F</url>
    <content type="text"><![CDATA[在Github中创建远程库，然后从远程库克隆到本地。 12345git clone git@github.com:shenghaishxt/ProjectName.git # 克隆cd ProjectName/git add . # 添加文件git commit -m &apos;ProjectName&apos;git push]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[统计学习方法概论]]></title>
    <url>%2Fposts%2F13076%2F</url>
    <content type="text"><![CDATA[本系列是《统计学习方法》这本书的读书笔记，本博客会陆续更新笔记内容。 统计学习对象：数据 目的：对数据进行预测和分析 方法：监督学习、非监督学习、半监督学习、强化学习（本书主要研究监督学习） 监督学习每个具体的输入是一个实例，通常由特征向量表示。 输入实例x的特征向量： x_i = (x_i^{(1)},x_i^{(2)},..., x_i^{(n)})^T$x^{(i)}$与$x_i$不同，后者表示多个输入变量中的第$i​$个： x_i = (x_i^{(1)},x_i^{(2)}, ...,x_i^{(n)} )^T训练集通常表示为： T = \{(x_1, y_1),(x_2, y_2) ,...,(x_N, y_N)\} 回归问题：输入变量与输出变量均为连续变量的预测问题 分类问题：输出变量为有限个离散变量的预测问题 标注问题：输入变量与输出变量均为变量序列的预测问题 输入与输出的随机变量X和Y具有联合概率分布的假设是监督学习关于数据的基本假设。 模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间。 监督学习分为学习和预测两个过程，可用下图来描述： 监督学习可以概括如下：从给定有限的训练数据出发，假设数据是独立同分布的，而且假设模型属于某个假设空间，应用某一评价准则，从假设空间中选取一个最优的模型，使它对已给训练数据及未知测试数据在给定评价标准意义下有最准确的预测。 统计学习三要素统计学习方法 = 模型 + 策略 + 算法。 模型假设空间中的模型一般有无穷多个，假设空间用$\digamma$表示。 假设空间可以定义为决策函数的集合：$\digamma = \{f| Y = f(X)\}​$ ，$\digamma​$通常是由一个参数向量决定的函数族：$\digamma = \{f| Y = f_{\theta}(X), \theta \in R^n\}​$ 假设空间也可以定义为条件概率的结合：$\digamma = \{P| P(Y|X)\}$，$\digamma$通常是由一个参数向量决定的条件概率分布族：$\digamma = \{P| P_{\theta}(Y|X), \theta \in R^n\}​$ 策略损失函数和风险函数有了模型的假设空间，统计学习接着考虑的就是按什么样的准则学习。 损失函数：度量模型一次预测的好坏，如0-1损失函数、平方损失函数、绝对损失函数、对数损失函数。 风险函数：度量平均意义下模型预测的好坏。 损失函数的期望（风险函数）： R_{exp}(f) = E_p[L(Y, f(X))] = \int L(y, f(x))P(x, y)dxdy用P(X, Y)可以直接求出P(Y | X)，但我们不知道。 模型$f(X)$关于训练数据集的平均损失称为经验风险或经验损失： R_{emp}(f) = \frac1N \sum_{i=1}^N L(y_i, f(x_i))根据大数理论，当样本N趋于无穷时，$R_{emp}(f)$趋于$R_{exp}(f)$，一个很自然的想法是用$R_{emp}(f)$来估计$R_{exp}(f)$。但现实中训练样本有限，要对经验风险进行矫正，这就关系到监督学习的两个基本策略：经验风险最小化和结构风险最小化。 经验风险最小化和结构风险最小化按照经验风险最小化求最优模型就是求解最优化问题： min_{f \in \digamma} \frac1N \sum_{i=1}^N L(y_i, f(x_i)) 当样本容量很小时，经验风险最小化的效果未必很好，会产生过拟合（over-fitting），结构风险最小化由此提出。 结构风险的定义是： R_{srm}(f)= \frac1N \sum_{i=1}^N L(y_i, f(x_i)) + \lambda J(f)按照结构风险最小化求最优模型就是求解最优化问题： min_{f \in \digamma} \frac1N \sum_{i=1}^N L(y_i, f(x_i)) + \lambda J(f)这时，监督学习的问题就编程了经验风险或结构风险函数的最优化问题。 算法将统计学习问题归结为最优化问题之后，统计学习的算法成为求解最优化问题的算法。 模型评估与模型选择训练误差与测试误差训练误差是模型$Y = \hat f (X)​$关于训练集的平均损失： R_{emp}(\hat f) = \frac1N \sum_{i=1}^N L(y_i, \hat f(x_i))测试误差是模型$Y = \hat f (X)$关于测试集的平均损失： e_{test} = \frac1 {N'} \sum_{i=1}^{N'} L(y_i, \hat f(x_i))测试误差反应了学习方法对未知的测试数据集的预测能力。 过拟合与模型选择下图是训练误差和测试误差与模型复杂度的关系。当模型的复杂度增大时，训练误差会逐渐减小；而测试误差会先减小，达到最小值后又增大。 正则化与交叉验证正则化正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。 正则化一般具有如下形式： min_{f \in \digamma} \frac1N \sum_{i=1}^N L(y_i, f(x_i)) + \lambda J(f)其中，第1项是经验风险，第2项是正则化项，$\lambda \geq 0$为调整两者之间关系的系数。 回归问题中： L(w) = \frac1N \sum_{i=1}^N(f(x_i;w)-y_i)^2 + \frac {\lambda}2 ||w||^2 L(w) = \frac1N \sum_{i=1}^N(f(x_i;w)-y_i)^2 + {\lambda} ||w||_1交叉验证如果给定的样本数据充足，进行模型选择的一种简单方法是随机地将数据集分为三部分，分别为训练集、验证集和测试集。训练集用来训练模型，验证集用于模型的选择，而测试集用于最终对学习方法的评估。 但在数据不充足的情况下，为选择好的模型，可以使用交叉验证。交叉验证的基本想法是重复地使用数据，把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复地进行训练、测试以及模型选择。 简单交叉验证将数据分为训练集和测试集，然后用训练集在各种条件下训练模型从而得到不同模型，在测试集桑拿评价各个模型的测试误差，选出测试误差最小的模型。 S折交叉验证应用最多的是S折交叉验证。首先随机地将已给数据切分为S个互不相交的大小相同的子集，然后利用S-1个子集的数据训练模型，利用余下的子集测试模型，将这一过程对可能的S种选择重复进行，最后选出S次评测中平均测试误差最小的模型。 留一交叉验证S折交叉验证的特殊情形是S = N，称为留一交叉验证，往往在数据缺乏的情况下使用。这里的N是给定数据集的容量。 泛化能力泛化误差 R_{exp}(\hat f) = E_p[L(Y, \hat f(X))] = \int L(y, \hat f(x))P(x, y)dxdy可以通过比较泛化误差上界来比较学习方法的泛化能力。 泛化误差上界的性质：样本容量增加，泛化误差趋于0；假设空间容量越大，泛化误差越大。 定理（泛化误差上界）：对二分类问题，当假设空间是有限个函数的集合时$\digamma = \{f_1, f_2, …, f_d\}$，对任意一个函数$f \in \digamma$，至少以概率$1-\delta$，以下不等式成立： R(f) \leq \hat R(f) + \varepsilon (d, N, \delta) \varepsilon(d, N, \delta) = \sqrt {\frac 1{2N}(\log d+\log\frac1{\delta})}生成模型与判别模型监督学习方法可以分为生成方法和判别方法，所学到的模型分别称为生成模型和判别模型。 生成方法由数据学习联合概率分布P(X, Y)，然后求出条件概率分布P(Y | X)作为预测的模型，即生成模型： P(Y|X) = \frac {P(X, Y)}{P(X)}典型的生成模型由：朴素贝叶斯法和隐马尔可夫模型。 判别方法由数据直接学习决策函数f(X)或者条件概率分布P(Y | X)作为预测的模型，即判别模型。典型的判别模型包括：k近邻法、感知机、决策树、Logistic回归模型、最大熵模型、支持向量机、提升方法和条件随机场等。 各自的优缺点： 生成方法：可还原出联合概率分布P(X,Y),而判别方法不能。生成方法的收敛速度更快，当样本容量增加的时候，学到的模型可以更快地收敛于真实模型；当存在隐变量时，仍可以使用生成方法，而判别方法则不能用。 判别方法：直接学习到条件概率或决策函数，直接进行预测，往往学习的准确率更高；由于直接学习Y=f(X)或P(Y|X),可对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习过程。 分类问题二分类问题常用的评价指标是精确率(precision)和召回率(recall)。 精确率定义为 P = \frac {TP}{TP+FP}召回率定义为 R = \frac {TP}{TP+FN}其中：TP (true positive) 将正类预测为正类FN (false negative) 将正类预测为负类FP (false positive) 将负类预测为正类TN (true negative) 将负类预测为负类 $F_1$是精确率和召回率的调和均值，即 \frac 2{F_1} = \frac 1P+ \frac1R F_1 = \frac {2TP}{2TP+FP+FN}标注问题评价标注模型的指标与评价分类模型的指标一样，可以认为标记问题是分类问题的一个推广。 标注问题的输入是一个观测序列，输出的是一个标记序列或状态序列。也就是说，分类问题的输出是一个值，而标注问题输出是一个向量，向量的每个值属于一种标记类型。 标注常用的统计学习方法有：隐马尔可夫模型、条件随机场。 回归问题回归模型是表示输入变量到输出变量之间映射的函数，回归问题的学习等价于函数拟合。 最常用的损失函数是平方损失函数，在此情况下，回归问题可以由著名的最小二乘法求解。]]></content>
      <categories>
        <category>《统计学习方法》读书笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决package'' is not available(for R version 3.4.1)的问题]]></title>
    <url>%2Fposts%2F8933%2F</url>
    <content type="text"><![CDATA[在R中使用 install.packages(“ “)的时候有时会提示 package’’ is not available(for R version 3.4.1)，此时可使用以下的代码安装： 12source(&quot;http://bioconductor.org/biocLite.R&quot;)biocLite(&quot; &quot;)]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>R语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性可分支持向量机与硬间隔最大化]]></title>
    <url>%2Fposts%2F44274%2F</url>
    <content type="text"><![CDATA[支持向量机的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。 线性可分支持向量机 考虑一个二分类问题， 假设输入空间和特征空间是两个不同的空间。 输入空间：欧式空间或离散集合 特征空间：欧式空间或希尔伯特空间 线性可分支持向量机、线性支持向量机：假设这两个空间中的元素一一对应，并将输入空间 中的输入映射为特征空间中的特征向量 非线性支持向量机：利用一个从输入空间到特征空间的非线性映射将输入映射为特征向量 输入都由输入空间转换到特征空间，支持向量机的学习是在特征空间进行的。 假设给定一个特征空间上的训练数据集： T = {(x_1, y_1), (x_2, y_2), ...,(x_N, y_N)}样本分为正例和负例。 学习的目标：找到分离超平面 $w^{\star} \cdot x + b^{\star} = 0 $。 和感知机不同（利用误分类最小的策略求分离超平面，解有无穷个），线性可分支持向量机利用间隔最大化求最优分离超平面，解是唯一的。 线性可分支持向量机的决策函数为$f(x) = sign(w^{\star} \cdot x + b^{\star} )$。 其实，在二维特征空间的分类问题中，有许多直线能将两类数据正确划分，线性可分支持向量机对应着将两类数据正确划分并且间隔最大的直线。 函数间隔和几何间隔函数间隔定义超平面($w, b$)关于样本点($x_i, y_i$)的函数间隔为 $\hat\gamma_i = y_i(w\cdot x_i +b)$。 定义超平面($w, b$)关于训练集$T$的函数间隔为 $\hat\gamma = min_{i=1,…N} \quad \hat \gamma_i$。 函数间隔表示分类预测的正确性和确信度。 几何间隔如下图所示，点A与超平面($w, b$)的距离由线段AB给出，记作$\gamma_i$： \gamma_i =y_i (\frac w {||w||} \cdot x_i +\frac b {||w||}) 定义超平面($w, b$)关于训练集$T$的几何间隔为 $\gamma = min_{i=1,…N} \quad \gamma_i$。 函数间隔和几何间隔的关系 样本点的几何间隔 \gamma_i = \frac{\hat\gamma_i}{||w||} 训练集的几何间隔 \gamma = \frac{\hat\gamma}{||w||}如果超平面参数w和b成比例地改变（超平面没有变），函数间隔也按此比例改变，而几何间隔不变。 间隔最大化最大间隔分离超平面具体地，求一个几何间隔最大的分离超平面，即最大间隔分离超平面可以表示为下面的约束最优化问题： max_{w, b} \quad \gamma \\\\ s.t. \qquad y_i (\frac w {||w||} \cdot x_i +\frac b {||w||})\geq \gamma \quad i = 1,2,...,N根据几何间隔和函数间隔的关系： max_{w, b}\quad \frac{\hat\gamma}{||w||} s.t. \qquad y_i ( w \cdot x_i + b)\geq \hat\gamma \quad i = 1,2,...,N函数间隔 $\hat\gamma$的取值不影响最优化问题的解，可以取 $\hat \gamma = 1$。 最大化 $\frac1{||w||}$和最小化 $||w||$是等价的，为了求导方便，我们将其等价为最小化 $\frac12 ||w^2||$。 则线性可分支持向量机的最优化问题如下，这是一个求解最优解$w^{\star}, b^{\star}$的凸二次规划问题： max_{w, b}\quad \frac12 ||w^2|| s.t. \qquad y_i ( w \cdot x_i + b)-1\geq 0 \quad i = 1,2,...,N 补充：凸优化问题是指约束最优化问题： min_w \quad f(w)\\ s.t. \quad g_i(w) \leq 0, \quad i = 1,2,...,k\\ \ h_i(w) = 0, i=1,2,...,l其中，目标函数$f(x)$和约束函数$g_i(w)$都是$R^n$上连续可微的凸函数，约束函数$h_i(w)$是$R^n$上的仿射函数。 当目标函数为二次函数且约束函数是仿射函数时，凸最优化问题成为凸二次规划问题。 支持向量和间隔边界在线性可分情况下，训练集的样本点中与分离超平面距离最近的样本点的实例称为支持向量。 支持向量使得约束条件式等号成立，即$y_i(wx_x+b)-1=0$。 正例：$H_1: \quad w\cdot x+b=-1$ 负例：$H_2: \quad w\cdot x+b = -1$ 如下图所示： $H_1$与$H_2$平行，它们之间形成一条长带，分离超平面位于它们中央且与之平行。长带的宽度称为间隔，等于$\frac 2 {||w||}$，$H_1$和$H_2$称为间隔边界。 在决定分离超平面时只有支持向量起作用，而其他实例点并不起作用。 线性可分支持向量机学习算法——最大间隔法输入：线性可分训练数据集线性可分训练数据集 $T={(x_1, y_1), (x_2, y_2),…,(x_N, y_N)}$，其中 $x_i \in R^n$，$y_i \in \{-1, 1\}$，$i=1,2,…,N$。 输出：最大间隔分离超平面和分类决策函数 构造并求解约束最优化问题： max_{w, b}\quad \frac12 ||w^2|| s.t. \qquad y_i ( w \cdot x_i + b)-1\geq 0 \quad i = 1,2,...,N得到最优解$w^{\star} , b^{\star}$。 得到分离超平面及分类决策函数： w^{\star} \cdot x + b^{\star} = 0 f(x) = sign(w^{\star} \cdot x +b^{\star})给出《统计学习方法》中的一个例子： 学习的对偶算法关于拉格朗日对偶的介绍可以参考我的上一篇笔记。应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解。 这样做有两个优点： 对偶问题往往更容易求解 自然引入核函数，进而推广到非线性分类问题 首先构建拉格朗日函数，为每一个不等式约束引进拉格朗日乘子$\alpha_i \geq 0, i=1,2,…,N$： \begin{align*} \\ & L \left( w, b, \alpha \right) = \dfrac{1}{2} \| w \|^{2} + \sum_{i=1}^{N} \alpha_{i} \left[- y_{i} \left( w \cdot x_{i} + b \right) + 1 \right] \\ & = \dfrac{1}{2} \| w \|^{2} - \sum_{i=1}^{N} \alpha_{i} y_{i} \left( w \cdot x_{i} + b \right) + \sum_{i=1}^{N} \alpha_{i} \end{align*}根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题： max_{\alpha}min_{w,b}L(w, b, \alpha)为了得到对偶问题的解，需要先求$L(w,b,\alpha)$对$w,b$的极小，再求对$\alpha$的极大。 求极小将拉格朗日函数$L(w,b,\alpha)$分别对$w, b$求偏导数并令其等于0： \begin{align*} \\ & \nabla _{w} L \left( w, b, \alpha \right) = w - \sum_{i=1}^{N} \alpha_{i} y_{i} x_{i} = 0 \\ & \nabla _{b} L \left( w, b, \alpha \right) = -\sum_{i=1}^{N} \alpha_{i} y_{i} = 0 \end{align*}得 \begin{align*} \\ & w ＝ \sum_{i=1}^{N} \alpha_{i} y_{i} x_{i} \\ & \sum_{i=1}^{N} \alpha_{i} y_{i} = 0 \end{align*}代入拉格朗日函数，得 \begin{align*} \\ & L \left( w, b, \alpha \right) = \dfrac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} \left( x_{i} \cdot x_{j} \right) - \sum_{i=1}^{N} \alpha_{i} y_{i} \left[ \left( \sum_{j=1}^{N} \alpha_{j} y_{j} x_{j} \right) \cdot x_{i} + b \right] + \sum_{i=1}^{N} \alpha_{i} \\ & = - \dfrac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} \left( x_{i} \cdot x_{j} \right) - \sum_{i=1}^{N} \alpha_{i} y_{i} b + \sum_{i=1}^{N} \alpha_{i} \\ & = - \dfrac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} \left( x_{i} \cdot x_{j} \right) + \sum_{i=1}^{N} \alpha_{i} \end{align*}即 \begin{align*} \\ & \min_{w,b}L \left( w, b, \alpha \right) = - \dfrac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} \left( x_{i} \cdot x_{j} \right) + \sum_{i=1}^{N} \alpha_{i} \end{align*}求极大求$max_{\alpha}min_{w,b}L(w, b, \alpha)​$，即是对偶问题： \begin{align*} \\ & \max_{\alpha} - \dfrac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} \left( x_{i} \cdot x_{j} \right) + \sum_{i=1}^{N} \alpha_{i} \\ & s.t. \sum_{i=1}^{N} \alpha_{i} y_{i} = 0 \\ & \alpha_{i} \geq 0, \quad i=1,2, \cdots, N \end{align*}将上式由求极大转换成求极小，得到下面与之等价的对偶最优化问题： \begin{align*} \\ & \min_{\alpha} \dfrac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} \left( x_{i} \cdot x_{j} \right) - \sum_{i=1}^{N} \alpha_{i} \\ & s.t. \sum_{i=1}^{N} \alpha_{i} y_{i} = 0 \\ & \alpha_{i} \geq 0, \quad i=1,2, \cdots, N \end{align*}对线性可分训练数据集，假设上面对偶最优化问题对$\alpha​$的解为$\alpha^{\star} = \left( \alpha_{1}^{\star}, \alpha_{1}^{\star}, \cdots, \alpha_{N}^{\star} \right)^T​$，可以由$\alpha^{\star} ​$求得原始最优化问题对(w, b)的解$w^{\star}, b^{\star}​$。有下面的定理： \begin{align*} \\ & w^{*} = \sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i} \end{align*} \begin{align*} \\ & b^{*} = y_{j} - \sum_{i=1}^{N} \alpha_{i}^{*} y_{i} \left( x_{i} \cdot x_{j} \right) \end{align*}线性可分支持向量机学习算法输入：线性可分训练数据集线性可分训练数据集 $T={(x_1, y_1), (x_2, y_2),…,(x_N, y_N)}​$，其中 $x_i \in R^n​$，$y_i \in \{-1, 1\}​$，$i=1,2,…,N​$。 输出：分离超平面和分类决策函数 构造并求解约束最优化问题： \begin{align*} \\ & \min_{\alpha} \dfrac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} \left( x_{i} \cdot x_{j} \right) - \sum_{i=1}^{N} \alpha_{i} \\ & s.t. \sum_{i=1}^{N} \alpha_{i} y_{i} = 0 \\ & \alpha_{i} \geq 0, \quad i=1,2, \cdots, N \end{align*}​ 得到最优解$\alpha^{\star} = \left( \alpha_{1}^{\star}, \alpha_{1}^{\star}, \cdots, \alpha_{N}^{\star} \right)​$。 计算 \begin{align*} \\ & w^{*} = \sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i} \end{align*}并选择$\alpha^{\star} $的一个正分量$\alpha_{j}^{\star} &gt; 0$，计算 \begin{align*} \\ & b^{*} = y_{j} - \sum_{i=1}^{N} \alpha_{i}^{*} y_{i} \left( x_{i} \cdot x_{j} \right) \end{align*} 求得分类超平面： \begin{align*} \\ & w^{*} \cdot x + b^{*} = 0 \end{align*}分类决策函数： \begin{align*} \\& f \left( x \right) = sign \left( w^{*} \cdot x + b^{*} \right) \end{align*} 同样给出一个对偶形式算法的例子，训练数据和上一个例子相同：]]></content>
      <categories>
        <category>《统计学习方法》读书笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo添加Aplayer播放器]]></title>
    <url>%2Fposts%2F30334%2F</url>
    <content type="text"><![CDATA[感谢开源！！让我们用上了这么好的插件！ 创建歌单页面由于我想在单独的页面加入歌单，所以额外创了个页面，也可以直接在文章中插入，原理都是一样的。 新建页面，命名为playlist： 1hexo new page playlist 这时候在 /Hexo/source 文件夹下会生成一个playlist文件夹，打开里面的index.md，修改如下： 123title: 歌单date: 2019-02-21 16:14:00type: &quot;playlist&quot; 打开主题的 _config.yml文件，在menu下新建一个名为playlist的类（注意这里使用的图标是图标库中的图标，网址为 http://www.fontawesome.com.cn/faicons/ 。可以选择自己喜欢的图标，我这里选择的是music）。完成后如下所示： 1234567menu: home: / || home categories: /categories/ || th tags: /tags/ || tags archives: /archives/ || archive playlist: /playlist/ || music about: /about/ || user 打开/Hexo/themes/hexo-theme-next/languages/zh-Hans.yml，添加对应的中文翻译： 12menu: playlist: 歌单 这样歌单就创建完成啦~ 使用 hexo-tag-aplayer 插件 hexo-tag-aplayer 是Aplayer在hexo上的插件，这里的配置参考的是官方文档 ，第一步安装 hexo-tag-aplayer： 1npm install --save hexo-tag-aplayer 最新版的 hexo-tag-aplayer 已经支持了MetingJS的使用，可以直接解析网络平台的歌曲（简直是神器），首先要在站点配置文件中开启meting模式，添加以下代码在配置文件的最后： 12aplayer: meting: true 在markdown文件中使用： 1&#123;% meting &quot;6726552355&quot; &quot;tencent&quot; &quot;playlist&quot; &quot;theme:#FF4081&quot; &quot;mode:circulation&quot; &quot;mutex:true&quot; &quot;listmaxheight:340px&quot; &quot;preload:auto&quot; %&#125; 效果还是很不错的：]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建Jupyter Notebook远程云服务器]]></title>
    <url>%2Fposts%2F37756%2F</url>
    <content type="text"><![CDATA[将Jupyter部署到课题组的深度学习服务器上，主要的考虑是可以避免在不同电脑上使用jupyter时数据不一致的问题，而且能够完美利用服务器的性能。 完成后的界面如下： 还是可以的，等之后有时间再把本地电脑上的那些插件都装上就完美啦~ 安装过程1、安装Jupyter Notebook1pip install jupyter 2、生成配置文件1jupyter notebook --generate-config 3、设置密码打开Python终端，输入 1234In [1]: from IPython.lib import passwdIn [2]: passwd()Enter password: Verify password: 4、设置配置文件1vim /home/zsh/jupyter/jupyter_notebook_config.py 在末尾增加配置信息（直接修改配置文件也可以，但这样看着更简洁，且便于之后的修改） 12345c.NotebookApp.ip = &apos;0.0.0.0&apos; # 所有IP都可以访问c.NotebookApp.port = 8888 # 默认的端口是8888c.NotebookApp.open_browser = False # 禁止在服务器上打开jupyterc.NotebookApp.notebook_dir = &apos;/home/zsh/jupyter&apos; # 设置Jupyter的根目录c.NotebookApp.allow_root = True # 以root权限启动jupyter 5、启动Jupyter Notebook1jupyter notebook 这样就搭建成功啦，可以在本地的浏览器上输入 服务器的ip地址:8888，这样就可以远程打开jupyter了。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令使用纪录]]></title>
    <url>%2Fposts%2F37930%2F</url>
    <content type="text"><![CDATA[ssh登陆的命令行方法： 1ssh amax@10.10.1.110 显示所有文件： 1ls -a 复制文件夹下的所有内容： 1cp -r dir1 dir2 删除文件夹： 1rm -rf dir 临时更改pip源 1sudo python -m pip install pyradiomics -i https://pypi.tuna.tsinghua.edu.cn/simple 安装g++： 1aptitude install build-essential 查找： 1find -name &apos;google*&apos; 解压tar.xz压缩包： 1tar -xvf nameofpackage.tar.xz 杀进程： 1killall -9 R 查看IP： 1hostname -I 查看各种版本： 123cat /usr/local/cuda/version.txt # 查看CUDA版本cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 # 查看CUDNN版本cat /etc/issue # 查看Ubuntu版本 切换用户： 12sudo su root # 切换root用户su user # 切换普通用户，也可以使用 sudo -i]]></content>
      <categories>
        <category>Linux命令使用纪录</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[调整hexo页面宽度]]></title>
    <url>%2Fposts%2F13179%2F</url>
    <content type="text"><![CDATA[博客在浏览器上的留白太多，因此想增加文章的宽度。对于Pisces Scheme，修改页面宽度的方式与其他三个主题不太一样，因此列出Pisces Scheme的修改方式。 打开/Hexo/themes/hexo-theme-next/source//css/_variables/custom.styl 添加两行代码即可： 12$main-desktop = 1200px $content-desktop = 900px]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用DNSPOD进行域名解析]]></title>
    <url>%2Fposts%2F27735%2F</url>
    <content type="text"><![CDATA[目前的域名解析有两种方法，一种是直接利用域名注册商的解析，另一种就是更换DNS用第三方的解析。为了便于管理，也为了更好的速度，将域名的解析方式由namesilo默认的解析方式改为使用国内的DNSPOD进行解析。 首先在namesilo上更换域名DNS（即Change Name Servers），将原来默认的Name Servers改为 12f1g1ns1.dnspod.netf1g1ns1.dnspod.net 删除默认的A记录和CNAME记录，等待DNSPOD的免费DNS更新完毕后就可以放心地关闭namesilo了。 然后打开DNSPOD控制台，添加域名即可。由于本博客是用Github托管的，因此这里记录了连接到Github博客的设置，也可以连接到自己的VPS，IP改为自己的VPS地址即可。 123@ A 192.30.252.153@ A 192.30.252.154www CNAME shenghaishxt.github.io A： 用来指定域名为 IPv4 的地址（如：8.8.8.8），如果需要将域名指向一个IP地址，就需要添加 A 记录。CNAME： 如果需要将域名指向另一个域名，再由另一个域名提供 ip 地址，就需要添加 CNAME 记录。 最后在本地的/Hexo/source文件夹下新建CNAME文件，注意不要带任何后缀，如果想要自己的网站是带有www的二级域名，那么就输入带有www的域名，否则的话直接添加自己购买的域名就好。我这里添加的是带有www的二级域名。 1www.zhangshenghai.com 接下来就等待DNS的解析啦，不知道为什么我的解析好像挺慢的。十几分钟后zhangshenghai.com这个域名是可以ping通，但加了www的域名就总是ping不通。貌似是过了一个晚上，我第二天打开才可以登陆上。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>DNS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CHAPTER10 线性规划在近似算法中的应用]]></title>
    <url>%2Fposts%2F20007%2F</url>
    <content type="text"><![CDATA[使用线性规划来近似顶点覆盖问题ILP（整数线性规划）对于每一个顶点$v\in V$，有$x(v) \in \{0, 1\}$，$x(v)=1$意为v在顶点覆盖中，$x(v)=0$意为v不再顶点覆盖中。那么，对于顶点覆盖问题的任意边$(u, v)$，u和v至少有一个必须在顶点覆盖中，即$x(u)+x(v)\geq1$。这样就引出了以下用于寻找最小顶点覆盖的0-1整数规划 。 min \sum_{v\in V} x(v)\\ s.t.\qquad x(u)+x(v) \geq 1 \quad \forall (u,v)\in E \\ \quad x(v)\in \{0, 1\} \quad \forall v \in Vrelax to LP（线性规划松弛）假设去掉了$x(v) \in \{0, 1\}​$这一限制，并代之以$0\leq x(v) \leq 1​$，就可以得到以下的线性规划，称为线性规划松弛。 min \sum_{v\in V} x(v)\\ s.t.\qquad x(u)+x(v) \geq 1 \quad \forall (u,v)\in E \\ \quad 0\leq x(v) \leq 1 \quad \forall v \in VRounding（使用Rounding的方法来构造近似算法）对于每一个顶点v，都会求得一个$x(v)$的值$x^*(v)$，对$x(v)$做以下的rounding： 若$x^*(v)\geq 1/2$，则将该顶点加入到点覆盖集合中（$x(v)= 1$），否则舍去顶点v（$x(v) = 0​$），直至图中的所有顶点处理完毕。 由以上算法可看出： 故此算法是一个近似度为2的近似算法。]]></content>
      <categories>
        <category>算法设计与分析</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CHAPTER9 线性规划]]></title>
    <url>%2Fposts%2F55620%2F</url>
    <content type="text"><![CDATA[线性规划概述在给定有限的资源和竞争约束情况下，很多问题都可以表达为最大化或最小化某个目标。如果可以把目标指定为某些变量的一个线性函数，而且如果可以将资源的约束指定为这些变量的等式或不等式，则得到一个线性规划问题（Linear-Programming Problem）。 在求解线性规划时又两种有用的格式：标准型和松弛型。在标准型中所有的约束都是不等式，而在松弛型中所有的约束都是等式。 下面给出一个将实际问题转换为线性规划形式的例子。 有m种不同的食物$F_1, …, F_m$，这些食物能够提供n种营养$N_1, …, N_n$，营养$N_j$每天的最低需求量是$c_j$，$b_i$是$F_i$的单位价格。$a_{ij}$代表食物$F_i$单位体积所含的营养$N_j$。问题是求在满足营养需求下的最小花费。 假设每种食物的数量为$x_i$，则使用线性规划的形式可表示为： min \sum_i b_i x_i \\ s.t. \sum_i a_{ij}x_i \geq c_j这个问题的目标是求满足营养需求的条件下最小化价格，接下来我们会看到，其实它的对偶问题就是最大化营养的需求量。 单纯形算法解决线性规划问题主要用三种算法： 单纯形算法：指数时间的复杂度，但是在实际中应用广泛，当它被精心实现时，通常能够快速地解决一般的线性规划问题。 椭圆算法：第一个指数时间算法，但是在实际中运行缓慢。 内点法：在理论和实际中都能比较有效率地解决线性规划问题。 本章我们主要讨论在实际问题中应用广泛的单纯形算法。 首先从一个例子开始，考虑下列松弛型的线性规划并将等式重写后可得到一个tableau： 现在，$x_3, x_4, x_5$是基本解，令$x_1=x_2=0$，可得$x_3=1, x_4=3, x_5=2$，且$z=0$。 我们当然希望改善$z$的值，很明显需要增加$x_1$或者$x_2$。令$x_1=0$，由于$x_1, …, x_5 \geq 0$，$x_2$最大可取至1，此时$x_3=0$。现在基本解变为$x_2, x_4, x_5$，重写tableau： 重复上面的过程，为了增加$z$的值，我们可以增加$x_1$（由于$x_3$的系数是负数，因此增加$x_3$是无效的）。令$x_3=0$，$x_1$最大可取至1，此时$x_5=0$。这时基本解变为$x_1, x_2, x_4$，重写tableau： 重复上面的过程，为了增加$z$的值，我们可以增加$x_3$（由于$x_5$的系数是负数，因此增加$x_5$是无效的）。令$x_5=0$，$x_3$最大可取至2，此时$x_4$变为0。这时基本解变为$x_1, x_2, x_3$，重写tableau： 此时可看出$z$的取值已达最优，因此解是$x_1=3, x_2=2, x_3=2, x_4=0, x_5=0$，目标值$z=5$。 对偶性对偶性是个非常重要的性质。在一个最优化问题中，一个对偶问题的识别几乎总是伴随着一个多项式时间算法的发现。 在线性规划的形式下，对偶问题可互相转化，具体如下图所示： 下面给出一个实际例子，照着原问题的线性规划形式，我们即可写出对偶问题的线性规划形式。 原问题有多少个未知数，对偶问题就有多少个式子；原问题有多少个式子，对偶问题就有多少个未知数。 如下图所示，原问题给出了对偶问题的可行解的下界，对偶问题给出了原问题的可行解的上界。 总结几个经典的对偶问题： 最大流的对偶问题是最小割 最大匹配的对偶问题是最小顶点覆盖 最优匹配的对偶问题是最小定标和 最大流与最小割的线性规划表示最大流满足两个性质：反对称性、容量限制。最大流是满足这两个约束和最大化流量值的流，其中流量值是从源流出的总流量。因此，流满足线性约束，且流的值是一个线性函数。可以将最大流问题表示为线性规划并作如下的转换： 其对偶问题的实际意义是最小割：]]></content>
      <categories>
        <category>算法设计与分析</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CHAPTER8 近似算法]]></title>
    <url>%2Fposts%2F63466%2F</url>
    <content type="text"><![CDATA[目前，所有的NP完全问题都没有能够在多项式时间内求解的算法，我们通常可以采用以下几种解题策略： 只对问题的特殊实例求解 用动态规划法或分支限界法求解 用概率算法求解 只求近似解 用启发式方法求解 本节主要讨论的是解NP完全问题的近似算法。 近似算法的性能若一个最优化问题的最优值为$C^{OPT}$，求解该问题的一个近似算法的一个近似最优解相应的目标函数值为$C$，则将近似算法的性能比定义为： \eta = max({\frac{C}{C^{OPT}}, \frac{C^{OPT}}{C}})通常情况下，该性能比是问题输入规模$n$的一个函数$\rho(n)$，即 max({\frac{C}{C^{OPT}}, \frac{C^{OPT}}{C}}) \leq \rho(n)顶点覆盖问题的近似算法问题描述无向图$G=(V,E)$的顶点覆盖是它的顶点集$V$的一个子集$V’⊆V$，使得若$(u,v)$是$G$的一条边，则$v∈V’$或$u∈V’$。顶点覆盖V’的大小是它所包含的顶点个数$|V’|$。 下面给出一个近似比为2的算法的伪代码： 123456789101112VertexSet approxVextexCover(Graph g)&#123; cset = NULL; e = g.e; while (e != NULL) &#123; 从e中任取一条边(u, v); 将顶点u,v加入cset; 从e中删去与u和v相关联的边; &#125; return cset;&#125; 算法运行过程下图是《算法导论》中顶点覆盖问题近似算法的图例，说明了算法的运行过程和结果。 图(e)表示近似算法产生的近似最优顶点覆盖cset，它由顶点b,c,d,e,f,g所组成。图(f)是图G的一个最小顶点覆盖，它只含有3个顶点：b,d和e。 性能分析假定算法选取的边集为A，则返回的顶点个数为2A。即$|C| = 2|A|$。图G的任一顶点覆盖都至少包含A中各条边中的一个顶点，即$|C^{OPT}| \geq |A|$。 则 \rho = \frac{|C|}{|C^{OPT}|} \leq 2旅行商问题的近似算法问题描述给定一个完全无向图$G=(V,E)$，其每一边$(u,v)∈E$有一非负整数费用$c(u,v)$。要找出$G$的最小费用哈密顿回路。 费用函数c往往具有三角不等式性质，即对任意的3个顶点$u,v,w∈V$，有：$c(u,w)≤c(u,v)+c(v,w)$。 在费用函数不一定满足三角不等式的一般情况下，不存在具有常数性能比的解TSP问题的多项式时间近似算法，除非$P=NP$。换句话说，若$P≠NP$，则对任意常数$ρ&gt;1$，不存在性能比为ρ的解决旅行售货员问题的多项式时间近似算法。 下面给出一个解决满足三角不等式的旅行商问题的近似算法伪代码： 123456APPROX-TSP-TOUR(G, c) 任意选择V中的一个顶点r，作为树根节点 调用Prim算法得到图G的最小生成树T 先序遍历T，得到顶点序列L 删除L中的重复顶点形成哈密顿环C 输出C 算法运行过程下图是APPROX-TSP-TOUR的操作过程，(a)示出了给定点的集合，(b)示出了一个最小生成树T，它是由MST-PRIM计算出来的，根为a节点，(c)是对T进行先序遍历时的顶点序列，(d)是近似算法得到的路线。 性能分析假设$H ^ {opt}​$是一个最优游程，如图e所示。由于我们通过删除一个游程路线中的任一边而得到一棵生成树，故最小生成树$T​$的权值是最优游程代价的一个下界，即$c(T) \leq c(H^{opt})​$。 假设图c中的遍历的代价为$c(W)$，该遍历经过了$T$的每条边两次，则有$c(W) = 2c(T)$，两式联立有$c(W) \leq 2c(H^{opt})$。由于$H$是从完全遍历$W$中删除了某些顶点得到的，故有$c(H) \leq c(W)$，则$c(H) \leq2c(H^{opt})$。 则 \frac{C(H)}{C({H^{opt}})} \leq 2]]></content>
      <categories>
        <category>算法设计与分析</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CHAPTER7 NP完全性]]></title>
    <url>%2Fposts%2F64398%2F</url>
    <content type="text"><![CDATA[P问题、NP问题、NPC问题的概念老师上课时强调过本章对于概念要特别清楚，因此首先给出这三个问题的概念： P问题：能够在多项式时间内解决的问题。 NP问题：能够在多项式时间内验证一个解的正确性的问题。 NPC问题：当一个问题满足下面两个条件的时候，那么这个问题是NPC问题。 首先，它是NP问题。 其次，所有其他的NP问题都能够在多项式时间内归约到此问题上（NP-hard）。 NPC问题是NP问题的子集。 NP-hard问题：问题A不一定是一个NP问题，但是所有的NPC问题都可以在多项式时间内转化为A，则称A为NP-hard问题。 NPC问题一定是NP-hard问题。 一些典型的NP完全问题通过问题变换的技巧，可以将2个不同问题的计算复杂性联系在一起。这样就可以将一个问题的计算复杂性归约为另一个问题的计算复杂性，从而实现问题的计算复杂性归约。 证明一个问题是NP完全问题分为两个步骤： 证明该问题是NP问题。 证明NP问题中的每一个问题都能在多项式时间内归约为该问题。 由于多项式问题具有传递性，因此只需证明一个已知的NP完全问题能够在多项式时间内归约到该问题即可。 下图给出了进行NP完全证明的结构，树的根为CIRCUIT-SAT。 电路可满足性问题（CIRCUIT-SAT）由《算法导论》第二版引理34.5：电路可满足性问题属于NP类，以及引理34.6：电路可满足性问题是NP难度的，结合NP完全性的定义可直接推出结论： 电路可满足性问题是NP完全的。 合取范式的可满足性问题（SAT）证明： SAT ∈ NP: 给定该问题的一个实例，用证书y={1, 1, 1}作为输入，对于给定的布尔公式x，我们都能按照布尔公式的数学运算规则在多项式时间内求解出来。 在多项式时间内将Circuit-SAT归约到SAT： 将电路中的输入用布尔变元表示，将电路中的每一个逻辑门用布尔公式来对应，就可以将Circuit-SAT问题归约到SAT问题。 显然，这是在多项式时间内完成的，因此SAT是NP完全问题。 三元合取范式的可满足性问题（3-CNF-SAT） 在这个问题的证明之前先补充一下合取范式（CNF）的定义。 如果一个布尔公式可表示为所有字句的“与”，且每个字句都是一个或多个文字的“或”，则称该布尔公式为合取范式（CNF）。 如果公式中的每个字句恰好都有三个不同的文字，则称该布尔公式为3-CNF。 例如，布尔公式 (x_1 \vee -x_1 \vee -x_2)\wedge(x_3 \vee x_2 \vee x_4)\wedge(-x_1 \vee -x3 \vee -x_4)就是一个3-CNF，其三个字句中的第一个为$(x_1 \vee -x_1 \vee -x_2)$，它包含3个文字$x_1$，$- x_1$，$-x_2$。 证明： 3-CNF-SAT ∈ NP： 这里的证明与上面的SAT问题是类似的。同样给定一个证书y作为输入，对于一个给定的合取范式，都可以在多项式时间内验证合取范式的值是1还是0。 在多项式时间内将SAT归约为3-CNF-SAT： 由于这里的证明对离散数学的要求较高，且老师上课时也没有细讲这部分内容，故只给出步骤，详细的证明可参考《算法导论》第二版定理34.10。 Ⅰ. 为输入公式画出一棵二叉“语法分析”树，文字作为树叶，连接词作为内部顶点。 Ⅱ. 把每个字句变换为合取范式。 Ⅲ. 继续对公式进行变换，使每个字句恰好有三个不同的文字。 从上面的步骤可以看出，SAT可以在多项式时间内归约到3-CNF-SAT，因此3-CNF-SAT是NP完全问题。 团问题（CLIQUE）证明： CLIQUE∈NP： 对于给定的图G(V, E)，如果给定顶点集V’作为证书，我们可以验证对于任意一对顶点u, v∈V’, 通过检查边(u, v)是否属于E，从而验证V’是否是一个团。显然，这是在多项式时间内完成的。 在多项式时间内将3-CNF-SAT归约到CLIQUE： 给定一个含有k个字句的3-CNF，假定其是可满足的，即3-CNF的结果为1。我们总是可以构造一个3k个顶点的图，构造方法是在不同的三元组、且不是自己的非的节点之间连线。 一共有k个分组，每个分组中的节点之间不能互相连接，而且这个3-CNF是可满足的。那么每个分组都会有一个节点与其他分组的节点相连，将每个分组的选出的那个节点互相连接起来，就是最大团。显然，最大团有k个节点。 下面给出《算法导论》中的一个k=3的实例，图中浅色的节点为每个分组中选出的节点。即3-CNF-SAT的一个可满足赋值为$x_2=0, x_3=1, x_1=0 或 1$。 归约过程在多项式内可以完成，因此团问题是NP完全问题。 顶点覆盖问题（VERTEX-COVER）证明： VERTEXT-COVER∈NP： 对于给定的图G(V, E)，如果给定顶点集V’作为证书，对于每条边(u, v)∈E，我们可以检查是否有u∈V’或v∈V’。这一验证在多项式时间内即可完成。 在多项式时间内将CLIQUE归约到VERTEX-COVER： 设G=(V, E)是CLIQUE的一个实例，设G的最大团为V’，取图G的补图，设补图上的边为E’，补图上的点为V-V’，那么V-V’是一个顶点覆盖。 原理：从E’中取任意的一条边(u, v)，那么在G中，边(u, v)是不存在的，那么u或v至少有一个在V-V’中。由于边(u, v)是任意取自E’的，即在补图中，任意一条边上都至少有一个点是属于V-V’的，即满足顶点覆盖的定义。 下面给出一个例子，V’ = {u, v, x, y}，V-V’ = {z, w}。 同样，这个归约过程能够在多项式时间内完成，因此顶点覆盖问题是NP完全问题。 哈密顿回路问题（HAM-CYCLE）证明： DIR-HAM-CYCLE∈NP： 对于一个给定的图G(V, E)，我们可以简单验证一个顶点序列是否经过所有顶点一次且仅一次，而且最后能够回到源点。这个过程显然是在多项式时间内完成的。 在多项式时间内将3-CNF-SAT归约到DIR-HAM-CYCLE： 构造一个能够表达3-CNF中的文字和子句的图结构，每行中的节点代表3-CNF中的每个文字，如第一行中的节点都代表了x1，那么跨行之间的节点之间即可代表3-CNF中的字句。 如果不清楚文字和字句的概念，可以参考上面3-CNF中关于合取范式的定义。 首先进行一个定义：当$x_i=1$时，遍历的顺序是从左到右，当$x_i=0$时，遍历的顺序是从右到左。从最上面的源点出发，以某种方式连接这些点以满足3-CNF，只要满足图中的3-CNF，那么这个图就是有向哈密顿回路。这个过程是在多项式时间内完成的。 接下来还要进行另外一个归约，才可以达到我们的目标： HAM-CYCLE∈NP： 与DIR-HAM-CYCLE的验证方法相同，这里不再赘述。 在多项式时间内将DIR-HAM-CYCLE归约到HAM-CYCLE： 给定一个具有n个顶点的有向图G=(V, E)，可以在多项式时间内构建一个具有3n个顶点的无向图G’ 以上两个归约都是在多项式时间内完成的，故哈密顿回路是NP完全的。 旅行商问题（TSP）证明： TSP∈ NP： 给定该问题的一个实例，用n个顶点组成的回路作为证书，我们可以验证该回路是否只包含每个顶点一次，并且检查各边费用之和是否小于k。这个过程能够在多项式时间内完成。 在多项式时间内将哈密顿回路归约到TSP： 设G=(V, E)是HAM-CYCLE的一个实例，可以构造对应的一个TSP实例。建立一个完全图G‘=(V, E’)，定义费用函数c为： 然后求解完全图G’上最大限定花费为0的路线即可。 下面来说明当且仅当图G’中有一个费用至多为0的回路的时候，G中才具有一个哈密顿回路： 假定图G中有一个哈密顿回路h。h中的每条边都属于E，因此在G’中的费用为0。因此，h在G‘中是费用为0的回路。 反之，假定图G’中有一个费用h‘至多为0的回路，回路上每条边的费用必为0。因此，h’仅包含E中的边。 这样，我们就得出结论，h’是图G中的一个哈密顿回路。 上述归约过程在多项式时间内完成，故旅行商问题是NP完全的。]]></content>
      <categories>
        <category>算法设计与分析</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CHAPTER6 动态规划]]></title>
    <url>%2Fposts%2F40122%2F</url>
    <content type="text"><![CDATA[概念分治法将问题划分成一些独立的子问题，递归地求解各子问题，然后合并子问题的解而得到原问题的解。 动态规划适用于子问题不是独立的情况，也就是个子问题包含公共的子子问题。动态规划对每个子子问题只求解一次，将其结果保存在一张表中，从而避免每次遇到各个子问题时重新计算答案。 动态规划的算法可分为以下4个步骤： 描述最优解的结构。 递归定义最优解的值。 按自底向上的方式计算最优解的值。 由计算的结果构造一个最优解。 0-1背包问题假设有n个物品，它们的重量分别为$w_1, w_2, …, w_n$，价值分别为$v_1, v_2, …, v_n$，给一承重为W的背包，求装入的物品具有最大的价值总和。 首先给出利用动态规划计算0-1背包问题的递归式： 在这个递归式中，$OPT(i, W)$表示前i件物品放入容量为W的背包中的最大价值。 初始化时，$OPT(0, j) = 0$，意为当没有物品放入时，不管背包容量多少，其最大价值为0；$OPT(i, 0)$意为当背包容量为0时，不管从前i件物品中怎么取，最大价值都是0。 接着进行动态规划，在已知$f(i-1, j)​$时，即已知在前$i-1​$件物品放入容量为j的背包时的最大价值情况下，求$f(i, j)​$。 在求$f(i, j)$时，首先判断物品i的重量是否超过目前背包的重量j，如果超过，则这个物品i放不进背包，则$f(i, j) = f(i-1, j)$。如果背包可以放下物品i，则尝试把背包中的重量减去物品i的重量$w_i$，这样$f(i-1, j-w_i)$表示前i-1件物品在背包容量为$i-w_i$下的最大价值，此时如果放入物品i，那么价值就变为$f(i-1, j-w_i)+v_i$。判断$f(i-1, j-w_i)+v_i$与$f(i-1, j)$的大小，选择较大的一个作为在背包容量为i下的最大价值。以上就是对于这个递归式的算法描述。 相对应的伪代码如下图所示： 可以将这个二维数组可视化，有如下图所示的五个物品和其对应的价值，且背包容量为11，那么如何让背包中装入的物品有最大的价值？ 遍历过程如下所示，从上到下代表i的遍历，从左到右代表j的遍历，最后可得到最大价值为40。 0-1背包问题已经被证明是NP完全问题，而它却有着一个动态规划解法，该解法有着O(nW)的时间复杂度，其中n是物品的个数，W是背包限制的最大负重。但是这种动态规划的算法称为伪多项式时间算法，这种算法不能真正意义上实现多项式时间内解决问题。 最长公共子序列（LCS问题）以最长公共子序列为例，将动态规划的四个步骤按部就班地走一遍。 Step1 描述最优解的结构（分析问题）假设$X = A, B, C, B$，$Y = B, D, C, A, B$，我们可以容易地想到暴力解法，即将枚举出X中的所有子序列，然后检查每个子序列是否是Y的子序列。假设X和Y的长度分别是m和n，那么暴力解法的时间复杂度是$O(2^mn)$。 我们可以观察到，LCS问题具有最优子结构，设$X=和Y=$为两个序列，$Z = $为X和Y的任意一个LCS，则有LCS的最优子结构定理： 如果$x_m=y_n$，那么$z_k = x_m = y_n$，而且$Z_{k-1}$是 $X_{m-1}$和$Y_{n-1}$的一个LCS。 如果$x_m \neq y_n$，那么$z_k \neq x_m$意味着$Z$是 $X_{m-1}$和$Y$的一个LCS。 如果$x_m \neq y_n$，那么$z_k \neq y_n$意味着$Z$是 $X$和$Y_{n-1}$的一个LCS。 Step2 递归定义最优解的值（递归解决）用$C[i,j]$表示$X_i$和$Y_j$的最长公共子序列LCS的长度，则有公式： Step3 按自底向上的方式计算最优解的值（计算LCS的长度）LCS_LENGTH以两个序列为输入，将LCS的长度保存到二维数组c中，将构造过程保存到另一个二维数组b中，伪代码如下所示： 1234567891011121314151617181920212223def LCS_LENGTH(X,Y): m = length(X) n = length(Y) # 初始化 for i = 1 to m: c[i][0] = 0 for j = 1 to n: c[0][j] = 0 # 计算LCS的长度 for i = 1 to m: for j = 1 to n: if x[i] == y[j]: c[i, j] = c[i-1, j-1] + 1 b[i, j] = '\' elif c[i-1, j] &gt;= c[i, j-1]: c[i, j] = c[i-1, j] b[i, j] = '|' else: c[i, j] = c[i, j-1] b[i, j] = '-'return c, b Step4 由计算的结果构造一个最优解（构建LCS）根据LCS_LENGTH返回的表b，可以构建一个LCS序列，输出所有值为’\’的元素，即可得到LCS，PRINT_LCS的伪代码如下所示： 123456789def PRINT_LCS(b, X, i, j): if i==0 or j==0: return 0 if b[i, j] == '\': PRINT_LCS(b, X, i-1, j-1) print X[i] elif b[i, j] == '|': PRINT_LCS(b, X, i-1, j) elif PRINT_LCS(b, X, i, j-1) 为了加深理解，使用C++实现了以上伪代码，使用PPT上的例子，最终得出结果如下图所示： 全部C++代码如下所示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172#include "stdafx.h"#include &lt;iostream&gt;using namespace std;#define m 4#define n 5#define skew 0#define up 1#define level 2void lcs_length(char*X, char*Y, int c[m+1][n+1], int b[m+1][n+1])&#123; int i, j; for (i=0; i&lt;m; i++) c[i][0] = 0; for (j=0; j&lt;n; j++) c[0][j] = 0; for (i=1; i&lt;=m; i++) for (j=1; j&lt;=n; j++) &#123; if (X[i] == Y[j]) &#123; c[i][j] = c[i-1][j-1] + 1; b[i][j] = skew; &#125; else if (c[i-1][j] &gt;= c[i][j-1]) &#123; c[i][j] = c[i-1][j]; b[i][j] = up; &#125; else &#123; c[i][j] = c[i][j-1]; b[i][j] = level; &#125; &#125;&#125;void print_lcs(int b[m+1][n+1], char* X, int i, int j)&#123; if (i==0 || j==0) return; if (b[i][j] == skew) &#123; print_lcs(b, X, i-1, j-1); cout &lt;&lt; X[i] &lt;&lt; ' '; &#125; else if(b[i][j] == level) print_lcs(b, X, i, j-1); else print_lcs(b, X, i-1, j);&#125;int main()&#123; char X[m+1] = &#123;' ','A','B','C','B'&#125;; char Y[n+1] = &#123;' ','B','D','C','A','B'&#125;; int c[5][6] = &#123;0&#125;; int b[5][6] = &#123;0&#125;; int i, j; cout &lt;&lt; "The X is ABCB" &lt;&lt; endl; cout &lt;&lt; "The Y is BDCAB" &lt;&lt; endl &lt;&lt; endl; lcs_length(X, Y, c, b); for(i=0; i&lt;=m; i++) &#123; for (j=0; j&lt;=n; j++) cout &lt;&lt; c[i][j] &lt;&lt; " "; cout &lt;&lt; endl; &#125; cout &lt;&lt; endl &lt;&lt; "The length of LCS is: " &lt;&lt; c[m][n] &lt;&lt; endl; cout &lt;&lt; "The LCS is: "; print_lcs(b, X, m, n); return 0;&#125; 编辑距离（Edit Distance）假设X和Y是两个字符串，我们要用最少的操作将X转换为Y，三种操作可供使用，分别是删除、插入和替换，最少操作的数目称为编辑距离。 与LCS类似，也可得到编辑距离的公式： 下面给出一个例子，具体的实现与LCS类似，这里不再赘述。 矩阵连乘问题（Chain MatrixMultiplication）给定n个矩阵$｛A1,A2,…,An｝$，其中$Ai$与$Ai+1$是可乘的，$i=1,2 ,…,n-1$。确定计算矩阵连乘积的计算次序，使得依此次序计算矩阵连乘积需要的数乘次数最少。 假如我们要得到计算从$A_i$到$A_j$的最优计算次序。首先假设这个计算次序在矩阵$A_k$和$A_{k+1}$之间断开，且$i \leq k &lt; j$，则计算量为前一部分的计算量、后一部分的计算量以及两部分相乘的计算量之和。 可以递归地定义$C[i, j]​$为： 可以看到，$k$的位置只有$j-i$种可能，此算法的时间复杂度为$O(n^3)​$，给出一个这种动态规划算法的计算实例。]]></content>
      <categories>
        <category>算法设计与分析</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CHAPTER5 分治法]]></title>
    <url>%2Fposts%2F57540%2F</url>
    <content type="text"><![CDATA[分治法是将一个复杂的问题分成一些规模较小而结构与原问题相似的子问题，递归地解这些子问题，然后将各子问题的解合并得到原问题的解。 分治法在每一层的递归上都有三个步骤： 分解（Divided）：将原问题分解成一系列子问题。 解决（Conquer）：递归地解各子问题。 合并（Combine）：将子问题的结果合并成原问题的解。 假设我们将原问题分解成a个子问题，每一个的大小是原问题的1/b。如果分解和合并的时间各为D(n)和C(n)，则可得到递归式： 合并排序（Merge Sort）当合并排序的运行时间如下分解： 分解：仅计算出子数组的中间位置，需要常量时间，故$D(n) = \Theta(1)$。 解决：递归地解两个规模为n/2的子问题，时间为$2T(n/2)$。 合并：在一个含有n个元素的子数组上，MERGE过程的运行时间为$\Theta(n)$，则$C(n) = \Theta(n)$。 因此，合并排序的最坏运行时间$T(n)$的递归表示是： 由之前第一章介绍的主定理，我们可以得到$T(n) = \Theta(nlgn)$，这里的$lgn$代表$log_2n$。同样，我们也可以通过递归树得到相同的答案。 最大子数组问题（Maximum Subarray Problem）对于一个具有连续连续值的数组A，寻找A的和最大的非空连续子数组，我们称这样的连续子数组为最大子数组。例如，对于下图的数组，A[1…16]的最大子数组是A[8…11]，其和为43。在实际的例子中可表示在第8天买入股票，并在第11天卖出，获得的收益为43美元。 使用分治法可以求解最大子数组问题。首先找到数组的中间点，将数组分为左右两个数组，那么最大子数组可能存在于下列三种情况之一： 完全位于左边的数组中。 完全位于右边的数组中。 跨越了左右两个数组。 对于前两种情况，使用同样的方式递归地划分为规模最小的子数组求解即可。对于第三种情况，我们采用的算法是从中间点向两边遍历，分别求出两边的最大子数组，然后将左右两边的子数组相加即为跨越中点的最大子数组。 下面给出算法的伪代码： 对FIND-MAXIMUM-SUBARRAY算法的运行时间进行分析： 首先计算出子数组的中间位置，需要常量时间$\Theta(1)$，然后递归地解两个规模为n/2的子问题，时间为$2T(n/2)$，调用FIND-MAX-CROSSING-SUBARRAY花费了$\Theta(n)$时间，则$T(n) = \Theta(1) + 2T(n/2) + \Theta(n) $，用主方法或递归树求解此递归式可得$T(n) = \Theta(nlgn)$。 斐波那契数列（Fibonacci Number）对于斐波那契数，我们有$F_0=0, F_1=1, F_n=F_{n-1}+F_{n-2}$。利用分治策略，我们可以将斐波那契数列转换为矩阵乘幂的问题，如下图所示： 使用分治法，我们将矩阵递归地分解成两个相同的矩阵，再将这两个矩阵相乘即可。 故$T(n) = T(n/2)+O(1) = T(n)/4+2O(1) = … = T(n/2^{logn})+O(logn)×O(1)$。 整数乘法（Integer Multiplication）假设x, y分别为两个n-bit的整数，如果要将它们相乘，模拟使用手动乘法得到的时间复杂度是$\Theta(n^2)$，考虑分治法。 令$x = (10^ma+b), y=(10^mc+d)$，如$x=1234567890, m=5, a=12345, b=67890$。那么$x×y = (10^ma+b)(10^mc+d)= 10^{2m}ac+10^m(bc+ad)+bd$，这里的时间复杂度$T(n) = 4T(n/2)+O(n)$，使用主方法可得$T(n) = O(n^2)$。 Anatolii, Karatsuba在1962年提出了一个只需要三次子乘法就可以完成运算的算法，其时间复杂度的递归表示为$T(n) = 3T(n/2)+O(n)$，使用主方法可得$T(n) = O(n^{lg3})$。 矩阵乘法（Matrix multiplication）给定一个n维矩阵X和Y，计算Z=XY。我们可以使用分治法求解这个问题。 分解：将X和Y分解为n/2维的矩阵。 解决：使用8次矩阵乘法递归地将这些n/2维的矩阵相乘。 合并：使用4次矩阵加法将矩阵合并。 下面给出一个例子： 1969年Strassen提出了一个只需要7次矩阵乘法就可以完成运算的算法，算法将原矩阵分为7个新的子矩阵如下图所示： 然后进行计算： 此算法的时间复杂度为$T(n) = 7T(n/2)+\Theta(n^2)=O(n{log_27})=O(n^{2.81})$。 凸包问题（The Convex Hull Problem）假设平面上有一系列点，过某些点作一个多边形，使这个多边形能把所有点都“包”起来，当这个多边形是凸多边形的时候，我们就叫它凸包，凸包问题就是求构成凸包的点，如下图所示。 使用蛮力法是最容易想到的，思路是由两点确定一条直线，如果剩余的点都在这条直线的同一侧，那么认为这两个点是构成凸包的点。蛮力法的时间复杂度为$O(n^3)$。 下面我们介绍解决凸包问题的分治法，下面为具体步骤和图例： 将所有点放在二维坐标系里，那么横坐标最大的两个点$p_1、p_n$一定是凸包上的点（具体可以用反证法证明，这里不展开说）。直线$p_1p_n$将点集合分为了两部分，分别叫上包和下包。 对于上包，求距离直线$p_1p_n$最远的点，即下图中的$p_{max}$。 作直线$p_1p_{max}$和$p_np_{max}$，把$p_1p_{max}$左侧的点当作上包，把$p_np_{max}$右侧的点也当作是上包。 重复步骤2、3。 对下包也做类似的操作。 分治法的时间复杂度为$T(n) = 2T(n/2) + O(n) = O(nlogn)$。 三格骨牌问题（Tromino Tiling）对于三格骨牌问题，同样可以用分治法求解，解决这个问题的思想是每次都将平板分成四块同等大小的子平板。 例如，在插入三格骨牌时，将平板分成四块，由于洞位于左上方的子平板，因此将三个骨牌放置为如图所示的位置，以确保四块子平板的大小相等。递归地进行这个过程即可得出结果。 最邻近点问题（Finding the Closest Pair of Points）顾名思义，最邻近点问题即在平面点集中找出距离最近的两个点。使用时间复杂度为$O(n^2)$的蛮力法可以解决这个问题。 下面我们介绍解决这个问题的分治法，首先将点集划分为两个部分，然后递归地寻找最近的点，若找到最近的两个点之间的距离为$\delta$，则寻找是否存在分别属于两个部分的点之间的距离小于$\delta$，算法的时间复杂度为$T(n) = O(n)+2T(n/2)+O(n)=O(nlgn)$。]]></content>
      <categories>
        <category>算法设计与分析</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CHAPTER4 贪心算法]]></title>
    <url>%2Fposts%2F17200%2F</url>
    <content type="text"><![CDATA[贪心算法即不从整体最优考虑，只是做出在当前看来最好的选择，它所做出的选择只是在某种意义上的局部最优选择。贪心算法对于大多数优化问题都能得到整体最优解（如单源最短路径问题、最小生成树问题等），虽然说并不总是这样的。在一些情况下，即使贪心算法不能得到整体最优解，但是其最终的结果是最优解的近似。 动态规划算法通常以自底向上的方式解各子问题，而贪心算法则通常以自顶向下的方式进行，每作一次贪心选择就将所求问题简化为规模更小的子问题。 下面来看一些具体的例子。 区间调度（Interval Scheduling）问题描述：假如我们有多个任务，每个任务都具有各自的开始时间和结束时间，求在任务不重叠的情况下任务的最大组合数。 根据贪心算法，我们可以从不同的角度分析这个问题。 Rule1：选择开始最早的任务 每次选择当前最先开始的任务，并依次选到最后一个。 从这个例子看出，失败！ Rule2：选择区间最短的任务 从最短的任务开始选择，并依次选择不重复的当前最短的任务，按照所用时间长短排序。 从这个例子看出，失败！ Rule3：选择冲突最少的任务对于每一个任务，计算与它冲突的任务个数，每次选择当前与其冲突最少的任务。 从这个例子看出，还是失败了。 Rule4：选择结束最早的任务每次选择当前最早结束的任务，并依次选到最后一个。 可以看出，在上面的三个例子中，本算法都是可行的。事实上，Rule4的算法是解决区间调度问题的一个可行算法。直观上来说，按这种方法选取的任务为未安排的任务留下了尽可能多的时间。也就是说，该算法的贪心选择使剩余的可安排时间段极大化，以安排尽可能多的不重叠活动。这个算法的时间复杂度是$O(nlogn)$。 集合覆盖（Set Cover）问题描述：在一个集合$B$以及$B$内元素构成的若干集合$S_1, S_2, … , S_m$中，找到数目最少的$S_i$使得$S_i$中的所有元素都包含了$B$中所有元素。为便于理解，给一个具体的例子。例如，$B$={1,2,3,4,5}，$S_1$={1,2,3}，$S_2$={2,4}，$S_3$={3,4}，$S_4$={4,5}，可以找到一个集合覆盖$S_1, S_4$。 其实集合覆盖问题是一个NP难的问题，但是我们仍然可以用贪心算法得到这个问题的近似解。 假设我们要在城镇里建几所学校，需要满足两点：1、每个学校都要建在一个城镇里。2、城镇离学校的距离不能超过30英里。 根据贪心算法，我们可以得到非常接近的解： 选出这样一个学校，即它覆盖了数量最多的城镇。 重复第一步，直到覆盖所有的城镇。 这是一种近似算法，贪心算法的运行时间为$O(n^2)​$，可得到如下图所示的结果，图中点的位置代表城镇的位置。 最小生成树（Minimum Spanning Tree）用贪心算法设计策略可以设计出构造最小生成树的有效算法，如Prim算法和Kruskal算法，尽管它们做贪心选择的方式不同。 关于MST的这两个算法相信大家应该都十分熟悉了，这里便不再展开。]]></content>
      <categories>
        <category>算法设计与分析</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CHAPTER3 最大匹配问题]]></title>
    <url>%2Fposts%2F63093%2F</url>
    <content type="text"><![CDATA[关于匹配的几个定义匹配（Matching）一个匹配是一个边的子集合$M\subseteq E$，且满足对所有顶点$v\in V$，$M$中至多有一条边与$v$相关联。也可以简单地说，一个匹配就是一个边的集合，其中任意两条边之间都没有公共顶点。 下面给出一个例子： 最大匹配（Maximum Matching）简单地说，最大匹配是一个图的所有匹配中边数最多的那个匹配。给出一个例子： 二分图（Bipartite Graph）二分图是图论中的一种特殊模型，设$G=(V,E)$是一个无向图，如果顶点$V$可分割为两个互不相交的子集$(A,B)$，并且图中的每条边$(i，j)$所关联的两个顶点$i$和$j$分别属于这两个不同的顶点集，则称图$G$为一个二分图。 完全匹配（Perfect Matching）简单地说，当一个图的某个匹配中所有的顶点都是匹配点，那么这个匹配就是完美匹配。同样给出一个例子： 在二分图中寻找最大匹配从本质上来说，二分图匹配其实是最大流的一种特殊情况。是解决这个问题的关键技巧在于建立一个流网络，其中流对应于匹配，如下图所示。 可以看出，图中添加了源点s和汇点t，它们是不属于V的新顶点。令已有边的容量为无穷大，且令s和t分别连接二分图，并设置其容量为1。这时，我们通过Ford-Fulkerson方法计算得到的最大流就等于最大二分匹配。 在一般图中寻找最大匹配在一般图中，我们使用增广路径（Agumenting path）来寻找最大匹配。如果一条路径的首尾是非匹配点，路径中除此之外（如果有）其他的点均是匹配点，那么这条路径就是一条增广路径。 如下图所示，我们从非匹配点9出发，经过匹配点4、8、1、6，最后在非匹配点2停止。所以，9-&gt;4-&gt;8-&gt;1-&gt;6-&gt;2 就是一条增广路径。 由于增广路径的首尾是非匹配点，那么增广路径的首尾边必为非匹配边。由于增广路径中匹配边与非匹配边一次交替，所以非匹配边的数目比匹配边多一条。我们可以利用这个特性来改进匹配，只要将匹配边与非匹配边互换即可，如下图所示。 所以，只要不断地迭代这个过程，直至找不到增广路径为止，就可以找到一般图的最大匹配。 补充：若要寻找带权一般图上的最大匹配，则在上面算法的基础上加个权重和判断即可。]]></content>
      <categories>
        <category>算法设计与分析</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CHAPTER2 最大流与最小割]]></title>
    <url>%2Fposts%2F16099%2F</url>
    <content type="text"><![CDATA[流网络$G=(V, E)​$是一个简单有向图，在V中指定顶点s和t，分别称为源点和汇点，有向图G中的每一条边$(u, v) \in E​$，对应有一个值$cap(u, v) \geq0​$，称为边的容量，这样的有向图G称作一个流网络，下图是一个例子。 $f(v, u)​$称作是从顶点u到顶点v的流，它满足以下性质： 容量限制：对所有$u, v \in V$，要求$f(u, v) \leq c(u, v)$。 反对称性：对所有$u, v \in V$，要求$f(u, v) = -f(v, u)$。 如果有一组流满足以下条件，那么这组流就成为一个可行流： 源点s：流出量 = 整个网络的流量 汇点t：流入量 = 整个网络的流量 中间点：总流入量 = 总流出量 最大流即网络G所有的可行流中，流量最大的一个可行流。 Ford-Fulkerson方法之所以称为Ford-Fulkerson方法而不是算法，是由于它包含具有不同运行时间的几种实现。Ford-Fulkerson方法依赖于三种重要思想：残留网络、增广路径、割。这三种思想是最大流最小割定理的精髓，该定理用流网络的割来描述最大流的值，我们将会在后面谈到。以下给出Ford-Fulkerson方法的伪代码： 12345Ford-Fulkerson-Method(G, s, t): initialize flow f to 0 while there exists an augmenting path p: do augment flow f along p return f 最大流最小割定理首先来介绍割的概念，一个割会把图G的顶点分成两个不相交的集合，其中s在一个集合中，t在另外一个集合中。割的容量就是从A指向B的所有边的容量和，最小割问题就是要找到割的容量最小的情况。下面给出两个例子，割的容量分别为30和62。 接着介绍残留网络和增广路径的概念，给定一个流网络$G$和一个可行流，流的残留网络$G_f$拥有与原网相同的顶点。流网络$G$中每条边将对应残留网中一条或者两条边，对于原流网络中的任意边(u, v)，流量为f(u, v)，容量为c(u, v)： 如果f(u, v) &gt; 0，则在残留网中包含一条容量为f(u, v)的边(v, u); 如果f(u, v) &lt; c(u, v)，则在残留网中包含一条容量为c(u, v) - f(u, v)的边(u, v)。 下图为一个例子： 对于一个已知的流网络$G=(V, E)$和流$f$，增广路径$p$为残留网络$G_f$中从s到t的一条简单路径。 最大流最小割定理：网络的最大流等于某一最小割的容量，并且下列条件是等价的： $f$是$G$的一个最大流。 残留网络$G_f$不包含增广路径。 对$G$的某个割$(S, T)$，有$|f| = c(S, T)$。 基本的Ford-Fulkerson算法根据，我们可以求给定有向图的最大流。下面给出《算法导论》中的一个实例： 上图中的左边表示开始时的残留网络，右边表示将增广路径加入残留网络后得到的新的可行流，通过三次迭代即可得到最大流，根据最大流最小割定理，我们同样可以得到最小割。 再通过本课程课件上的一个例题进行练习。 同样通过基本的Ford-Fulkerson算法，可得到答案如下。 Edmonds-Karp算法Edmonds和Karp曾经证明了如果每步的增广路径都是最短，那么整个算法会执行$O(mn)$步，Edmonds-Karp算法是用广度优先搜索来实现对增广路径p的计算的，实现的伪代码如下图所示。 由于在广度优先搜索时最坏情况下需要$O(m)$次操作，所以此算法的复杂度为$O(m^2n)$。之后，Dinitz改进了Edmonds-Karp算法，得到一个时间复杂度为$O(mn^2)​$的算法，下面给出一张关于最短增广路径算法研究历史的表格，这里就不再展开了。]]></content>
      <categories>
        <category>算法设计与分析</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CHAPTER1 函数的增长与递归式]]></title>
    <url>%2Fposts%2F36105%2F</url>
    <content type="text"><![CDATA[渐进记号用来表示算法的渐进运行时间的记号是用定义域为自然数集$N=\{ 0, 1, 2, …\}$的函数来定义的，这些记号便于用来表示最坏情况运行时间$T(n)$，因为$T(n)​$一般仅定义于整数的输入规模上。 $\Theta$记号（紧渐进界）对于$\Theta​$记号有如下的定义： $\Theta$记号限制一个函数在常数因子内，如图所示，$n_0$是最小的可能值。如果存在正常数$n_0, c_1, c_2$使得在$n_0$右边$f(n)$的值永远在$c_1g(n)$与$c_2g(n)$之间，那么可以写成$f(n) = \Theta(g(n))​$。 O记号（渐进上界）对于$O$记号有如下的定义： $O$记号给出一个函数在常数因子内的上限。如图所示，$n_0$是最小的可能值。如果存在正常数$n_0, c$使得在$n_0$右边$f(n)$的值永远等于或小于$cg(n)$，那么可以写成$f(n) = O(g(n))$。 Ω记号（渐进下界）对于$Ω​$记号有如下的定义： $Ω$记号给出一个函数在常数因子内的下限。如图所示，$n_0$是最小的可能值。如果存在正常数$n_0, c$使得在$n_0$右边$f(n)$的值永远等于或大于$cg(n)$，那么可以写成$f(n) = Ω(g(n))$。 o记号（渐进非紧上界）$O$记号所提供的渐进上界可能是紧的，但也有可能不是。例如，$2n^2=O(n^2)$是一个紧的上界，但$2n=O(n^2)$却不是一个紧的上界。于是，我们使用$o$记号来表示一个紧的上界。 对于$o​$记号有如下的定义： 例如，$2n=o(n^2)$，但$2n^2 \neq o(n^2)$。 $O$记号与$o$记号的定义是类似的，主要区别在于对于$f(n)=O(g(n))$，界$0 \leq f(n) \leq cg(n)$对某个常数$c&gt;0$成立即可，而对于$f(n)=o(g(n))$，界$0 \leq f(n) \leq cg(n)$对所有常数$c&gt;0$成立。 $ω$记号（渐进非紧下界）$ω$记号与$Ω$记号的关系就与前面小o和大o之间的关系是类似的，我们用$ω$记号表示一个紧的下界。 对于$ω$记号有如下的定义： 例如，$n^2/2=ω(n)$，但$n^2/2\neqω(n^2)$。 函数间的比较实数的许多关系属性可以用于渐进比较，以上的记号之间具有传递性和对称性，下面假设$f(n)$和$g(n)$是渐进正值函数。 解递归式的三种方法求解递归式，即找出解的渐进“$\Theta$”或“$O$”界的方法主要有三种： 代换法：先猜某个界存在，然后用数学归纳法证明该猜测的正确性。 递归树方法：将递归式转换成树形结构，树中的节点代表在不同递归层次付出的代价。 主方法：给出递归形式$T(n) = aT(n/b)+f(n)$的界，其中$a \geq 1, b&gt;1$，$f(n)$是给定的函数。这种方法要记忆三种情况，就可以确定很多简单递归式的界了。 代换法用代换法解递归式需要两个步骤： 猜测解的形式。 用数学归纳法找出使解真正有效地常数。 递归树方法虽然代换法给递归式的解的正确性提供了一种简单的证明方法，但是有的时候很难得到一个好的猜测。此时，画出一个递归树是一种得到好猜测的直接方法。 设$T(n) = 3T(n/4)+n^2​$，则使用递归树求解该递归式的过程如下图所示： 主方法设$a \geq 1, b&gt;1$，$f(n) $为一函数，$T(n)$由递归式 T(n) = aT(n/b)+f(n)对非负整数定义，那么$T(n)$有如下的渐进界： 求解和式时有一个比较常用的公式，假设$f(k)$是单调递增的函数，那么有如下的性质：]]></content>
      <categories>
        <category>算法设计与分析</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
</search>
