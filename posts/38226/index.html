<!DOCTYPE html>




<html class="theme-next mist" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Computer Vision,Deep Learning,">










<meta name="description" content="神经网络共有三个部分，在前两个部分中，我们讨论了神经网络的静态部分：如何创建网络的连接、数据和损失函数。本节将致力于讲解神经网络的动态部分，即神经网络学习和搜索最优超参数的过程。 梯度检查使用中心化公式：  \frac{df(x)}{dx} = \frac{f(x+h)-f(x-h)}{2h}使用相对误差来比较：如何比较数值梯度$f’_n$和$f’_a$？使用相对误差是更加合适的：  \frac{">
<meta name="keywords" content="Computer Vision,Deep Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="CHAPTER8 神经网络3">
<meta property="og:url" content="http://www.zhangshenghai.com/posts/38226/index.html">
<meta property="og:site_name" content="shenghai&#39;s blog | shxt">
<meta property="og:description" content="神经网络共有三个部分，在前两个部分中，我们讨论了神经网络的静态部分：如何创建网络的连接、数据和损失函数。本节将致力于讲解神经网络的动态部分，即神经网络学习和搜索最优超参数的过程。 梯度检查使用中心化公式：  \frac{df(x)}{dx} = \frac{f(x+h)-f(x-h)}{2h}使用相对误差来比较：如何比较数值梯度$f’_n$和$f’_a$？使用相对误差是更加合适的：  \frac{">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://pic2.zhimg.com/80/753f398b46cc28c1916d6703cf2080f5_hd.png">
<meta property="og:image" content="https://pic3.zhimg.com/80/05a6960a01c0204ced8d875ac3d91fba_hd.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/80/96573094f9d7f4b3b188069726840a2e_hd.png">
<meta property="og:image" content="https://pic1.zhimg.com/80/412afb713ddcff0ba9165ab026563304_hd.png">
<meta property="og:image" content="http://www.zhangshenghai.com/images/opt2.gif">
<meta property="og:image" content="http://www.zhangshenghai.com/images/opt1.gif">
<meta property="og:image" content="https://pic4.zhimg.com/80/d25cf561835c7b96ae6d1c91868bcbff_hd.png">
<meta property="og:updated_time" content="2019-03-26T03:34:03.301Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CHAPTER8 神经网络3">
<meta name="twitter:description" content="神经网络共有三个部分，在前两个部分中，我们讨论了神经网络的静态部分：如何创建网络的连接、数据和损失函数。本节将致力于讲解神经网络的动态部分，即神经网络学习和搜索最优超参数的过程。 梯度检查使用中心化公式：  \frac{df(x)}{dx} = \frac{f(x+h)-f(x-h)}{2h}使用相对误差来比较：如何比较数值梯度$f’_n$和$f’_a$？使用相对误差是更加合适的：  \frac{">
<meta name="twitter:image" content="https://pic2.zhimg.com/80/753f398b46cc28c1916d6703cf2080f5_hd.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"right","display":"always","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.zhangshenghai.com/posts/38226/">





  <title>CHAPTER8 神经网络3 | shenghai's blog | shxt</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">
  
  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>
	
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">shenghai's blog | shxt</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">good good study up up every day</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-playlist">
          <a href="/playlist/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-music"></i> <br>
            
            歌单
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>


 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.zhangshenghai.com/posts/38226/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="shenghai">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="shenghai's blog | shxt">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">CHAPTER8 神经网络3</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-17T08:00:00+08:00">
                2018-11-17
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/cs231n/" itemprop="url" rel="index">
                    <span itemprop="name">cs231n</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>神经网络共有三个部分，在前两个部分中，我们讨论了神经网络的静态部分：如何创建网络的连接、数据和损失函数。本节将致力于讲解神经网络的动态部分，即神经网络学习和搜索最优超参数的过程。</p>
<h1 id="梯度检查"><a href="#梯度检查" class="headerlink" title="梯度检查"></a>梯度检查</h1><p><strong>使用中心化公式</strong>：</p>
<script type="math/tex; mode=display">
\frac{df(x)}{dx} = \frac{f(x+h)-f(x-h)}{2h}</script><p><strong>使用相对误差来比较</strong>：如何比较数值梯度$f’_n$和$f’_a$？使用相对误差是更加合适的：</p>
<script type="math/tex; mode=display">
\frac{\left|f'_a - f'_n\right|}{max(\left|f'_a\right|,\left|f'_n\right|)}</script><p>上式考虑了差值占两个梯度绝对值的比例。在实践中：</p>
<ul>
<li>相对误差 &gt; 1e-2，通常意味着梯度可能出错。</li>
<li>1e-4 &lt; 相对误差 &lt; 1e-2，我们要对这个值感到不舒服。</li>
<li>相对误差 &lt; 1e-4，这个值对于有不可导点的目标函数是OK的，但是目标函数中如果没有kink（目标函数的不可导点），那么相对误差还是太高。</li>
<li>相对误差 &lt; 1e-7，这个值是个好结果。</li>
</ul>
<p>要知道的是网络的深度越深，相对误差就越高。如果是在对一个10层网络的输入数据做梯度检查，那么1e-2的相对误差值可能就OK了，因为误差一直在累积。</p>
<p><strong>使用双精度</strong>：一个常见的错误是使用单精度浮点数来进行梯度检查。这样会导致即使梯度实现正确，相对误差值也会很高（比如1e-2）。</p>
<p><strong>目标函数的不可导点（kinks）</strong>：在进行梯度检查时，一个导致不准确的原因是不可导点问题。如ReLU函数的原点就是不可导点，比如的当$x=-1e6$时，对ReLU函数进行梯度检查。因为x&lt;0，所以解析梯度在该点的梯度为0。然而，这里的数值梯度可能会突然计算出一个非零的梯度值，因为f(x+h)可能越过了不可导点。</p>
<p><strong>使用少量数据</strong>：解决上面的不可导点问题的一个办法是使用更少的数据。因为含有不可导点的损失函数的数据点越少，不可导点就越少，所以在计算有限差值近似时越过不可导点的几率就越小。</p>
<p><strong>谨慎设置步长h。</strong>在实践中h并不是越小越好，因为当h特别小的时候，就可能就会遇到数值精度问题。有时候如果梯度检查无法进行，可以试试将h调到1e-4或者1e-6，然后突然梯度检查可能就恢复正常。</p>
<p><strong>在特定的模式下使用梯度检查</strong>：最好让网络学习一小段时间，等到损失函数开始下降之后再进行梯度检查。在第一次迭代后就进行梯度检查的危险在于，此时可能正处在不正常的边界情况，从而掩盖了梯度没有正确实现的事实。</p>
<p><strong>不要让正则化吞没数据</strong>：需要注意的危险是正则化损失可能吞没掉数据损失，在这种情况下梯度主要来源于正则化部分。这样就会掩盖掉数据损失梯度的不正确实现。因此，推荐先关掉正则化对数据损失做单独检查，然后对正则化做单独检查。对于正则化的单独检查可以是修改代码，去掉其中数据损失的部分，也可以提高正则化强度，确认其效果在梯度检查中是无法忽略的，这样不正确的实现就会被观察到了。</p>
<p><strong>记得关闭随机失活（dropout）和数据扩张（augmentation）</strong>：在进行梯度检查时，记得关闭网络中任何不确定的效果的操作，比如随机失活，随机数据扩展等。不然它们会在计算数值梯度的时候导致巨大误差。</p>
<p><strong>检查少量的维度。</strong>在实际中，梯度可以有上百万的参数，在这种情况下只能检查其中一些维度然后假设其他维度是正确的。</p>
<h1 id="学习之前的合理性检查"><a href="#学习之前的合理性检查" class="headerlink" title="学习之前的合理性检查"></a>学习之前的合理性检查</h1><p>在进行费时费力的最优化之前，最好进行一些合理性检查：</p>
<ul>
<li><strong>寻找特定情况的正确损失值</strong>。在使用小参数进行初始化时，确保得到的损失值与期望一致。最好先单独检查数据损失（让正则化强度为0）。例如，对于一个跑CIFAR-10的Softmax分类器，一般期望它的初始损失值是2.302，这是因为初始时预计每个类别的概率是0.1（因为有10个类别），然后Softmax损失值正确分类的负对数概率：-ln(0.1)=2.302。对于Weston Watkins SVM，假设所有的边界都被越过（因为所有的分值都近似为零），所以损失值是9（因为对于每个错误分类，边界值是1）。如果没看到这些损失值，那么初始化中就可能有问题。</li>
<li><strong>提高正则化强度时导致损失值变大。</strong></li>
<li><strong>对小数据子集过拟合。</strong>最后也是最重要的一步，在整个数据集进行训练之前，尝试在一个很小的数据集上进行训练（比如20个数据），然后确保能到达0的损失值。进行这个实验的时候，最好让正则化强度为0，不然它会阻止得到0的损失。除非能通过这一个正常性检查，不然进行整个数据集训练是没有意义的。但是注意，能对小数据集进行过拟合并不代表万事大吉，依然有可能存在不正确的实现。比如，因为某些错误，数据点的特征是随机的，这样算法也可能对小数据进行过拟合，但是在整个数据集上跑算法的时候，就没有任何泛化能力。</li>
</ul>
<h1 id="检查整个学习过程"><a href="#检查整个学习过程" class="headerlink" title="检查整个学习过程"></a>检查整个学习过程</h1><p>在训练神经网络的时候，<strong>应该跟踪多个重要数值</strong>。这些数值输出的图表是观察训练进程的一扇窗口，是直观理解不同的超参数设置效果的工具，从而知道如何修改超参数以获得更高效的学习过程。</p>
<p>在下面的图表中，x轴通常都是表示<strong>周期（epochs）</strong>单位，该单位衡量了在训练中每个样本数据都被观察过次数的期望（一个周期意味着每个样本数据都被观察过了一次）。相较于<strong>迭代次数（iterations）</strong>，一般更倾向跟踪周期，这是因为迭代次数与数据的批尺寸（batchsize）有关，而批尺寸的设置又可以是任意的。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>训练期间第一个要跟踪的数值就是损失值，它在前向传播时对每个独立的批数据进行计算。下图展示的是随着损失值随时间的变化。</p>
<p><img src="https://pic2.zhimg.com/80/753f398b46cc28c1916d6703cf2080f5_hd.png" alt></p>
<p>左图展示了不同学习率的效果，右图展示了一个典型的随时间变化的损失函数值。</p>
<p>损失值的震荡程度和批尺寸（batch size）有关，当批尺寸为1，震荡会相对较大。当批尺寸就是整个数据集时震荡就会最小，因为每个梯度更新都是单调地优化损失函数（除非学习率设置得过高）。</p>
<h2 id="训练集和验证集准确率"><a href="#训练集和验证集准确率" class="headerlink" title="训练集和验证集准确率"></a>训练集和验证集准确率</h2><p>在训练分类器的时候，需要跟踪的第二重要的数值是训练集和验证集的准确率。</p>
<p><img src="https://pic3.zhimg.com/80/05a6960a01c0204ced8d875ac3d91fba_hd.jpg" alt></p>
<p>两者之间的空隙表示模型过拟合的程度。</p>
<ul>
<li>图中蓝色的曲线表示此模型有很强的过拟合，此时，我们应该增大正则化强度（更强的L2权重惩罚，更多的随机失活等）或收集更多的数据。</li>
<li>另一种可能是训练集曲线和验证集曲线很贴近，这说明模型容量还不够大，应该增加参数数量增大模型容量。</li>
</ul>
<h2 id="权重更新比例"><a href="#权重更新比例" class="headerlink" title="权重更新比例"></a>权重更新比例</h2><p>最后一个应该跟踪的是权重中更新值的数量和全部值的数量之间的比例。一个经验性的结论是这个比例应该在1e-3左右。如果更低，说明学习率可能太小，如果更高，说明学习率可能太高。下面是具体例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设参数向量为W，其梯度向量为dW</span></span><br><span class="line">param_scale = np.linalg.norm(W.ravel())</span><br><span class="line">update = -learning_rate*dW <span class="comment"># 简单SGD更新</span></span><br><span class="line">update_scale = np.linalg.norm(update.ravel())</span><br><span class="line">W += update <span class="comment"># 实际更新</span></span><br><span class="line"><span class="keyword">print</span> update_scale / param_scale <span class="comment"># 要得到1e-3左右</span></span><br></pre></td></tr></table></figure>
<h2 id="每层的激活数据及梯度分布"><a href="#每层的激活数据及梯度分布" class="headerlink" title="每层的激活数据及梯度分布"></a>每层的激活数据及梯度分布</h2><p>一个不正确的初始化可能让学习过程变慢，甚至彻底停止。一个方法是输出网络中所有层的激活数据和梯度分布的柱状图。直观地说，就是如果看到任何奇怪的分布情况，那都不是好兆头。比如，对于使用tanh的神经元，我们应该看到激活数据的值在整个[-1,1]区间中都有分布。如果看到神经元的输出全部是0，或者全都饱和了往-1和1上跑，那肯定就是有问题了。</p>
<h2 id="第一层可视化"><a href="#第一层可视化" class="headerlink" title="第一层可视化"></a>第一层可视化</h2><p>如果数据是图像，那么把第一层特征可视化可能会有帮助，以下是一个将神经网络第一层的权重可视化的例子。</p>
<p><img src="https://pic3.zhimg.com/80/96573094f9d7f4b3b188069726840a2e_hd.png" alt></p>
<p>左图中的特征充满了噪音，这表明网络可能出现了问题：网络没有收敛，学习率设置不恰当，正则化惩罚的权重过低等。</p>
<p>右图的特征不错，平滑、干净且种类繁多，说明训练过程进行良好。</p>
<h1 id="参数更新"><a href="#参数更新" class="headerlink" title="参数更新"></a>参数更新</h1><p>一旦能使用反向传播计算解析梯度，梯度就能够被用来进行参数更新了，参数更新有以下的好几种方法。深度网络的最优化是现在非常活跃的研究领域，本节将介绍一些公认有效的常用技巧。</p>
<h2 id="随机梯度下降及各种更新方法"><a href="#随机梯度下降及各种更新方法" class="headerlink" title="随机梯度下降及各种更新方法"></a>随机梯度下降及各种更新方法</h2><h3 id="随机梯度下降（SGD）"><a href="#随机梯度下降（SGD）" class="headerlink" title="随机梯度下降（SGD）"></a>随机梯度下降（SGD）</h3><p>最简单的更新形式是沿着负梯度方向改变参数，其最简单的更新形式是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 普通更新</span></span><br><span class="line">x += -learning_rate * dx</span><br></pre></td></tr></table></figure>
<h3 id="动量方法（Momentum）"><a href="#动量方法（Momentum）" class="headerlink" title="动量方法（Momentum）"></a>动量方法（Momentum）</h3><p>另一个有效的方法是<strong>动量更新（Momentum）</strong>，这个方法在深度网络上几乎总能得到更好的收敛速度，可以看作是从物理角度对于最优化问题得到的启发。损失值可以理解为山的高度，用随机数字初始化参数等同于在某个位置给质点设定初始速度为0。这样最优化过程可以看做是模拟参数向量（即质点）在地形上滚动的过程。</p>
<p>在SGD中，梯度直接影响位置。而在Momentum中，物理观点建议梯度只影响速度，然后速度再影响位置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 动量更新</span></span><br><span class="line">v = mu * v - learning_rate * dx <span class="comment"># 与速度融合</span></span><br><span class="line">x += v <span class="comment"># 与位置融合</span></span><br></pre></td></tr></table></figure>
<p>这里引入了一个初始化为0的变量v和超参数mu。mu在最优化的过程中被看作是动量（一般设为[0.5, 0.9, 0.95, 0.99]中的一个），但其物理意义与摩擦系数更一致。这个变量有效地抑制了速度，降低了系统的动能。和学习率退火类似，动量随时间变化的设置有时能略微改善最优化的结果，动量在后期应该上升。一个典型的例子是刚开始时将mu设为0.5，在后面的周期中慢慢提升到0.99。</p>
<p>通过动量更新，参数向量会在任何有持续梯度的方向上增加速度。</p>
<h3 id="NAG（Nesterov-Accelerated-Gradient）"><a href="#NAG（Nesterov-Accelerated-Gradient）" class="headerlink" title="NAG（Nesterov Accelerated Gradient）"></a>NAG（Nesterov Accelerated Gradient）</h3><p>与普通动量方法有些许不同，理论上NAG对于凸函数能得到更好地收敛，在实际中也比普通动量方法表现的更好一些。</p>
<p>NAG的核心思路是，当参数向量位于某个位置<strong>x</strong>时，Momentum会通过<strong>mu*v</strong>稍微改变参数向量。因此，如果要计算梯度，可以直接计算<strong>x + mu*v</strong>而不是<strong>x</strong>，可以将<strong>x + mu*v</strong>看作是未来的近似位置，这样的计算就更有意义了。</p>
<p><img src="https://pic1.zhimg.com/80/412afb713ddcff0ba9165ab026563304_hd.png" alt></p>
<p>如上图所示，既然我们知道动量会把我们带到绿色箭头指向的点，我们就不要在原点那里计算梯度了。使用Nesterov向量，我们就在这个x_ahead的地方计算梯度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_ahead = x + mu * v</span><br><span class="line"><span class="comment"># 计算dx_ahead(在x_ahead处的梯度，而不是在x处的梯度)</span></span><br><span class="line">v = mu * v - learning_rate * dx_ahead</span><br><span class="line">x += v</span><br></pre></td></tr></table></figure>
<p>然而在实践中，人们更喜欢使用与SGD、Momentum类似简单的表达式，通过改写可得到：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">v_prev = v <span class="comment"># 存储备份</span></span><br><span class="line">v = mu * v - learning_rate * dx <span class="comment"># 速度更新保持不变</span></span><br><span class="line">x += -mu * v_prev + (<span class="number">1</span> + mu) * v <span class="comment"># 位置更新变了形式</span></span><br></pre></td></tr></table></figure>
<h2 id="学习率衰减方式"><a href="#学习率衰减方式" class="headerlink" title="学习率衰减方式"></a>学习率衰减方式</h2><p>如果学习率很高，系统的动能就过大，参数向量就会无规律地跳动，不能够稳定到损失函数更深更窄的部分去。通常，实现学习率退火有3种方式：</p>
<ul>
<li><strong>随步数衰减</strong>：每进行几个周期就根据一些因素降低学习率。典型的值是每过5个周期就将学习率减少一半，或者每20个周期减少到之前的0.1。这些数值的设定是严重依赖具体问题和模型的选择的。在实践中可能看见这么一种经验做法：使用一个固定的学习率来进行训练的同时观察验证集错误率，每当验证集错误率停止下降，就乘以一个常数（比如0.5）来降低学习率。</li>
<li><strong>指数衰减</strong>：数学公式是$\alpha = \alpha_0e^{-kt}$，其中$\alpha_0, k$是超参数，$t$是迭代次数。</li>
<li><strong>1/t衰减</strong>：数学公式是$\alpha = \alpha_0/(1+kt)$。其中$\alpha_0, k$是超参数，$t$是迭代次数。</li>
</ul>
<p>在实践中，我们发现随步数衰减的<strong>随机失活（dropout）</strong>更受欢迎，因为它使用的超参数（衰减系数和以周期为时间单位的步数）比k更有解释性。最后，如果你有足够的计算资源，可以让衰减更加缓慢一些，让训练时间更长些。</p>
<h2 id="二阶方法"><a href="#二阶方法" class="headerlink" title="二阶方法"></a>二阶方法</h2><p>第二类常用的最优化方法是基于<strong>牛顿法</strong>的，其迭代如下：</p>
<script type="math/tex; mode=display">
x \leftarrow x - [H f(x)]^{-1} \nabla f(x)</script><p>这里的$Hf(x)$是Hessian矩阵，它是函数的二阶偏导数的平方矩阵。直观理解上，就是乘以Hessian转置矩阵可以让最优化过程在曲率小的时候大步前进，在曲率大的时候小步前进。这个公式中没有学习率这个超参数，这相较于一阶方法是一个巨大的优势。</p>
<p>然而此方法很难用到实际中去，因为计算（以及求逆）Hessian矩阵操作非常耗费时间和空间。举例来说，假设一个有一百万个参数的神经网络，其Hessian矩阵大小就是[1,000,000 x 1,000,000]，将占用将近3,725GB的内存。在这些方法中最流行的是L-BFGS，该方法使用随时间的梯度中的信息来隐式地近似。</p>
<p>然而，即使解决了存储空间的问题，L-BFGS的一个巨大劣势是要对整个训练集进行计算，而整个训练集一般包含几百万的样本。而与mini-batch SGD不同，让L-BFGS在小批量上运行起来很需要技巧。</p>
<p>在实践中，使用L-BFGS之类的二阶方法并不常见。相反，基于Nesterov的动量更新的各种随机梯度下降方法更加常用，因为它们更简单而且更容易扩展。</p>
<h2 id="逐参数适应学习率方法"><a href="#逐参数适应学习率方法" class="headerlink" title="逐参数适应学习率方法"></a>逐参数适应学习率方法</h2><p>前面讨论的所有方法都是对学习率进行全局的操作，对于所有的参数都是一样的。学习率调参是很耗费计算资源的过程，所以很多工作都在研究能够适应性地对学习率调参的方法，甚至是逐个参数适应学习率调参。</p>
<h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p><strong>Adagrad</strong>是一个适应性学习率算法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设有梯度和参数向量x</span></span><br><span class="line">cache += dx**<span class="number">2</span></span><br><span class="line">x += -learning_rate * dx / (np.sqrt(cache) + eps)</span><br></pre></td></tr></table></figure>
<p>cache的尺寸和梯度矩阵的尺寸是一样的，还跟踪了每个参数梯度dx的平方和。注意，接收到高梯度值的权重更新的效果被减弱，而接收到低梯度值的权重更新的效果将会增强。eps（一般设为1e-4到1e-8之间）是防止出现除以0的情况。Adagrad的一个缺点是，单调的学习率通常过于激进且过早停止学习。</p>
<h3 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h3><p><strong>RMSprop</strong>是一个非常高效但没有公开发表的适应性学习率算法。这个方法用一种很简单的方式修改了Adagrad方法，让它不那么激进：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cache =  decay_rate * cache + (<span class="number">1</span> - decay_rate) * dx**<span class="number">2</span></span><br><span class="line">x += - learning_rate * dx / (np.sqrt(cache) + eps)</span><br></pre></td></tr></table></figure>
<p>decay_rate是一个超参数，常用的值是[0.9, 0.99, 0.999]。RMSProp仍然是基于梯度的大小来对每个权重的学习率进行修改，这样的效果同样不错。但与Adagrad不同的是，RMSProp的更新不会让学习率单调变小。</p>
<h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p><strong>Adam</strong>是最近才提出的一种更新方法，它看起来像是RMSprop的动量版：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m = beta1*m + (<span class="number">1</span>-beta1)*dx</span><br><span class="line">v = beta2*v + (<span class="number">1</span>-beta2)*(dx**<span class="number">2</span>)</span><br><span class="line">x += -learning_rate * m / (np.sqrt(v) + eps)</span><br></pre></td></tr></table></figure>
<p>这个方法和RMSProp很像，只是把原始梯度向量dx替换成了平滑版的梯度m。论文中推荐的参数值eps=1e-8，beta1=0.9，beta2=0.999。</p>
<p>在实践中，推荐Adam作为默认的算法，它一般比RMSProp的效果好一些。但是也可以试试SGD+Nesterov动量。</p>
<p><img src="/images/opt2.gif" alt="opt2"></p>
<p><img src="/images/opt1.gif" alt="opt2"></p>
<p>第一张动画是一个损失函数的等高线图，基于动量的方法出现了走偏了的情况。</p>
<p>第二张动画展示了一个马鞍状的最优化地形，其中对于不同维度它的曲率不同（在一个维度下降，而在另一个维度上升）。SGD、Momentum、NAG很难突破对称性，一直卡在顶部。而逐参数适应性学习率算法可以看到马鞍另一个维度上具有很低的梯度。</p>
<h2 id="经验之谈"><a href="#经验之谈" class="headerlink" title="经验之谈"></a>经验之谈</h2><ul>
<li>对于稀疏数据，尽量使用学习率可自适应的优化方法，不用手动调节，而且最好采用默认值</li>
<li>SGD通常训练时间更长，但是在好的初始化和学习率调度方案的情况下，结果更可靠</li>
<li>如果在意更快的收敛，并且需要训练较深较复杂的网络时，推荐使用学习率自适应的优化方法。</li>
<li>Adadelta，RMSprop，Adam是比较相近的算法，在相似的情况下表现差不多。</li>
<li>在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果</li>
</ul>
<h1 id="超参数调优"><a href="#超参数调优" class="headerlink" title="超参数调优"></a>超参数调优</h1><p>我们已经看到，训练一个神经网络会遇到很多超参数设置。神经网络最常用的设置有：</p>
<ul>
<li>初始学习率。</li>
<li>学习率衰减方式（例如一个衰减常量）。</li>
<li>正则化强度（L2惩罚，随机失活强度）。</li>
</ul>
<p><strong>实现</strong>。更大的神经网络需要更长的时间去训练，所以调参可能需要几天甚至几周。一个具体的设计是用<strong>worker</strong>持续地随机设置参数然后进行最优化。</p>
<p><strong>比起交叉验证最好使用一个验证集</strong>。在大多数情况下，一个尺寸合理的验证集可以让代码更简单，不需要用几个数据集来交叉验证。</p>
<p><strong>超参数范围</strong>。在对数尺度上进行超参数搜索。例如，一个典型的学习率应该看起来是这样：<strong>learning_rate = 10 \</strong> uniform(-6, 1)<strong>。对于正则化强度，可以采用同样的策略。但是有一些参数（比如随机失活）还是在原始尺度上进行搜索（例如：</strong>dropout=uniform(0,1)**）。</p>
<p><strong>随机搜索优于网格搜索</strong>。Bergstra和Bengio在文章Random Search for Hyper-Parameter Optimization中说“随机选择比网格化的选择更加有效，而且在实践中也更容易实现。</p>
<p><img src="https://pic4.zhimg.com/80/d25cf561835c7b96ae6d1c91868bcbff_hd.png" alt></p>
<p>上面是Random Search for Hyper-Parameter Optimization的核心说明图。通常，有些超参数比其余的更重要，通过随机搜索，可以让你更精确地发现那些比较重要的超参数的好数值。</p>
<p><strong>对于边界上的最优值要小心</strong>。这种情况一般发生在你在一个不好的范围内搜索超参数（比如学习率）的时候。比如，假设我们使用<strong>learning_rate = 10 \</strong> uniform(-6,1)**来进行搜索。一旦我们得到一个比较好的值，一定要确认它不在这个范围的边界上，不然你可能错过更好的其他搜索范围。</p>
<p><strong>从粗到细地分阶段搜索</strong>。在实践中，先进行初略范围（比如10 ** [-6, 1]）搜索，然后根据好的结果出现的地方，缩小范围进行搜索。进行粗搜索的时候，让模型训练1个周期就可以了，因为很多超参数的设定会让模型没法学习，或者突然就爆出很大的损失值。第二个阶段就是对一个更小的范围进行搜索，这时可以让模型运行5个周期，而最后一个阶段就在最终的范围内进行仔细搜索，运行很多次周期。</p>
<p><strong>贝叶斯超参数最优化</strong>。是一整个研究领域，主要是研究在超参数空间中更高效的导航算法，这里不展开讨论。</p>
<h1 id="模型集成"><a href="#模型集成" class="headerlink" title="模型集成"></a>模型集成</h1><p>在实践的时候，有一个总是能提升神经网络几个百分点准确率的办法，就是在训练的时候训练几个独立的模型，然后在测试的时候将它们的预测结果进行平均。<strong>模型数量增加，算法的结果也单调提升</strong>（尽管提升效果越来越少）。<strong>模型之间的差异度越大，提升效果可能越好</strong>。进行集成有以下几种方法：</p>
<ul>
<li><strong>同一个模型，不同的初始化</strong>。使用交叉验证来得到最好的超参数，然后用最好的参数来训练不同初始化条件的模型。</li>
<li><strong>在交叉验证中发现最好的模型</strong>。使用交叉验证来得到最好的超参数，然后取其中最好的几个（比如10个）模型来进行集成。这样就提高了集成的多样性，但风险在于可能会包含不够理想的模型。在实际操作中，这样操作起来比较简单，在交叉验证后就不需要额外的训练了。</li>
<li><strong>一个模型设置多个记录点</strong>。如果训练非常耗时，那就在不同的训练时间对网络留下记录点（比如每个周期结束），然后用它们来进行模型集成。很显然，这样做多样性不足，但是在实践中效果还是不错的，这种方法的优势是代价比较小。</li>
<li><strong>在训练的时候跑参数的平均值</strong>。还有一个也能得到1-2个百分点的提升的小代价方法，就是在训练过程中，如果损失值相较于前一次权重出现指数下降时，就在内存中对网络的权重进行一个备份。将最后几次得到的权重进行平均，你会发现这个“平滑”过的版本的权重总是能得到更少的误差。直观的理解就是目标函数是一个碗状的，你的网络在这个周围跳跃，所以对它们平均一下，就更可能跳到中心去。</li>
</ul>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>训练一个神经网络需要：</p>
<ul>
<li>利用少量数据进行梯度检查。</li>
<li>进行合理性检查，确认初始损失值的合理的，在小数据集上能得到100%的准确率。</li>
<li>在训练时，跟踪损失函数值、训练集和验证集的准确率，还可以跟踪更新的参数量相对于总参数量的比例（1e-3左右）。对于卷积神经网络，可以将第一层的权重可视化。</li>
<li>推荐的两个参数更新方法是SGD+Nesterov动量方法和Adam方法。</li>
<li>随着训练进行学习率衰减。</li>
<li>使用随机搜索（而不是网格搜索）来搜索最优的超参数，从粗（比较宽的超参数范围训练1-5个周期）到细（窄范围训练很多个周期）地搜索。</li>
<li>进行模型集成来获得额外的性能提高。</li>
</ul>
<blockquote>
<p>参考：<a href="https://zhuanlan.zhihu.com/p/21930884" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21930884</a></p>
</blockquote>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Computer-Vision/" rel="tag"><i class="fa fa-tag"></i> Computer Vision</a>
          
            <a href="/tags/Deep-Learning/" rel="tag"><i class="fa fa-tag"></i> Deep Learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/posts/20956/" rel="next" title="CHAPTER7 神经网络2">
                <i class="fa fa-chevron-left"></i> CHAPTER7 神经网络2
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/posts/36105/" rel="prev" title="CHAPTER1 函数的增长与递归式">
                CHAPTER1 函数的增长与递归式 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
 	<div id="gitalk-container">
 	</div>

  




        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">shenghai</p>
              <p class="site-description motion-element" itemprop="description">good good study up up every day</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">73</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">44</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#梯度检查"><span class="nav-number">1.</span> <span class="nav-text">梯度检查</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#学习之前的合理性检查"><span class="nav-number">2.</span> <span class="nav-text">学习之前的合理性检查</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#检查整个学习过程"><span class="nav-number">3.</span> <span class="nav-text">检查整个学习过程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#损失函数"><span class="nav-number">3.1.</span> <span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#训练集和验证集准确率"><span class="nav-number">3.2.</span> <span class="nav-text">训练集和验证集准确率</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#权重更新比例"><span class="nav-number">3.3.</span> <span class="nav-text">权重更新比例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#每层的激活数据及梯度分布"><span class="nav-number">3.4.</span> <span class="nav-text">每层的激活数据及梯度分布</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第一层可视化"><span class="nav-number">3.5.</span> <span class="nav-text">第一层可视化</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参数更新"><span class="nav-number">4.</span> <span class="nav-text">参数更新</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#随机梯度下降及各种更新方法"><span class="nav-number">4.1.</span> <span class="nav-text">随机梯度下降及各种更新方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#随机梯度下降（SGD）"><span class="nav-number">4.1.1.</span> <span class="nav-text">随机梯度下降（SGD）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#动量方法（Momentum）"><span class="nav-number">4.1.2.</span> <span class="nav-text">动量方法（Momentum）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#NAG（Nesterov-Accelerated-Gradient）"><span class="nav-number">4.1.3.</span> <span class="nav-text">NAG（Nesterov Accelerated Gradient）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#学习率衰减方式"><span class="nav-number">4.2.</span> <span class="nav-text">学习率衰减方式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二阶方法"><span class="nav-number">4.3.</span> <span class="nav-text">二阶方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#逐参数适应学习率方法"><span class="nav-number">4.4.</span> <span class="nav-text">逐参数适应学习率方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Adagrad"><span class="nav-number">4.4.1.</span> <span class="nav-text">Adagrad</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RMSProp"><span class="nav-number">4.4.2.</span> <span class="nav-text">RMSProp</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adam"><span class="nav-number">4.4.3.</span> <span class="nav-text">Adam</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#经验之谈"><span class="nav-number">4.5.</span> <span class="nav-text">经验之谈</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#超参数调优"><span class="nav-number">5.</span> <span class="nav-text">超参数调优</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#模型集成"><span class="nav-number">6.</span> <span class="nav-text">模型集成</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#总结"><span class="nav-number">7.</span> <span class="nav-text">总结</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">shenghai</span>

  
</div>








        ﻿






        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
  <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
   <script type="text/javascript">
        var gitalk = new Gitalk({
          clientID: '08a34024c7a6ddfb2ebf',
          clientSecret: 'cc5fd7df568462e80b8db1fde4886dfb511a0ca4',
          repo: 'shenghaishxt.github.io',
          owner: 'shenghaishxt',
          admin: ['shenghaishxt'],
          id: location.pathname,
          distractionFreeMode: 'true'
        })
        gitalk.render('gitalk-container')           
       </script>



  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
  <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
   <script type="text/javascript">
        var gitalk = new Gitalk({
          clientID: '08a34024c7a6ddfb2ebf',
          clientSecret: 'cc5fd7df568462e80b8db1fde4886dfb511a0ca4',
          repo: 'shenghaishxt.github.io',
          owner: 'shenghaishxt',
          admin: ['shenghaishxt'],
          id: location.pathname,
          distractionFreeMode: 'true'
        })
        gitalk.render('gitalk-container')           
       </script>

  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
